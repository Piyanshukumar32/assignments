{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZNMxnCgATLv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.What is a Decision Tree, and how does it work\n",
        "\n",
        "ans:-A decision tree is a supervised machine learning algorithm that can be used for both classification and regression tasks. It's a flowchart-like structure where each internal node represents a test on an attribute, each branch represents the outcome of the test, and each leaf node represents a class label (in classification) or a numerical value (in regression).\n",
        "\n",
        "How does it work?\n",
        "\n",
        "Building the Tree:\n",
        "\n",
        "The algorithm starts with the entire dataset and selects the best attribute to split the data based on a certain criterion (e.g., Gini impurity, information gain).\n",
        "It creates a node for this attribute and branches for each possible outcome of the test on that attribute.\n",
        "This process is recursively applied to each subset of data until a stopping condition is met (e.g., all data points in a node belong to the same class, a maximum depth is reached).\n",
        "Making Predictions:\n",
        "\n",
        "To classify a new data point, the algorithm starts at the root node and traverses the tree based on the values of the attributes in the data point.\n",
        "It follows the branches corresponding to the attribute values until it reaches a leaf node.\n",
        "The class label or numerical value associated with the leaf node is then assigned as the prediction for the data point.\n",
        "Analogy:\n",
        "\n",
        "Imagine you're trying to decide whether to go to a party. You might consider factors like the weather, who's going, and if there will be food. A decision tree would represent this decision-making process as a series of questions and answers:\n",
        "\n",
        "Is the weather good? If yes, go to the next question. If no, stay home.\n",
        "Are my friends going? If yes, go to the next question. If no, stay home.\n",
        "Will there be food? If yes, go to the party! If no, stay home.\n",
        "This series of questions and answers forms a tree-like structure, where each question is a node and each answer is a branch. The final decision (go to the party or stay home) is represented by the leaf nodes."
      ],
      "metadata": {
        "id": "rWkBHs5_ATuu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.What are impurity measures in Decision Trees\n",
        "\n",
        "ans:-Impurity Measures\n",
        "\n",
        "Impurity measures are used in decision trees to evaluate the homogeneity of a set of data points within a node. In other words, they quantify how mixed the data is in a particular node. A node is considered \"pure\" if all data points within it belong to the same class.\n",
        "\n",
        "Common Impurity Measures:\n",
        "\n",
        "Gini Impurity:\n",
        "\n",
        "It measures the probability of misclassifying a randomly chosen element in the dataset if it were randomly labeled according to the class distribution in the dataset.\n",
        "It ranges from 0 (pure) to 1 (impure).\n",
        "Formula: Gini(S) = 1 - Σ(pᵢ)², where pᵢ is the proportion of data points belonging to class i in the set S.\n",
        "Entropy:\n",
        "\n",
        "It measures the uncertainty or randomness in a set of data points.\n",
        "It ranges from 0 (pure) to log₂(k) (impure), where k is the number of classes.\n",
        "Formula: Entropy(S) = -Σ(pᵢ * log₂(pᵢ)), where pᵢ is the proportion of data points belonging to class i in the set S.\n",
        "Misclassification Error:\n",
        "\n",
        "It measures the fraction of data points that would be incorrectly classified if they were assigned the most common class in the node.\n",
        "It ranges from 0 (pure) to 1 (impure).\n",
        "Formula: Misclassification Error(S) = 1 - max(pᵢ), where pᵢ is the proportion of data points belonging to class i in the set S.\n",
        "Why are impurity measures important?\n",
        "\n",
        "Impurity measures play a crucial role in decision tree construction. They are used to select the best attribute to split the data at each node. The goal is to choose the attribute that minimizes the impurity of the resulting child nodes. This process helps to create a tree that effectively separates data points into their respective classes, leading to better classification accuracy."
      ],
      "metadata": {
        "id": "Ajs_OtC_AmzG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3 What is the mathematical formula for Gini Impurity\n",
        "\n",
        "Gini Impurity Formula:\n",
        "\n",
        "\n",
        "Gini(S) = 1 - Σ(pᵢ)²\n",
        "Use code with caution\n",
        "where:\n",
        "\n",
        "S is the dataset or subset of data at a particular node.\n",
        "pᵢ is the proportion of data points belonging to class i in the set S.\n",
        "The summation (Σ) is taken over all classes in the dataset.\n",
        "Explanation:\n",
        "\n",
        "The formula calculates the probability of misclassifying a randomly chosen element from the dataset if it were randomly labeled according to the class distribution in the dataset.\n",
        "It essentially measures the impurity or heterogeneity of the data at a node.\n",
        "A Gini Impurity of 0 indicates a perfectly pure node (all data points belong to the same class), while a Gini Impurity closer to 1 indicates a more impure or heterogeneous node.\n",
        "Example:\n",
        "\n",
        "Let's say we have a dataset with two classes, A and B. At a particular node, there are 6 data points: 4 belonging to class A and 2 belonging to class B.\n",
        "\n",
        "p(A) = 4/6 = 2/3\n",
        "p(B) = 2/6 = 1/3\n",
        "Using the Gini Impurity formula:\n",
        "\n",
        "\n",
        "Gini(S) = 1 - [(2/3)² + (1/3)²] = 1 - [4/9 + 1/9] = 1 - 5/9 = 4/9 ≈ 0"
      ],
      "metadata": {
        "id": "kV3ruHYmAw8B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4 What is the mathematical formula for Entropy\n",
        "\n",
        "Entropy Formula:\n",
        "\n",
        "\n",
        "Entropy(S) = - Σ(pᵢ * log₂(pᵢ))\n",
        "Use code with caution\n",
        "where:\n",
        "\n",
        "S is the dataset or subset of data at a particular node.\n",
        "pᵢ is the proportion of data points belonging to class i in the set S.\n",
        "The summation (Σ) is taken over all classes in the dataset.\n",
        "log₂ represents the logarithm to the base 2.\n",
        "Explanation:\n",
        "\n",
        "The formula calculates the uncertainty or randomness in a set of data points.\n",
        "It essentially measures the average amount of information needed to identify the class of a randomly chosen data point from the set.\n",
        "An Entropy of 0 indicates a perfectly pure node (all data points belong to the same class), while a higher Entropy value indicates a more impure or heterogeneous node.\n",
        "The maximum Entropy value is log₂(k), where k is the number of classes. This occurs when all classes are equally likely.\n",
        "Example\n",
        "\n",
        "Let's say we have a dataset with two classes, A and B. At a particular node, there are 6 data points: 4 belonging to class A and 2 belonging to class B.\n",
        "\n",
        "p(A) = 4/6 = 2/3\n",
        "p(B) = 2/6 = 1/3\n",
        "Using the Entropy formula:\n",
        "\n",
        "\n",
        "Entropy(S) = - [(2/3) * log₂(2/3) + (1/3) * log₂(1/3)] ≈ 0.92\n",
        "Use code with caution"
      ],
      "metadata": {
        "id": "i4jEluWwBn3o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5 What is Information Gain, and how is it used in Decision Trees\n",
        "\n",
        "\n",
        "ans:-Information Gain is a concept used in Decision Trees to measure how much information a feature provides about the target variable. It essentially quantifies the reduction in uncertainty or entropy achieved by splitting the data based on a particular feature.\n",
        "\n",
        "In simpler terms, it helps us determine which feature is the best to split the data on at each node of the tree to create the most informative and accurate tree possible.\n",
        "\n",
        "How is it used in Decision Trees?\n",
        "\n",
        "Calculate Entropy: First, we calculate the entropy of the target variable before any split. Entropy represents the uncertainty or randomness in the data.\n",
        "\n",
        "Split Data: Next, we consider each feature and split the data based on its possible values. For each split, we calculate the entropy of the target variable within each resulting subset.\n",
        "\n",
        "Calculate Information Gain: For each feature split, we calculate the Information Gain by subtracting the weighted average entropy of the subsets from the original entropy.\n",
        "\n",
        "Select Best Feature: The feature that results in the highest Information Gain is chosen as the best feature to split the data on at that node. This process is repeated recursively to build the entire tree.\n",
        "\n",
        "Formula:\n",
        "\n",
        "Information Gain (S, A) = Entropy(S) - Σ [ (|Sv| / |S|) * Entropy(Sv) ]\n",
        "\n",
        "where:\n",
        "\n",
        "S: The original dataset\n",
        "A: The feature being considered for the split\n",
        "Sv: The subset of data where feature A has value v\n",
        "|S|: The number of data points in the original dataset\n"
      ],
      "metadata": {
        "id": "RMCv93bLBzGN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6 What is the difference between Gini Impurity and Entropy\n",
        "\n",
        "ans:-Both Gini Impurity and Entropy are metrics used to measure the impurity or uncertainty of a node in a Decision Tree. They help determine the best feature to split the data on at each node. Here's a breakdown of their differences:\n",
        "\n",
        "Gini Impurity:\n",
        "\n",
        "Concept: It measures the probability of misclassifying a randomly chosen element in the dataset if it were randomly labeled according to the class distribution in the dataset.\n",
        "Formula: Gini(S) = 1 - Σ(pi)^2, where pi is the proportion of data points belonging to class i in the set S.\n",
        "Range: 0 (pure) to 1 (impure)\n",
        "Interpretation: A lower Gini Impurity value indicates a more pure node, meaning the data points within that node mostly belong to the same class.\n",
        "Entropy:\n",
        "\n",
        "Concept: It measures the uncertainty or randomness in a set of data points.\n",
        "Formula: Entropy(S) = - Σ(pi * log2(pi)), where pi is the proportion of data points belonging to class i in the set S.\n",
        "Range: 0 (pure) to log2(k) (impure), where k is the number of classes.\n",
        "Interpretation: A lower Entropy value indicates a more pure node, meaning the data points within that node are more homogeneous in terms of their class labels.\n",
        "Key Differences:\n",
        "\n",
        "Calculation: Gini Impurity is calculated using a simpler formula compared to Entropy, which involves logarithms.\n",
        "\n",
        "Range: Gini Impurity ranges from 0 to 1, while Entropy ranges from 0 to log2(k).\n",
        "\n",
        "Computational Cost: Gini Impurity is generally faster to compute than Entropy.\n",
        "\n",
        "Impact on Tree Structure: While both metrics usually lead to similar tree structures, there might be slight variations in the chosen features and split points due to the differences in their calculations.\n",
        "\n",
        "In Practice:\n",
        "\n",
        "Both Gini Impurity and Entropy are widely used in Decision Trees and often produce similar results. Gini Impurity is often preferred for its computational efficiency, while Entropy is sometimes favored for its theoretical foundation in information theory. The choice between the two often depends on the specific"
      ],
      "metadata": {
        "id": "CMtoavgBCGtM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7 What is the mathematical explanation behind Decision Trees\n",
        "\n",
        "\n",
        "ans:-1. Entropy:\n",
        "\n",
        "Concept: Entropy measures the uncertainty or randomness in a set of data points. It quantifies the average amount of information needed to identify the class of a randomly chosen data point from the set.\n",
        "Formula: Entropy(S) = - Σ(pi * log2(pi)), where pi is the proportion of data points belonging to class i in the set S.\n",
        "Range: 0 (pure) to log2(k) (impure), where k is the number of classes.\n",
        "Interpretation: A lower Entropy value indicates a more pure node, meaning the data points within that node are more homogeneous in terms of their class labels.\n",
        "2. Information Gain:\n",
        "\n",
        "Concept: Information Gain measures the reduction in entropy achieved by splitting the data based on a particular feature. It helps determine which feature is the best to split the data on at each node of the tree.\n",
        "Formula: Information Gain (S, A) = Entropy(S) - Σ [ (|Sv| / |S|) * Entropy(Sv) ], where S is the original dataset, A is the feature being considered for the split, Sv is the subset of data where feature A has value v, |S| is the number of data points in the original dataset, and |Sv| is the number of data points in the subset Sv.\n",
        "Interpretation: The feature that results in the highest Information Gain is chosen as the best feature to split the data on at that node.\n",
        "3. Gini Impurity:\n",
        "\n",
        "Concept: Gini Impurity measures the probability of misclassifying a randomly chosen element in the dataset if it were randomly labeled according to the class distribution in the dataset.\n",
        "Formula: Gini(S) = 1 - Σ(pi)^2, where pi is the proportion of data points belonging to class i in the set S.\n",
        "Range: 0 (pure) to 1 (impure)\n",
        "Interpretation: A lower Gini Impurity value indicates a more pure node, meaning the data points within that node mostly belong to the same class.\n",
        "Building the Decision Tree:\n",
        "\n",
        "The Decision Tree algorithm uses these mathematical concepts to recursively split the data and build the tree structure. It starts with the entire dataset and selects the best attribute to split the data based on either Information Gain or Gini Impurity. It creates a node for this attribute and branches for each possible outcome of the test on that attribute. This process is recursively applied to each subset of data until a stopping condition is met (e.g., all data points in a node belong to the same class, a maximum depth is reached).\n",
        "\n",
        "Making Predictions:\n",
        "\n",
        "To classify a new data point, the algorithm starts at the root node and traverses the tree based on the values of the attributes in the data point. It follows the branches corresponding to the attribute values until it reaches a leaf node. The class label or numerical value associated with the leaf node is then assigned as the prediction for the data point."
      ],
      "metadata": {
        "id": "Prl9fs16CgpP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8 What is Pre-Pruning in Decision Trees\n",
        "\n",
        "\n",
        "ans:-Pre-Pruning, also known as early stopping, is a technique used in Decision Trees to prevent overfitting by stopping the tree growth before it becomes too complex. It involves setting certain criteria or limits during the tree construction process to prevent the tree from growing unnecessarily deep or creating too many branches.\n",
        "\n",
        "How does Pre-Pruning work?\n",
        "\n",
        "Pre-Pruning works by introducing stopping conditions that halt the tree growth process based on specific criteria. These criteria can include:\n",
        "\n",
        "Maximum Depth: Limiting the maximum depth of the tree.\n",
        "Minimum Number of Samples per Leaf: Specifying the minimum number of data points required to create a leaf node.\n",
        "Maximum Number of Leaf Nodes: Restricting the total number of leaf nodes in the tree.\n",
        "Minimum Impurity Decrease: Setting a threshold for the minimum reduction in impurity required for a split to occur.\n",
        "Benefits of Pre-Pruning:\n",
        "\n",
        "Reduces Overfitting: By stopping the tree growth early, pre-pruning helps prevent the tree from becoming too specific to the training data and overfitting.\n",
        "Improves Generalization: A simpler, less complex tree generated through pre-pruning is more likely to generalize well to unseen data.\n",
        "Reduces Computational Cost: Pre-pruning can reduce the computational cost of building the tree by stopping the growth process earlier.\n",
        "Example:\n",
        "\n",
        "Let's say we're building a Decision Tree and we set a maximum depth of 3. This means that the tree will not be allowed to grow beyond 3 levels. As the tree is being constructed, if a branch reaches the maximum depth, it will not be further split, even if there are potential features to split on. This helps prevent the tree from becoming too complex and overfitting the training data"
      ],
      "metadata": {
        "id": "OhP-qbQSCwNF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9 What is Post-Pruning in Decision Trees\n",
        "\n",
        "ans:=Post-Pruning is a technique used in Decision Trees to address overfitting by pruning or removing unnecessary branches from a fully grown tree. It's applied after the tree has been constructed using the training data.\n",
        "\n",
        "How does Post-Pruning work?\n",
        "\n",
        "Grow the Tree: First, the Decision Tree is allowed to grow fully without any restrictions, potentially leading to a complex tree that might overfit the training data.\n",
        "\n",
        "Prune the Tree: Then, the tree is pruned by removing or collapsing branches that do not significantly improve the model's performance on a validation dataset. This is typically done by replacing subtrees with leaf nodes or by merging adjacent leaf nodes.\n",
        "\n",
        "Evaluation: The pruning process is guided by evaluating the tree's performance on a validation dataset. Different pruning strategies and criteria can be used to determine which branches to remove.\n",
        "\n",
        "Common Post-Pruning techniques:\n",
        "\n",
        "Reduced Error Pruning: This technique iteratively removes branches that do not improve the tree's accuracy on a validation dataset.\n",
        "Cost-Complexity Pruning: This technique uses a cost-complexity parameter to balance the tree's size and accuracy, finding the optimal tree structure that minimizes both error and complexity.\n",
        "Minimum Error Pruning: This technique removes branches that do not increase the tree's accuracy on a validation dataset, prioritizing accuracy over complexity.\n",
        "Benefits of Post-Pruning:\n",
        "\n",
        "Reduces Overfitting: Post-pruning helps reduce overfitting by removing unnecessary branches that might be specific to the training data.\n",
        "Improves Generalization: By simplifying the tree structure, post-pruning improves the tree's ability to generalize to unseen data.\n",
        "Enhances Interpretability: Post-pruning can make the tree easier to understand and interpret by removing irrelevant or redundant branches."
      ],
      "metadata": {
        "id": "HVyd1AZtC9mY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10 What is the difference between Pre-Pruning and Post-Pruning\n",
        "\n",
        "\n",
        "ans:-Both Pre-Pruning and Post-Pruning are techniques used to address overfitting in Decision Trees, but they differ in their approach and timing:\n",
        "\n",
        "Pre-Pruning (Early Stopping):\n",
        "\n",
        "When it's applied: During the tree construction process, before the tree is fully grown.\n",
        "How it works: It stops the tree growth early by setting stopping criteria or limits, preventing the tree from becoming too complex.\n",
        "Focus: Preventing the tree from overfitting in the first place.\n",
        "Post-Pruning:\n",
        "\n",
        "When it's applied: After the tree has been fully grown using the training data.\n",
        "How it works: It removes or collapses unnecessary branches from the fully grown tree based on evaluation on a validation dataset.\n",
        "Focus: Simplifying the tree structure to reduce overfitting.\n",
        "Key Differences:\n",
        "\n",
        "Feature\tPre-Pruning\tPost-Pruning\n",
        "Timing\tDuring tree construction\tAfter tree construction\n",
        "Approach\tStops tree growth early\tRemoves branches from a fully grown tree\n",
        "Complexity\tSimpler to implement\tMore complex to implement\n",
        "Computational Cost\tGenerally less computationally expensive\tCan be more computationally expensive\n",
        "Risk of Underfitting\tHigher risk if stopping criteria are too restrictive\tLower risk as it starts with a fully grown tree\n",
        "Choosing between Pre-Pruning and Post-Pruning:\n",
        "\n",
        "Pre-Pruning is often preferred when computational resources are limited or when a simpler tree is desired. However, it can be challenging to determine the optimal stopping criteria, and there's a risk of underfitting if the criteria are too restrictive.\n",
        "\n",
        "Post-Pruning is generally considered more effective in reducing overfitting as it starts with a fully grown tree and then selectively removes unnecessary branches. However, it can be more computatio"
      ],
      "metadata": {
        "id": "R6030FhoDRuq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11 What is a Decision Tree Regressor\n",
        "\n",
        "\n",
        "ans:-A Decision Tree Regressor is a type of machine learning model that uses a tree-like structure to predict continuous numerical values (targets) instead of discrete classes (as in classification). It works by recursively partitioning the data based on features to create a tree where each leaf node represents a predicted value.\n",
        "\n",
        "How it works:\n",
        "\n",
        "Building the Tree: The algorithm starts with the entire dataset and selects the best feature to split the data based on a certain criterion (e.g., minimizing mean squared error).\n",
        "Splitting: It creates a node for this feature and branches for different ranges or values of the feature.\n",
        "Recursive Partitioning: This process is recursively applied to each subset of data until a stopping condition is met (e.g., a maximum depth is reached, a minimum number of samples per leaf is reached).\n",
        "Leaf Nodes: Each leaf node represents a predicted value, which is typically the average or median of the target values of the data points in that leaf.\n",
        "Making Predictions:\n",
        "\n",
        "To predict the target value for a new data point, the algorithm starts at the root node and traverses the tree based on the values of the features in the data point. It follows the branches corresponding to the feature values until it reaches a leaf node. The predicted value associated with the leaf node is then assigned as the prediction for the data point.\n",
        "\n",
        "Example:\n",
        "\n",
        "Imagine predicting house prices based on features like size, location, and number of bedrooms. A Decision Tree Regressor would create a tree where each node represents a decision based on a feature (e.g., \"is the size greater than 1500 sq ft?\"), and each leaf node represents a predicted price range (e.g., \"$200,000 - $$200,000 - $250,000\").\n",
        "\n",
        "Advantages:\n",
        "\n",
        "Easy to understand and interpret: The tree structure is visually intuitive and can be easily explained.\n",
        "Can handle both numerical and categorical data: Decision Trees can handle a variety of data types without requiring extensive preprocessing.\n",
        "Non-linear relationships: Decision Trees can capture non-linear relationships between features and the target variable.\n",
        "Disadvantages:\n",
        "\n",
        "Prone to overfitting: Decision Trees can become too complex and overfit the training data if not properly pruned.\n",
        "Instability: Small changes in the data can lead to significant changes in the tree structure, making them less stable than some other models."
      ],
      "metadata": {
        "id": "KC2rBTwFDogh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. What are the advantages and disadvantages of Decision Trees\n",
        "\n",
        "\n",
        "ans:-Advantages of Decision Trees:\n",
        "\n",
        "Easy to Understand and Interpret: Decision Trees are visually intuitive and can be easily understood by non-technical users. The tree structure clearly shows the decision-making process, making it easy to explain how predictions are made.\n",
        "\n",
        "Can Handle Both Numerical and Categorical Data: Decision Trees can handle a mix of numerical and categorical features without requiring extensive data preprocessing. This makes them versatile and adaptable to various datasets.\n",
        "\n",
        "Non-linear Relationships: Decision Trees can capture non-linear relationships between features and the target variable. They can learn complex decision boundaries that linear models might miss.\n",
        "\n",
        "Feature Importance: Decision Trees provide insights into feature importance, indicating which features are most influential in making predictions. This can be valuable for feature selection and understanding the data.\n",
        "\n",
        "Requires Little Data Preparation: Decision Trees often require minimal data preparation compared to other algorithms. They are less sensitive to outliers and missing values, making them robust in certain scenarios.\n",
        "\n",
        "Disadvantages of Decision Trees:\n",
        "\n",
        "Prone to Overfitting: Decision Trees can easily overfit the training data, especially if they are allowed to grow too deep and complex. This can lead to poor generalization performance on unseen data.\n",
        "\n",
        "Instability: Small changes in the data can lead to significant changes in the tree structure, making them less stable than some other models. This can make them unreliable in certain situations.\n",
        "\n",
        "Bias Towards Dominant Features: Decision Trees can be biased towards features with many categories or levels. This can lead to inaccurate predictions if the dataset is imbalanced or if some features are more dominant than others.\n",
        "\n",
        "Difficulty with Continuous Variables: While Decision Trees can handle continuous variables, they tend to create splits that are not always optimal. This can lead to less accurate predictions for continuous targets.\n",
        "\n",
        "Not Ideal for High-Dimensional Data: Decision Trees can struggle with datasets containing many features, as the tree structure can become overly complex and difficult to interpret."
      ],
      "metadata": {
        "id": "QN_T19nmDylk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13 How does a Decision Tree handle missing values\n",
        "\n",
        "\n",
        "\n",
        "ans:-During Training:\n",
        "\n",
        "Ignoring Missing Values: The simplest approach is to ignore data points with missing values for the specific feature being considered for a split. This can lead to information loss, especially if there are many missing values.\n",
        "\n",
        "Imputation with the Most Frequent Value: Missing values can be replaced with the most frequent value of that feature in the training data. This is a simple imputation method but might introduce bias.\n",
        "\n",
        "Imputation with the Average Value (for numerical features): For numerical features, missing values can be replaced with the average value of that feature in the training data.\n",
        "\n",
        "Surrogate Splits: This is a more sophisticated approach where the algorithm searches for alternative features that provide similar information to the feature with missing values. During prediction, if the primary feature is missing, the surrogate split is used to guide the decision.\n",
        "\n",
        "During Prediction:\n",
        "\n",
        "Using Surrogate Splits: If a data point has a missing value for a feature used in a split, the Decision Tree can follow a surrogate split based on another feature that provides similar information.\n",
        "\n",
        "Predicting with the Most Frequent Class in the Leaf Node: If the data point reaches a leaf node and has missing values for features used in previous splits, the prediction can be based on the most frequent class in that leaf node.\n",
        "\n",
        "Methods used by popular libraries:\n",
        "\n",
        "Scikit-learn (Python): Scikit-learn's DecisionTreeClassifier and DecisionTreeRegressor use surrogate splits to handle missing values during both training and prediction.\n",
        "\n",
        "R (rpart package): The rpart package in R uses surrogate splits to handle missing values."
      ],
      "metadata": {
        "id": "zTSKl0K-D91l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14 How does a Decision Tree handle categorical features\n",
        "\n",
        "\n",
        "ans:-During Training:\n",
        "\n",
        "Finding the Best Split: When evaluating a categorical feature for a split, the Decision Tree algorithm considers all possible ways to divide the categories into two or more groups. It searches for the split that maximizes information gain or minimizes impurity.\n",
        "\n",
        "Creating Branches: For each group of categories resulting from the split, a separate branch is created in the tree. This branch represents a specific combination of categories for that feature.\n",
        "\n",
        "Recursive Partitioning: This process is repeated recursively for each subset of data, considering both numerical and categorical features, until a stopping condition is met.\n",
        "\n",
        "During Prediction:\n",
        "\n",
        "Traversing the Tree: To predict the target value for a new data point, the algorithm starts at the root node and traverses the tree based on the values of both numerical and categorical features.\n",
        "\n",
        "Following Branches: For categorical features, the algorithm follows the branch corresponding to the specific category or combination of categories present in the data point.\n",
        "\n",
        "Reaching a Leaf Node: Once the algorithm reaches a leaf node, it assigns the predicted value associated with that leaf node to the data point.\n",
        "\n",
        "Methods for Handling Categorical Features:\n",
        "\n",
        "Binary Splits: For binary categorical features (e.g., yes/no, true/false), the split is straightforward, creating two branches for each category.\n",
        "\n",
        "Multi-way Splits: For categorical features with more than two categories, the algorithm considers all possible ways to group the categories. This can lead to a larger number of branches, potentially increasing the tree's complexity.\n",
        "\n",
        "Using Surrogate Splits (for missing values): If a data point has a missing value for a categorical feature used in a split, the Decision Tree can follow a surrogate split based on another feature that provides similar information."
      ],
      "metadata": {
        "id": "rinIFv2DEIPr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15 What are some real-world applications of Decision Trees?\n",
        "\n",
        "\n",
        "ans:-1. Customer Relationship Management (CRM):\n",
        "\n",
        "Customer Churn Prediction: Decision Trees can be used to identify customers who are likely to churn (cancel their subscriptions or switch to a competitor). By analyzing customer demographics, usage patterns, and interactions, the model can predict churn risk and enable proactive retention strategies.\n",
        "Customer Segmentation: Decision Trees can segment customers into different groups based on their characteristics and preferences. This helps businesses tailor marketing campaigns, product recommendations, and customer support strategies to specific customer segments.\n",
        "2. Healthcare:\n",
        "\n",
        "Disease Diagnosis: Decision Trees can assist in diagnosing diseases based on patient symptoms, medical history, and test results. By learning patterns from historical data, the model can predict the likelihood of a particular disease and aid healthcare professionals in making informed decisions.\n",
        "Treatment Recommendation: Decision Trees can recommend personalized treatment plans based on patient characteristics and disease severity. By considering various factors, the model can suggest the most effective treatment options, improving patient outcomes.\n",
        "3. Finance:\n",
        "\n",
        "Loan Approval: Decision Trees can assess the creditworthiness of loan applicants and predict the likelihood of loan default. By analyzing financial data, credit history, and other relevant factors, the model can automate loan approval decisions and reduce risk for lenders.\n",
        "Fraud Detection: Decision Trees can identify fraudulent transactions by detecting patterns and anomalies in financial data. By learning from historical fraud cases, the model can flag suspicious transactions and prevent financial losses.\n",
        "4. Marketing:\n",
        "\n",
        "Targeted Advertising: Decision Trees can help target advertising campaigns to specific customer segments based on their demographics, interests, and online behavior. By predicting which customers are most likely to respond to an ad, the model can improve campaign effectiveness and ROI.\n",
        "Product Recommendation: Decision Trees can recommend products to customers based on their past purchases, browsing history, and preferences. By learning customer preferences, the model can personalize recommendations and increase sales.\n",
        "5. Operations:\n",
        "\n",
        "Inventory Management: Decision Trees can optimize inventory levels by predicting future demand for products. By analyzing historical sales data, seasonality, and other factors, the model can help businesses avoid stockouts and minimize inventory holding costs.\n",
        "Supply Chain Optimization: Decision Trees can improve supply chain efficiency by predicting potential disruptions and identifying areas for improvement. By analyzing logistics data, supplier performance, and market trends, the model can help businesses streamline their operations."
      ],
      "metadata": {
        "id": "Qw4ZXDTXET7P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16 Write a Python program to train a Decision Tree Classifier on the Iris dataset and print the model accuracy"
      ],
      "metadata": {
        "id": "mYUNCE-AEgvo"
      }
    },
    {
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier()\n",
        "\n",
        "# Train the classifier\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the accuracy\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSbslAmCEp9I",
        "outputId": "29e8e330-33be-4ef4-98d6-7697a474b059"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "17 Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the\n",
        "feature importances*\n",
        "\n",
        "\n",
        "ans:-"
      ],
      "metadata": {
        "id": "Dy8XY3kuE1e8"
      }
    },
    {
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create Decision Tree Classifier with Gini Impurity\n",
        "clf = DecisionTreeClassifier(criterion=\"gini\", random_state=42)\n",
        "\n",
        "# Train the classifier\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importances\n",
        "importances = clf.feature_importances_\n",
        "\n",
        "# Print feature importances\n",
        "for i, importance in enumerate(importances):\n",
        "    print(f\"Feature {i}: {iris.feature_names[i]}, Importance: {importance}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I7w8BC9eFait",
        "outputId": "3c780051-8430-43a3-c7fd-edddd28ca474"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature 0: sepal length (cm), Importance: 0.0\n",
            "Feature 1: sepal width (cm), Importance: 0.01911001911001911\n",
            "Feature 2: petal length (cm), Importance: 0.8932635518001373\n",
            "Feature 3: petal width (cm), Importance: 0.08762642908984374\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "18 Write a Python program to train a Decision Tree Classifier using Entropy as the splitting criterion and print the\n",
        "model accuracy*"
      ],
      "metadata": {
        "id": "HV4lLiUKFcpi"
      }
    },
    {
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Classifier with Entropy as the criterion\n",
        "clf = DecisionTreeClassifier(criterion=\"entropy\", random_state=42)\n",
        "\n",
        "# Train the classifier\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the accuracy\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5h5DQvZFnXs",
        "outputId": "f2de9a66-3de2-408c-eea6-1403c9f95679"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9777777777777777\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "19 Write a Python program to train a Decision Tree Regressor on a housing dataset and evaluate using Mean\n",
        "Squared Error (MSE)*"
      ],
      "metadata": {
        "id": "UJurKwoCFou0"
      }
    },
    {
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the housing dataset (replace with your dataset path)\n",
        "data = pd.read_csv('housing.csv')\n",
        "\n",
        "# Select features (X) and target (y)\n",
        "X = data[['feature1', 'feature2', ...]]  # Replace with your feature columns\n",
        "y = data['target']  # Replace with your target column\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Regressor\n",
        "regressor = DecisionTreeRegressor(random_state=42)\n",
        "\n",
        "# Train the regressor\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "# Calculate the Mean Squared Error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "# Print the MSE\n",
        "print(\"Mean Squared Error:\", mse)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "q9WoEESTFxbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1SgsEOTcEqei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the housing dataset using the full file path\n",
        "data = pd.read_csv('data/housing.csv')  # Updated path\n",
        "\n",
        "# Select features (X) and target (y)\n",
        "X = data[['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']]  # Replace with your feature columns\n",
        "y = data['MEDV']  # Replace with your target column\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "xWfsL_2BF6Yc",
        "outputId": "b5f04330-0f72-4133-d8c2-00bfcb9276a9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "incomplete input (<ipython-input-5-85010e725a92>, line 14)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-85010e725a92>\"\u001b[0;36m, line \u001b[0;32m14\u001b[0m\n\u001b[0;31m    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0\u001b[0m\n\u001b[0m                                                                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I6-NYk1YF6sQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the housing dataset using the full file path\n",
        "data = pd.read_csv('data/housing.csv')  # Updated path\n",
        "\n",
        "# Select features (X) and target (y)\n",
        "X = data[['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']]  # Replace with your feature columns\n",
        "y = data['MEDV']  # Replace with your target column\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Regressor\n",
        "regressor = DecisionTreeRegressor(random_state=42)\n",
        "\n",
        "# Train the regressor\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "# Calculate the Mean Squared Error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "# Print the MSE\n",
        "print(\"Mean Squared Error:\", mse)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "id": "Tuii4sX9F7iy",
        "outputId": "b3c86b24-716b-43d1-a440-74c57032f5e1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'data/housing.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-a6027711e61b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Load the housing dataset using the full file path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/housing.csv'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Updated path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Select features (X) and target (y)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/housing.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "20 Write a Python program to train a Decision Tree Classifier and visualize the tree using graphviz"
      ],
      "metadata": {
        "id": "M9hpCODgF9jV"
      }
    },
    {
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
        "import graphviz\n",
        "import pydotplus\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Train the classifier\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Export the tree to a DOT file\n",
        "dot_data = export_graphviz(clf, out_file=None,\n",
        "                         feature_names=iris.feature_names,\n",
        "                         class_names=iris.target_names,\n",
        "                         filled=True, rounded=True,\n",
        "                         special_characters=True)\n",
        "\n",
        "# Create a graphviz object from the DOT data\n",
        "graph = graphviz.Source(dot_data)\n",
        "\n",
        "# Render the graph to an image file (optional)\n",
        "# graph.render(\"iris_decision_tree\")\n",
        "\n",
        "# Display the graph in the notebook\n",
        "display(graph)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0pu9uZHfGJVl",
        "outputId": "31aae172-d042-4b8e-8e85-52f0d7a8d2a1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.43.0 (0)\n -->\n<!-- Title: Tree Pages: 1 -->\n<svg width=\"749pt\" height=\"790pt\"\n viewBox=\"0.00 0.00 749.00 790.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 786)\">\n<title>Tree</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-786 745,-786 745,4 -4,4\"/>\n<!-- 0 -->\n<g id=\"node1\" class=\"node\">\n<title>0</title>\n<path fill=\"#ffffff\" stroke=\"black\" d=\"M265,-782C265,-782 130,-782 130,-782 124,-782 118,-776 118,-770 118,-770 118,-711 118,-711 118,-705 124,-699 130,-699 130,-699 265,-699 265,-699 271,-699 277,-705 277,-711 277,-711 277,-770 277,-770 277,-776 271,-782 265,-782\"/>\n<text text-anchor=\"start\" x=\"126\" y=\"-766.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal length (cm) ≤ 2.45</text>\n<text text-anchor=\"start\" x=\"162\" y=\"-751.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.664</text>\n<text text-anchor=\"start\" x=\"152.5\" y=\"-736.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 105</text>\n<text text-anchor=\"start\" x=\"139.5\" y=\"-721.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [31, 37, 37]</text>\n<text text-anchor=\"start\" x=\"145\" y=\"-706.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 1 -->\n<g id=\"node2\" class=\"node\">\n<title>1</title>\n<path fill=\"#e58139\" stroke=\"black\" d=\"M166,-655.5C166,-655.5 73,-655.5 73,-655.5 67,-655.5 61,-649.5 61,-643.5 61,-643.5 61,-599.5 61,-599.5 61,-593.5 67,-587.5 73,-587.5 73,-587.5 166,-587.5 166,-587.5 172,-587.5 178,-593.5 178,-599.5 178,-599.5 178,-643.5 178,-643.5 178,-649.5 172,-655.5 166,-655.5\"/>\n<text text-anchor=\"start\" x=\"91.5\" y=\"-640.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"78.5\" y=\"-625.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 31</text>\n<text text-anchor=\"start\" x=\"69\" y=\"-610.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [31, 0, 0]</text>\n<text text-anchor=\"start\" x=\"76\" y=\"-595.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = setosa</text>\n</g>\n<!-- 0&#45;&gt;1 -->\n<g id=\"edge1\" class=\"edge\">\n<title>0&#45;&gt;1</title>\n<path fill=\"none\" stroke=\"black\" d=\"M170.44,-698.91C162.93,-687.65 154.78,-675.42 147.24,-664.11\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"150.07,-662.05 141.61,-655.67 144.25,-665.93 150.07,-662.05\"/>\n<text text-anchor=\"middle\" x=\"136.71\" y=\"-676.48\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">True</text>\n</g>\n<!-- 2 -->\n<g id=\"node3\" class=\"node\">\n<title>2</title>\n<path fill=\"#ffffff\" stroke=\"black\" d=\"M343,-663C343,-663 208,-663 208,-663 202,-663 196,-657 196,-651 196,-651 196,-592 196,-592 196,-586 202,-580 208,-580 208,-580 343,-580 343,-580 349,-580 355,-586 355,-592 355,-592 355,-651 355,-651 355,-657 349,-663 343,-663\"/>\n<text text-anchor=\"start\" x=\"204\" y=\"-647.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal length (cm) ≤ 4.75</text>\n<text text-anchor=\"start\" x=\"247.5\" y=\"-632.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.5</text>\n<text text-anchor=\"start\" x=\"234.5\" y=\"-617.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 74</text>\n<text text-anchor=\"start\" x=\"221\" y=\"-602.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 37, 37]</text>\n<text text-anchor=\"start\" x=\"223\" y=\"-587.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 0&#45;&gt;2 -->\n<g id=\"edge2\" class=\"edge\">\n<title>0&#45;&gt;2</title>\n<path fill=\"none\" stroke=\"black\" d=\"M224.56,-698.91C230.43,-690.1 236.7,-680.7 242.76,-671.61\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"245.85,-673.28 248.49,-663.02 240.03,-669.4 245.85,-673.28\"/>\n<text text-anchor=\"middle\" x=\"253.39\" y=\"-683.84\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">False</text>\n</g>\n<!-- 3 -->\n<g id=\"node4\" class=\"node\">\n<title>3</title>\n<path fill=\"#3fe685\" stroke=\"black\" d=\"M252.5,-544C252.5,-544 130.5,-544 130.5,-544 124.5,-544 118.5,-538 118.5,-532 118.5,-532 118.5,-473 118.5,-473 118.5,-467 124.5,-461 130.5,-461 130.5,-461 252.5,-461 252.5,-461 258.5,-461 264.5,-467 264.5,-473 264.5,-473 264.5,-532 264.5,-532 264.5,-538 258.5,-544 252.5,-544\"/>\n<text text-anchor=\"start\" x=\"126.5\" y=\"-528.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal width (cm) ≤ 1.6</text>\n<text text-anchor=\"start\" x=\"156\" y=\"-513.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.059</text>\n<text text-anchor=\"start\" x=\"150.5\" y=\"-498.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 33</text>\n<text text-anchor=\"start\" x=\"141\" y=\"-483.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 32, 1]</text>\n<text text-anchor=\"start\" x=\"139\" y=\"-468.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 2&#45;&gt;3 -->\n<g id=\"edge3\" class=\"edge\">\n<title>2&#45;&gt;3</title>\n<path fill=\"none\" stroke=\"black\" d=\"M246.36,-579.91C239.97,-571.01 233.15,-561.51 226.56,-552.33\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"229.27,-550.1 220.59,-544.02 223.58,-554.19 229.27,-550.1\"/>\n</g>\n<!-- 6 -->\n<g id=\"node7\" class=\"node\">\n<title>6</title>\n<path fill=\"#9254e9\" stroke=\"black\" d=\"M424.5,-544C424.5,-544 294.5,-544 294.5,-544 288.5,-544 282.5,-538 282.5,-532 282.5,-532 282.5,-473 282.5,-473 282.5,-467 288.5,-461 294.5,-461 294.5,-461 424.5,-461 424.5,-461 430.5,-461 436.5,-467 436.5,-473 436.5,-473 436.5,-532 436.5,-532 436.5,-538 430.5,-544 424.5,-544\"/>\n<text text-anchor=\"start\" x=\"290.5\" y=\"-528.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal width (cm) ≤ 1.75</text>\n<text text-anchor=\"start\" x=\"324\" y=\"-513.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.214</text>\n<text text-anchor=\"start\" x=\"318.5\" y=\"-498.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 41</text>\n<text text-anchor=\"start\" x=\"309\" y=\"-483.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 5, 36]</text>\n<text text-anchor=\"start\" x=\"311\" y=\"-468.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 2&#45;&gt;6 -->\n<g id=\"edge6\" class=\"edge\">\n<title>2&#45;&gt;6</title>\n<path fill=\"none\" stroke=\"black\" d=\"M304.64,-579.91C311.03,-571.01 317.85,-561.51 324.44,-552.33\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"327.42,-554.19 330.41,-544.02 321.73,-550.1 327.42,-554.19\"/>\n</g>\n<!-- 4 -->\n<g id=\"node5\" class=\"node\">\n<title>4</title>\n<path fill=\"#39e581\" stroke=\"black\" d=\"M109,-417.5C109,-417.5 12,-417.5 12,-417.5 6,-417.5 0,-411.5 0,-405.5 0,-405.5 0,-361.5 0,-361.5 0,-355.5 6,-349.5 12,-349.5 12,-349.5 109,-349.5 109,-349.5 115,-349.5 121,-355.5 121,-361.5 121,-361.5 121,-405.5 121,-405.5 121,-411.5 115,-417.5 109,-417.5\"/>\n<text text-anchor=\"start\" x=\"32.5\" y=\"-402.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"19.5\" y=\"-387.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 32</text>\n<text text-anchor=\"start\" x=\"10\" y=\"-372.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 32, 0]</text>\n<text text-anchor=\"start\" x=\"8\" y=\"-357.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 3&#45;&gt;4 -->\n<g id=\"edge4\" class=\"edge\">\n<title>3&#45;&gt;4</title>\n<path fill=\"none\" stroke=\"black\" d=\"M146.05,-460.91C132.83,-449.1 118.4,-436.22 105.23,-424.45\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"107.43,-421.72 97.64,-417.67 102.76,-426.94 107.43,-421.72\"/>\n</g>\n<!-- 5 -->\n<g id=\"node6\" class=\"node\">\n<title>5</title>\n<path fill=\"#8139e5\" stroke=\"black\" d=\"M240,-417.5C240,-417.5 151,-417.5 151,-417.5 145,-417.5 139,-411.5 139,-405.5 139,-405.5 139,-361.5 139,-361.5 139,-355.5 145,-349.5 151,-349.5 151,-349.5 240,-349.5 240,-349.5 246,-349.5 252,-355.5 252,-361.5 252,-361.5 252,-405.5 252,-405.5 252,-411.5 246,-417.5 240,-417.5\"/>\n<text text-anchor=\"start\" x=\"167.5\" y=\"-402.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"158\" y=\"-387.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 1</text>\n<text text-anchor=\"start\" x=\"148.5\" y=\"-372.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 0, 1]</text>\n<text text-anchor=\"start\" x=\"147\" y=\"-357.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 3&#45;&gt;5 -->\n<g id=\"edge5\" class=\"edge\">\n<title>3&#45;&gt;5</title>\n<path fill=\"none\" stroke=\"black\" d=\"M192.89,-460.91C193.25,-450.2 193.65,-438.62 194.02,-427.78\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"197.52,-427.78 194.37,-417.67 190.53,-427.54 197.52,-427.78\"/>\n</g>\n<!-- 7 -->\n<g id=\"node8\" class=\"node\">\n<title>7</title>\n<path fill=\"#ffffff\" stroke=\"black\" d=\"M424,-425C424,-425 289,-425 289,-425 283,-425 277,-419 277,-413 277,-413 277,-354 277,-354 277,-348 283,-342 289,-342 289,-342 424,-342 424,-342 430,-342 436,-348 436,-354 436,-354 436,-413 436,-413 436,-419 430,-425 424,-425\"/>\n<text text-anchor=\"start\" x=\"285\" y=\"-409.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal length (cm) ≤ 4.95</text>\n<text text-anchor=\"start\" x=\"328.5\" y=\"-394.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.5</text>\n<text text-anchor=\"start\" x=\"319\" y=\"-379.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 8</text>\n<text text-anchor=\"start\" x=\"309.5\" y=\"-364.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 4, 4]</text>\n<text text-anchor=\"start\" x=\"304\" y=\"-349.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 6&#45;&gt;7 -->\n<g id=\"edge7\" class=\"edge\">\n<title>6&#45;&gt;7</title>\n<path fill=\"none\" stroke=\"black\" d=\"M358.46,-460.91C358.25,-452.56 358.02,-443.67 357.8,-435.02\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"361.29,-434.93 357.54,-425.02 354.3,-435.11 361.29,-434.93\"/>\n</g>\n<!-- 14 -->\n<g id=\"node15\" class=\"node\">\n<title>14</title>\n<path fill=\"#853fe6\" stroke=\"black\" d=\"M601,-425C601,-425 466,-425 466,-425 460,-425 454,-419 454,-413 454,-413 454,-354 454,-354 454,-348 460,-342 466,-342 466,-342 601,-342 601,-342 607,-342 613,-348 613,-354 613,-354 613,-413 613,-413 613,-419 607,-425 601,-425\"/>\n<text text-anchor=\"start\" x=\"462\" y=\"-409.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal length (cm) ≤ 4.85</text>\n<text text-anchor=\"start\" x=\"498\" y=\"-394.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.059</text>\n<text text-anchor=\"start\" x=\"492.5\" y=\"-379.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 33</text>\n<text text-anchor=\"start\" x=\"483\" y=\"-364.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 1, 32]</text>\n<text text-anchor=\"start\" x=\"485\" y=\"-349.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 6&#45;&gt;14 -->\n<g id=\"edge14\" class=\"edge\">\n<title>6&#45;&gt;14</title>\n<path fill=\"none\" stroke=\"black\" d=\"M419.87,-460.91C434.31,-451.2 449.83,-440.76 464.63,-430.81\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"466.89,-433.51 473.24,-425.02 462.99,-427.7 466.89,-433.51\"/>\n</g>\n<!-- 8 -->\n<g id=\"node9\" class=\"node\">\n<title>8</title>\n<path fill=\"#39e581\" stroke=\"black\" d=\"M252,-298.5C252,-298.5 155,-298.5 155,-298.5 149,-298.5 143,-292.5 143,-286.5 143,-286.5 143,-242.5 143,-242.5 143,-236.5 149,-230.5 155,-230.5 155,-230.5 252,-230.5 252,-230.5 258,-230.5 264,-236.5 264,-242.5 264,-242.5 264,-286.5 264,-286.5 264,-292.5 258,-298.5 252,-298.5\"/>\n<text text-anchor=\"start\" x=\"175.5\" y=\"-283.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"166\" y=\"-268.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 2</text>\n<text text-anchor=\"start\" x=\"156.5\" y=\"-253.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 2, 0]</text>\n<text text-anchor=\"start\" x=\"151\" y=\"-238.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 7&#45;&gt;8 -->\n<g id=\"edge8\" class=\"edge\">\n<title>7&#45;&gt;8</title>\n<path fill=\"none\" stroke=\"black\" d=\"M303.42,-341.91C287.69,-329.88 270.5,-316.73 254.88,-304.79\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"256.94,-301.96 246.87,-298.67 252.69,-307.52 256.94,-301.96\"/>\n</g>\n<!-- 9 -->\n<g id=\"node10\" class=\"node\">\n<title>9</title>\n<path fill=\"#c09cf2\" stroke=\"black\" d=\"M424.5,-306C424.5,-306 294.5,-306 294.5,-306 288.5,-306 282.5,-300 282.5,-294 282.5,-294 282.5,-235 282.5,-235 282.5,-229 288.5,-223 294.5,-223 294.5,-223 424.5,-223 424.5,-223 430.5,-223 436.5,-229 436.5,-235 436.5,-235 436.5,-294 436.5,-294 436.5,-300 430.5,-306 424.5,-306\"/>\n<text text-anchor=\"start\" x=\"290.5\" y=\"-290.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal width (cm) ≤ 1.55</text>\n<text text-anchor=\"start\" x=\"324\" y=\"-275.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.444</text>\n<text text-anchor=\"start\" x=\"322\" y=\"-260.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 6</text>\n<text text-anchor=\"start\" x=\"312.5\" y=\"-245.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 2, 4]</text>\n<text text-anchor=\"start\" x=\"311\" y=\"-230.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 7&#45;&gt;9 -->\n<g id=\"edge9\" class=\"edge\">\n<title>7&#45;&gt;9</title>\n<path fill=\"none\" stroke=\"black\" d=\"M357.54,-341.91C357.75,-333.56 357.98,-324.67 358.2,-316.02\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"361.7,-316.11 358.46,-306.02 354.71,-315.93 361.7,-316.11\"/>\n</g>\n<!-- 10 -->\n<g id=\"node11\" class=\"node\">\n<title>10</title>\n<path fill=\"#8139e5\" stroke=\"black\" d=\"M254,-179.5C254,-179.5 165,-179.5 165,-179.5 159,-179.5 153,-173.5 153,-167.5 153,-167.5 153,-123.5 153,-123.5 153,-117.5 159,-111.5 165,-111.5 165,-111.5 254,-111.5 254,-111.5 260,-111.5 266,-117.5 266,-123.5 266,-123.5 266,-167.5 266,-167.5 266,-173.5 260,-179.5 254,-179.5\"/>\n<text text-anchor=\"start\" x=\"181.5\" y=\"-164.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"172\" y=\"-149.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 3</text>\n<text text-anchor=\"start\" x=\"162.5\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 0, 3]</text>\n<text text-anchor=\"start\" x=\"161\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 9&#45;&gt;10 -->\n<g id=\"edge10\" class=\"edge\">\n<title>9&#45;&gt;10</title>\n<path fill=\"none\" stroke=\"black\" d=\"M307.46,-222.91C292.18,-210.99 275.49,-197.98 260.29,-186.12\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"262.06,-183.06 252.02,-179.67 257.75,-188.58 262.06,-183.06\"/>\n</g>\n<!-- 11 -->\n<g id=\"node12\" class=\"node\">\n<title>11</title>\n<path fill=\"#9cf2c0\" stroke=\"black\" d=\"M431,-187C431,-187 296,-187 296,-187 290,-187 284,-181 284,-175 284,-175 284,-116 284,-116 284,-110 290,-104 296,-104 296,-104 431,-104 431,-104 437,-104 443,-110 443,-116 443,-116 443,-175 443,-175 443,-181 437,-187 431,-187\"/>\n<text text-anchor=\"start\" x=\"292\" y=\"-171.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal length (cm) ≤ 5.45</text>\n<text text-anchor=\"start\" x=\"328\" y=\"-156.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.444</text>\n<text text-anchor=\"start\" x=\"326\" y=\"-141.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 3</text>\n<text text-anchor=\"start\" x=\"316.5\" y=\"-126.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 2, 1]</text>\n<text text-anchor=\"start\" x=\"311\" y=\"-111.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 9&#45;&gt;11 -->\n<g id=\"edge11\" class=\"edge\">\n<title>9&#45;&gt;11</title>\n<path fill=\"none\" stroke=\"black\" d=\"M360.89,-222.91C361.17,-214.56 361.48,-205.67 361.77,-197.02\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"365.27,-197.13 362.11,-187.02 358.27,-196.9 365.27,-197.13\"/>\n</g>\n<!-- 12 -->\n<g id=\"node13\" class=\"node\">\n<title>12</title>\n<path fill=\"#39e581\" stroke=\"black\" d=\"M345,-68C345,-68 248,-68 248,-68 242,-68 236,-62 236,-56 236,-56 236,-12 236,-12 236,-6 242,0 248,0 248,0 345,0 345,0 351,0 357,-6 357,-12 357,-12 357,-56 357,-56 357,-62 351,-68 345,-68\"/>\n<text text-anchor=\"start\" x=\"268.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"259\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 2</text>\n<text text-anchor=\"start\" x=\"249.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 2, 0]</text>\n<text text-anchor=\"start\" x=\"244\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 11&#45;&gt;12 -->\n<g id=\"edge12\" class=\"edge\">\n<title>11&#45;&gt;12</title>\n<path fill=\"none\" stroke=\"black\" d=\"M338.55,-103.73C333.19,-94.97 327.52,-85.7 322.14,-76.91\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"325.08,-75 316.88,-68.3 319.11,-78.66 325.08,-75\"/>\n</g>\n<!-- 13 -->\n<g id=\"node14\" class=\"node\">\n<title>13</title>\n<path fill=\"#8139e5\" stroke=\"black\" d=\"M476,-68C476,-68 387,-68 387,-68 381,-68 375,-62 375,-56 375,-56 375,-12 375,-12 375,-6 381,0 387,0 387,0 476,0 476,0 482,0 488,-6 488,-12 488,-12 488,-56 488,-56 488,-62 482,-68 476,-68\"/>\n<text text-anchor=\"start\" x=\"403.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"394\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 1</text>\n<text text-anchor=\"start\" x=\"384.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 0, 1]</text>\n<text text-anchor=\"start\" x=\"383\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 11&#45;&gt;13 -->\n<g id=\"edge13\" class=\"edge\">\n<title>11&#45;&gt;13</title>\n<path fill=\"none\" stroke=\"black\" d=\"M388.82,-103.73C394.26,-94.97 400.01,-85.7 405.48,-76.91\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"408.52,-78.64 410.82,-68.3 402.57,-74.95 408.52,-78.64\"/>\n</g>\n<!-- 15 -->\n<g id=\"node16\" class=\"node\">\n<title>15</title>\n<path fill=\"#c09cf2\" stroke=\"black\" d=\"M594,-306C594,-306 469,-306 469,-306 463,-306 457,-300 457,-294 457,-294 457,-235 457,-235 457,-229 463,-223 469,-223 469,-223 594,-223 594,-223 600,-223 606,-229 606,-235 606,-235 606,-294 606,-294 606,-300 600,-306 594,-306\"/>\n<text text-anchor=\"start\" x=\"465\" y=\"-290.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">sepal width (cm) ≤ 3.1</text>\n<text text-anchor=\"start\" x=\"496\" y=\"-275.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.444</text>\n<text text-anchor=\"start\" x=\"494\" y=\"-260.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 3</text>\n<text text-anchor=\"start\" x=\"484.5\" y=\"-245.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 1, 2]</text>\n<text text-anchor=\"start\" x=\"483\" y=\"-230.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 14&#45;&gt;15 -->\n<g id=\"edge15\" class=\"edge\">\n<title>14&#45;&gt;15</title>\n<path fill=\"none\" stroke=\"black\" d=\"M532.81,-341.91C532.66,-333.56 532.51,-324.67 532.36,-316.02\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"535.86,-315.96 532.19,-306.02 528.86,-316.08 535.86,-315.96\"/>\n</g>\n<!-- 18 -->\n<g id=\"node19\" class=\"node\">\n<title>18</title>\n<path fill=\"#8139e5\" stroke=\"black\" d=\"M729,-298.5C729,-298.5 636,-298.5 636,-298.5 630,-298.5 624,-292.5 624,-286.5 624,-286.5 624,-242.5 624,-242.5 624,-236.5 630,-230.5 636,-230.5 636,-230.5 729,-230.5 729,-230.5 735,-230.5 741,-236.5 741,-242.5 741,-242.5 741,-286.5 741,-286.5 741,-292.5 735,-298.5 729,-298.5\"/>\n<text text-anchor=\"start\" x=\"654.5\" y=\"-283.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"641.5\" y=\"-268.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 30</text>\n<text text-anchor=\"start\" x=\"632\" y=\"-253.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 0, 30]</text>\n<text text-anchor=\"start\" x=\"634\" y=\"-238.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 14&#45;&gt;18 -->\n<g id=\"edge18\" class=\"edge\">\n<title>14&#45;&gt;18</title>\n<path fill=\"none\" stroke=\"black\" d=\"M585.19,-341.91C600.37,-329.99 616.95,-316.98 632.04,-305.12\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"634.56,-307.6 640.26,-298.67 630.24,-302.09 634.56,-307.6\"/>\n</g>\n<!-- 16 -->\n<g id=\"node17\" class=\"node\">\n<title>16</title>\n<path fill=\"#8139e5\" stroke=\"black\" d=\"M573,-179.5C573,-179.5 484,-179.5 484,-179.5 478,-179.5 472,-173.5 472,-167.5 472,-167.5 472,-123.5 472,-123.5 472,-117.5 478,-111.5 484,-111.5 484,-111.5 573,-111.5 573,-111.5 579,-111.5 585,-117.5 585,-123.5 585,-123.5 585,-167.5 585,-167.5 585,-173.5 579,-179.5 573,-179.5\"/>\n<text text-anchor=\"start\" x=\"500.5\" y=\"-164.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"491\" y=\"-149.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 2</text>\n<text text-anchor=\"start\" x=\"481.5\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 0, 2]</text>\n<text text-anchor=\"start\" x=\"480\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 15&#45;&gt;16 -->\n<g id=\"edge16\" class=\"edge\">\n<title>15&#45;&gt;16</title>\n<path fill=\"none\" stroke=\"black\" d=\"M530.46,-222.91C530.18,-212.2 529.89,-200.62 529.61,-189.78\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"533.11,-189.57 529.35,-179.67 526.11,-189.75 533.11,-189.57\"/>\n</g>\n<!-- 17 -->\n<g id=\"node18\" class=\"node\">\n<title>17</title>\n<path fill=\"#39e581\" stroke=\"black\" d=\"M712,-179.5C712,-179.5 615,-179.5 615,-179.5 609,-179.5 603,-173.5 603,-167.5 603,-167.5 603,-123.5 603,-123.5 603,-117.5 609,-111.5 615,-111.5 615,-111.5 712,-111.5 712,-111.5 718,-111.5 724,-117.5 724,-123.5 724,-123.5 724,-167.5 724,-167.5 724,-173.5 718,-179.5 712,-179.5\"/>\n<text text-anchor=\"start\" x=\"635.5\" y=\"-164.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"626\" y=\"-149.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 1</text>\n<text text-anchor=\"start\" x=\"616.5\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 1, 0]</text>\n<text text-anchor=\"start\" x=\"611\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 15&#45;&gt;17 -->\n<g id=\"edge17\" class=\"edge\">\n<title>15&#45;&gt;17</title>\n<path fill=\"none\" stroke=\"black\" d=\"M577.3,-222.91C590.62,-211.1 605.15,-198.22 618.43,-186.45\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"620.92,-188.92 626.08,-179.67 616.28,-183.68 620.92,-188.92\"/>\n</g>\n</g>\n</svg>\n",
            "text/plain": [
              "<graphviz.sources.Source at 0x79621673ded0>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "21 Write a Python program to train a Decision Tree Classifier with a maximum depth of 3 and compare its\n",
        "accuracy with a fully grown tree"
      ],
      "metadata": {
        "id": "ERBKfghxGOe4"
      }
    },
    {
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Classifier with maximum depth 3\n",
        "clf_depth3 = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "\n",
        "# Create a fully grown Decision Tree Classifier\n",
        "clf_full = DecisionTreeClassifier(random_state=42)  # No max_depth specified\n",
        "\n",
        "# Train the classifiers\n",
        "clf_depth3.fit(X_train, y_train)\n",
        "clf_full.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_depth3 = clf_depth3.predict(X_test)\n",
        "y_pred_full = clf_full.predict(X_test)\n",
        "\n",
        "# Calculate the accuracies\n",
        "accuracy_depth3 = accuracy_score(y_test, y_pred_depth3)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "# Print the accuracies\n",
        "print(\"Accuracy (max_depth=3):\", accuracy_depth3)\n",
        "print(\"Accuracy (fully grown):\", accuracy_full)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8k3lXsJ0Gb2e",
        "outputId": "bef73bd3-de01-4c26-af41-5894a866ef61"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (max_depth=3): 1.0\n",
            "Accuracy (fully grown): 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "22.Write a Python program to train a Decision Tree Classifier using min_samples_split=5 and compare its\n",
        "accuracy with a default tree"
      ],
      "metadata": {
        "id": "sPsSLa4BGdEC"
      }
    },
    {
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Classifier with min_samples_split=5\n",
        "clf_min_split = DecisionTreeClassifier(min_samples_split=5, random_state=42)\n",
        "\n",
        "# Create a default Decision Tree Classifier\n",
        "clf_default = DecisionTreeClassifier(random_state=42)  # No min_samples_split specified\n",
        "\n",
        "# Train the classifiers\n",
        "clf_min_split.fit(X_train, y_train)\n",
        "clf_default.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_min_split = clf_min_split.predict(X_test)\n",
        "y_pred_default = clf_default.predict(X_test)\n",
        "\n",
        "# Calculate the accuracies\n",
        "accuracy_min_split = accuracy_score(y_test, y_pred_min_split)\n",
        "accuracy_default = accuracy_score(y_test, y_pred_default)\n",
        "\n",
        "# Print the accuracies\n",
        "print(\"Accuracy (min_samples_split=5):\", accuracy_min_split)\n",
        "print(\"Accuracy (default):\", accuracy_default)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUzFWScOGmvp",
        "outputId": "e2da41c0-471d-4d08-a7e1-0c0c21e356de"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (min_samples_split=5): 1.0\n",
            "Accuracy (default): 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. Write a Python program to apply feature scaling before training a Decision Tree Classifier and compare its\n",
        "accuracy with unscaled data*"
      ],
      "metadata": {
        "id": "BrR_10OoGn6_"
      }
    },
    {
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Classifier (unscaled data)\n",
        "clf_unscaled = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Train the classifier on unscaled data\n",
        "clf_unscaled.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set (unscaled data)\n",
        "y_pred_unscaled = clf_unscaled.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy (unscaled data)\n",
        "accuracy_unscaled = accuracy_score(y_test, y_pred_unscaled)\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# Apply feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Create a Decision Tree Classifier (scaled data)\n",
        "clf_scaled = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Train the classifier on scaled data\n",
        "clf_scaled.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions on the test set (scaled data)\n",
        "y_pred_scaled = clf_scaled.predict(X_test_scaled)\n",
        "\n",
        "# Calculate the accuracy (scaled data)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# Print the accuracies\n",
        "print(\"Accuracy (unscaled data):\", accuracy_unscaled)\n",
        "print(\"Accuracy (scaled data):\", accuracy_scaled)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCzccnQAG2WZ",
        "outputId": "43c046ae-a35e-48f3-d908-375c7eb4b964"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (unscaled data): 1.0\n",
            "Accuracy (scaled data): 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. Write a Python program to train a Decision Tree Classifier using One-vs-Rest (OvR) strategy for multiclass\n",
        "classification*"
      ],
      "metadata": {
        "id": "n56PzntqG3mJ"
      }
    },
    {
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Classifier as the base estimator\n",
        "base_estimator = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Create an OvR classifier using the Decision Tree\n",
        "ovr_clf = OneVsRestClassifier(base_estimator)\n",
        "\n",
        "# Train the OvR classifier\n",
        "ovr_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = ovr_clf.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the accuracy\n",
        "print(\"Accuracy (One-vs-Rest):\", accuracy)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HG1tpbbHDgx",
        "outputId": "179516d7-5c38-465d-8e78-6623b65aeae0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (One-vs-Rest): 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. Write a Python program to train a Decision Tree Classifier and display the feature importance scores*"
      ],
      "metadata": {
        "id": "PTLdNdx1HElT"
      }
    },
    {
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Train the classifier\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importances\n",
        "importances = clf.feature_importances_\n",
        "\n",
        "# Print feature importances\n",
        "for i, importance in enumerate(importances):\n",
        "    print(f\"Feature {i}: {iris.feature_names[i]}, Importance: {importance}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGZGELRBHRlC",
        "outputId": "d3fb7e38-5969-48a3-9fd4-a3aae544859d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature 0: sepal length (cm), Importance: 0.0\n",
            "Feature 1: sepal width (cm), Importance: 0.01911001911001911\n",
            "Feature 2: petal length (cm), Importance: 0.8932635518001373\n",
            "Feature 3: petal width (cm), Importance: 0.08762642908984374\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "26 Write a Python program to train a Decision Tree Regressor with max_depth=5 and compare its performance\n",
        "with an unrestricted tree*"
      ],
      "metadata": {
        "id": "9vfIvPJhHTwq"
      }
    },
    {
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the Boston Housing dataset\n",
        "boston = load_boston()\n",
        "X = boston.data\n",
        "y = boston.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Regressor with max_depth=5\n",
        "regressor_depth5 = DecisionTreeRegressor(max_depth=5, random_state=42)\n",
        "\n",
        "# Create an unrestricted Decision Tree Regressor\n",
        "regressor_unrestricted = DecisionTreeRegressor(random_state=42)  # No max_depth specified\n",
        "\n",
        "# Train the regressors\n",
        "regressor_depth5.fit(X_train, y_train)\n",
        "regressor_unrestricted.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_depth5 = regressor_depth5.predict(X_test)\n",
        "y_pred_unrestricted = regressor_unrestricted.predict(X_test)\n",
        "\n",
        "# Calculate the Mean Squared Error (MSE)\n",
        "mse_depth5 = mean_squared_error(y_test, y_pred_depth5)\n",
        "mse_unrestricted = mean_squared_error(y_test, y_pred_unrestricted)\n",
        "\n",
        "# Print the MSE values\n",
        "print(\"MSE (max_depth=5):\", mse_depth5)\n",
        "print(\"MSE (unrestricted):\", mse_unrestricted)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MK-79LIpHgQ3",
        "outputId": "70a05fb0-c3f2-4280-a77f-e206ef8eadf8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "\n`load_boston` has been removed from scikit-learn since version 1.2.\n\nThe Boston housing prices dataset has an ethical problem: as\ninvestigated in [1], the authors of this dataset engineered a\nnon-invertible variable \"B\" assuming that racial self-segregation had a\npositive impact on house prices [2]. Furthermore the goal of the\nresearch that led to the creation of this dataset was to study the\nimpact of air quality but it did not give adequate demonstration of the\nvalidity of this assumption.\n\nThe scikit-learn maintainers therefore strongly discourage the use of\nthis dataset unless the purpose of the code is to study and educate\nabout ethical issues in data science and machine learning.\n\nIn this special case, you can fetch the dataset from the original\nsource::\n\n    import pandas as pd\n    import numpy as np\n\n    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n    target = raw_df.values[1::2, 2]\n\nAlternative datasets include the California housing dataset and the\nAmes housing dataset. You can load the datasets as follows::\n\n    from sklearn.datasets import fetch_california_housing\n    housing = fetch_california_housing()\n\nfor the California housing dataset and::\n\n    from sklearn.datasets import fetch_openml\n    housing = fetch_openml(name=\"house_prices\", as_frame=True)\n\nfor the Ames housing dataset.\n\n[1] M Carlisle.\n\"Racist data destruction?\"\n<https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>\n\n[2] Harrison Jr, David, and Daniel L. Rubinfeld.\n\"Hedonic housing prices and the demand for clean air.\"\nJournal of environmental economics and management 5.1 (1978): 81-102.\n<https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-790654cfaea1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_boston\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDecisionTreeRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/datasets/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    159\u001b[0m             \"\"\"\n\u001b[1;32m    160\u001b[0m         )\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: \n`load_boston` has been removed from scikit-learn since version 1.2.\n\nThe Boston housing prices dataset has an ethical problem: as\ninvestigated in [1], the authors of this dataset engineered a\nnon-invertible variable \"B\" assuming that racial self-segregation had a\npositive impact on house prices [2]. Furthermore the goal of the\nresearch that led to the creation of this dataset was to study the\nimpact of air quality but it did not give adequate demonstration of the\nvalidity of this assumption.\n\nThe scikit-learn maintainers therefore strongly discourage the use of\nthis dataset unless the purpose of the code is to study and educate\nabout ethical issues in data science and machine learning.\n\nIn this special case, you can fetch the dataset from the original\nsource::\n\n    import pandas as pd\n    import numpy as np\n\n    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n    target = raw_df.values[1::2, 2]\n\nAlternative datasets include the California housing dataset and the\nAmes housing dataset. You can load the datasets as follows::\n\n    from sklearn.datasets import fetch_california_housing\n    housing = fetch_california_housing()\n\nfor the California housing dataset and::\n\n    from sklearn.datasets import fetch_openml\n    housing = fetch_openml(name=\"house_prices\", as_frame=True)\n\nfor the Ames housing dataset.\n\n[1] M Carlisle.\n\"Racist data destruction?\"\n<https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>\n\n[2] Harrison Jr, David, and Daniel L. Rubinfeld.\n\"Hedonic housing prices and the demand for clean air.\"\nJournal of environmental economics and management 5.1 (1978): 81-102.\n<https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>\n",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xP9BDLh3F725"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the Boston Housing dataset\n",
        "boston = load_boston()\n",
        "X = boston.data\n",
        "y = boston.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Regressor with max_depth=5\n",
        "regressor_depth5 = DecisionTreeRegressor(max_depth=5, random_state=42)\n",
        "\n",
        "# Create an unrestricted Decision Tree Regressor\n",
        "regressor_unrestricted = DecisionTreeRegressor(random_state=42)  # No max_depth specified\n",
        "\n",
        "# Train the regressors\n",
        "regressor_depth5.fit(X_train, y_train)\n",
        "regressor_unrestricted.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_depth5 = regressor_depth5.predict(X_test)\n",
        "y_pred_unrestricted = regressor_unrestricted.predict(X_test)\n",
        "\n",
        "# Calculate the Mean Squared Error (MSE)\n",
        "mse_depth5 = mean_squared_error(y_test, y_pred_depth5)\n",
        "mse_unrestricted = mean_squared_error(y_test, y_pred_unrestricted)\n",
        "\n",
        "# Print the MSE values\n",
        "print(\"MSE (max_depth=5):\", mse_depth5)\n",
        "print(\"MSE (unrestricted):\", mse_unrestricted)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "UZ357QlUHiq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "28 Write a Python program to train a Decision Tree Classifier, apply Cost Complexity Pruning (CCP), and\n",
        "visualize its effect on accuracy"
      ],
      "metadata": {
        "id": "3yCjHqk8HjQx"
      }
    },
    {
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Train the classifier\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Get the path of the decision tree\n",
        "path = clf.cost_complexity_pruning_path(X_train, y_train)\n",
        "ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
        "\n",
        "# Create an array to store the accuracy scores for different CCP alpha values\n",
        "clfs = []\n",
        "for ccp_alpha in ccp_alphas:\n",
        "    clf = DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)\n",
        "    clf.fit(X_train, y_train)\n",
        "    clfs.append(clf)\n",
        "\n",
        "# Remove the last classifier (with the highest alpha value) as it is a trivial tree\n",
        "clfs = clfs[:-1]\n",
        "ccp_alphas = ccp_alphas[:-1]\n",
        "\n",
        "# Calculate the accuracy scores using cross-validation\n",
        "# This helps to estimate the performance of the pruned trees on unseen data\n",
        "node_counts = [clf.tree_.node_count for clf in clfs]\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Train the classifier\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Get the path of the decision tree\n",
        "path = clf.cost_complexity_pruning_path(X_train, y_train)\n",
        "ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
        "\n",
        "# Create an array to store the accuracy scores for different CCP alpha values\n",
        "clfs = []\n",
        "for ccp_alpha in ccp_alphas:\n",
        "    clf = DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)\n",
        "    clf.fit(X_train, y_train)\n",
        "    clfs.append(clf)\n",
        "\n",
        "# Remove the last classifier (with the highest alpha value) as it is a trivial tree\n",
        "clfs = clfs[:-1]\n",
        "ccp_alphas = ccp_alphas[:-1]\n",
        "\n",
        "# Calculate the accuracy scores using cross-validation\n",
        "# This helps to estimate the performance of the pruned trees on unseen data\n",
        "node_counts = [clf.tree_.node_count for clf in clfs]\n",
        "depth = [clf.tree_.max_depth for clf in clfs]\n",
        "train_scores = [clf.score(X_train, y_train) for clf in clfs]\n",
        "test_scores = [clf.score(X_test, y_test) for clf in clfs]"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "w7-6JmibHt32"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "28. Write a Python program to train a Decision Tree Classifier and evaluate its performance using Precision,\n",
        "Recall, and F1-Score\n",
        "\n",
        "\n",
        "ans:-"
      ],
      "metadata": {
        "id": "z-Phat4wHy28"
      }
    },
    {
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Train the classifier\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Generate the classification report\n",
        "report = classification_report(y_test, y_pred, target_names=iris.target_names)\n",
        "\n",
        "# Print the report\n",
        "print(report)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zRA02cH5H_eh",
        "outputId": "f7789f2f-a2b0-4c4d-ef43-74b5db2071b7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "      setosa       1.00      1.00      1.00        19\n",
            "  versicolor       1.00      1.00      1.00        13\n",
            "   virginica       1.00      1.00      1.00        13\n",
            "\n",
            "    accuracy                           1.00        45\n",
            "   macro avg       1.00      1.00      1.00        45\n",
            "weighted avg       1.00      1.00      1.00        45\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "29. Write a Python program to train a Decision Tree Classifier and visualize the confusion matrix using seaborn*"
      ],
      "metadata": {
        "id": "SIDkRP3BIAn6"
      }
    },
    {
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Train the classifier\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Create a pandas DataFrame from the confusion matrix for better visualization\n",
        "cm_df = pd.DataFrame(cm, index=iris.target_names, columns=iris.target_names)\n",
        "\n",
        "# Plot the confusion matrix using seaborn\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_df, annot=True, cmap=\"Blues\", fmt=\"g\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "nHTG_dKqII--",
        "outputId": "733b1248-980d-41a4-b15c-1b1353c147cb"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "30 Write a Python program to train a Decision Tree Classifier and use GridSearchCV to find the optimal values\n",
        "for max_depth and min_samples_split.\n",
        "\n",
        "\n",
        "ams:-"
      ],
      "metadata": {
        "id": "kxuTcZ-hILe-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZWAvgsMcHuMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris() # Call the load_iris() function to load the dataset"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "fpXD3JwFIeWQ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eKzaUb4yIfBR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}