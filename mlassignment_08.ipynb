{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70HAfdjbVGKk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Can we use Bagging for regression problems\u0010\n",
        "\n",
        "\n",
        "ans;-Bagging for Regression\n",
        "\n",
        "In regression problems, bagging involves creating multiple subsets of the training data using bootstrapping (random sampling with replacement). Then, a base regression model (e.g., a decision tree) is trained on each subset. Finally, the predictions from all the base models are averaged to get the final prediction. This process helps to reduce variance and improve the accuracy of the model.\n",
        "\n",
        "Here's a step-by-step explanation:\n",
        "\n",
        "Create multiple subsets of the training data: Randomly sample data points from the training data with replacement to create multiple subsets. This is known as bootstrapping.\n",
        "\n",
        "Train base regression models: Train a base regression model (e.g., a decision tree) on each subset of the data.\n",
        "\n",
        "Average predictions: When making predictions on new data, use each base model to generate a prediction and then average the predictions to get the final prediction.\n",
        "\n",
        "Benefits of Bagging for Regression:\n",
        "\n",
        "Reduced Variance: Bagging helps to reduce the variance of the model, making it less sensitive to the specific training data used. This leads to more stable and consistent predictions.\n",
        "Improved Accuracy: By combining predictions from multiple models, bagging can improve the overall accuracy of the regression model.\n",
        "Reduced Overfitting: Bagging can help to reduce overfitting, especially when using complex base models like decision trees.\n",
        "Example using scikit-learn in Python:\n",
        "\n",
        "\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "# Create a BaggingRegressor with DecisionTreeRegressor as the base model\n",
        "bagging_regressor = BaggingRegressor(base_estimator=DecisionTreeRegressor(),\n",
        "                                    n_estimators=10,  # Number of base models\n",
        "                                    random_state=42)  # Random seed for reproducibility\n",
        "\n",
        "# Fit the BaggingRegressor to the training data\n",
        "bagging_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on new data\n",
        "y_pred = bagging_regressor.predict(X_test)"
      ],
      "metadata": {
        "id": "cR-MgweHVK4w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is the difference between multiple model training and single model training\u0010\n",
        "\n",
        "\n",
        "ans;-Single Model Training\n",
        "\n",
        "In single model training, you train one model on your entire dataset. This model is then used to make predictions on new data. This approach is straightforward and often works well for simple problems.\n",
        "\n",
        "Multiple Model Training\n",
        "\n",
        "In multiple model training, you train multiple models, either on different subsets of the data or using different algorithms. These models are then combined to make a final prediction. This approach can be more complex, but it can often lead to better performance, especially for complex problems.\n",
        "\n",
        "Here's a table summarizing the key differences:\n",
        "\n",
        "Feature\tSingle Model Training\tMultiple Model Training\n",
        "Number of models\tOne\tMultiple\n",
        "Data used\tEntire dataset\tSubsets of the data or different datasets\n",
        "Algorithms\tOne\tCan be the same or different\n",
        "Complexity\tSimpler\tMore complex\n",
        "Performance\tCan be good for simple problems\tOften better for complex problems\n",
        "Interpretability\tEasier to interpret\tCan be more difficult to interpret\n",
        "When to Use Multiple Model Training\n",
        "\n",
        "Multiple model training is often preferred in the following situations:\n",
        "\n",
        "Complex problems: When the relationship between the input features and the target variable is complex, multiple models can often capture different aspects of the relationship and lead to better predictions.\n",
        "Large datasets: When you have a large dataset, training multiple models on different subsets of the data can be more efficient and can lead to better performance.\n",
        "Ensemble methods: Ensemble methods, such as bagging and boosting, rely on training multiple models to improve performance.\n",
        "Types of Multiple Model Training\n",
        "\n",
        "There are several different types of multiple model training, including:\n",
        "\n",
        "Bagging: Training multiple models on different bootstrapped samples of the data and averaging the predictions.\n",
        "Boosting: Training multiple models sequentially, where each model tries to correct the errors of the previous models.\n",
        "Stacking: Training multiple models and then using a meta-model to combine their predictions."
      ],
      "metadata": {
        "id": "tmnSLU8fVWPG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Explain the concept of feature randomness in Random Forest2\n",
        "\n",
        "\n",
        "ans;-Feature Randomness in Random Forest\n",
        "\n",
        "Feature randomness, also known as feature bagging or the random subspace method, is a crucial aspect of the Random Forest algorithm. It involves randomly selecting a subset of features at each node of a decision tree during the tree-building process. This means that each tree in the forest is built using a different random subset of features.\n",
        "\n",
        "Reasoning Behind Feature Randomness:\n",
        "\n",
        "The primary reason for introducing feature randomness is to reduce correlation between individual trees in the forest. By using different feature subsets for each tree, the algorithm ensures that the trees are more diverse and less likely to make the same mistakes. This diversity ultimately leads to improved performance and robustness of the overall model.\n",
        "\n",
        "Benefits of Feature Randomness\n",
        "\n",
        "Reduced Overfitting: Feature randomness helps to prevent overfitting, which is a common problem with decision trees. Overfitting occurs when a model learns the training data too well and performs poorly on unseen data. By introducing randomness, the algorithm encourages the trees to focus on different aspects of the data, reducing the risk of overfitting.\n",
        "\n",
        "Improved Accuracy: By reducing correlation between trees, feature randomness leads to a more accurate and robust model. The combined predictions of diverse trees are less likely to be influenced by individual errors, resulting in improved overall accuracy.\n",
        "\n",
        "Handling High-Dimensional Data: Feature randomness is particularly useful when dealing with high-dimensional data, where there are many features. By randomly selecting a subset of features, the algorithm can effectively reduce the dimensionality of the problem and improve computational efficiency.\n",
        "\n",
        "How it Works in Practice\n",
        "\n",
        "During the construction of each tree in a Random Forest:\n",
        "\n",
        "Node Splitting: At each node of the tree, the algorithm randomly selects a subset of features from the total number of features available.\n",
        "\n",
        "Feature Selection: The best feature for splitting the node is selected from this random subset, based on a criterion such as information gain or Gini impurity.\n",
        "\n",
        "Tree Growth: The tree is grown by recursively repeating this process for each child node until a stopping criterion is met.\n",
        "\n",
        "Illustrative Example\n",
        "\n",
        "Imagine you have a dataset with 10 features. In a Random Forest with feature randomness, each tree might be built using a random subset of 5 features. This means that each tree will focus on different aspects of the data, leading to a more diverse and robust forest."
      ],
      "metadata": {
        "id": "CoWmJ_BeVdeR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is OOB (Out-of-Bag) Score\u0010\n",
        "\n",
        "\n",
        "\n",
        "ans:-OOB (Out-of-Bag) Score\n",
        "\n",
        "The OOB score is a method for evaluating the performance of a Random Forest model without the need for a separate validation dataset. It leverages the bootstrapping process inherent in Random Forest to estimate the model's generalization error.\n",
        "\n",
        "How it Works\n",
        "\n",
        "Bootstrapping: In Random Forest, each decision tree is trained on a bootstrapped sample of the training data. This means that each tree only sees a random subset of the data, and some data points are left out.\n",
        "\n",
        "OOB Samples: The data points that are not used to train a particular tree are called \"out-of-bag\" (OOB) samples for that tree.\n",
        "\n",
        "OOB Predictions: For each data point in the training set, we can obtain predictions from the trees for which it was an OOB sample.\n",
        "\n",
        "OOB Score: The OOB score is calculated by aggregating the predictions for each data point across all the trees where it was an OOB sample. This provides an estimate of the model's performance on unseen data.\n",
        "\n",
        "Benefits of OOB Score\n",
        "\n",
        "No Need for Validation Set: The OOB score eliminates the need to split the data into training and validation sets, which can be beneficial when data is limited.\n",
        "\n",
        "Efficient Evaluation: It provides an efficient way to evaluate the model's performance during training without incurring the computational cost of cross-validation.\n",
        "\n",
        "Unbiased Estimate: The OOB score is considered an unbiased estimate of the model's generalization error, as it is based on predictions made on data points that were not used for training.\n",
        "\n",
        "Calculation\n",
        "\n",
        "The OOB score can be calculated for both classification and regression problems.\n",
        "\n",
        "Classification: For classification, the OOB score is typically the accuracy or error rate calculated on the OOB predictions.\n",
        "\n",
        "Regression: For regression, the OOB score is typically the mean squared error (MSE) or R-squared calculated on the OOB predictions.\n",
        "\n",
        "Example in scikit-learn\n",
        "\n",
        "In scikit-learn, you can enable OOB score calculation by setting the oob_score parameter to True when creating a Random Forest model.\n",
        "\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf_classifier = RandomForestClassifier(oob_score=True)\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "oob_score = rf_classifier.oob_score_\n",
        "print(\"OOB Score:\", oob_score)\n",
        "Use code with caution\n",
        "This will calculate and print the OOB score after training the model."
      ],
      "metadata": {
        "id": "dSnNoWl9VnSu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.How can you measure the importance of features in a Random Forest model\u0010\n",
        "\n",
        "\n",
        "\n",
        "ans:-Feature Importance in Random Forest\n",
        "\n",
        "Random Forest models offer a built-in mechanism to assess the importance of features in predicting the target variable. This information can be valuable for understanding the underlying relationships in your data and for feature selection.\n",
        "\n",
        "Methods for Measuring Feature Importance\n",
        "\n",
        "Gini Importance (Mean Decrease Impurity):\n",
        "\n",
        "This method is based on the Gini impurity, which measures the disorder or uncertainty in a set of samples.\n",
        "For each feature, the Gini importance is calculated by summing the weighted decrease in Gini impurity achieved by splits using that feature across all trees in the forest.\n",
        "Features that lead to larger decreases in impurity are considered more important.\n",
        "In scikit-learn, this is the default method for feature importance in Random Forest.\n",
        "Permutation Importance (Mean Decrease Accuracy):\n",
        "\n",
        "This method measures the importance of a feature by observing the decrease in model performance when the values of that feature are randomly shuffled.\n",
        "For each feature, the values are permuted, and the model's performance is evaluated on the permuted data.\n",
        "The decrease in performance (e.g., accuracy or R-squared) is attributed to the importance of the feature.\n",
        "This method is less biased than Gini importance, especially for high-cardinality features.\n",
        "Accessing Feature Importance in scikit-learn\n",
        "\n",
        "After training a Random Forest model in scikit-learn, you can access the feature importances using the feature_importances_ attribute.\n",
        "\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Train the Random Forest model\n",
        "rf_classifier = RandomForestClassifier()\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importances\n",
        "feature_importances = rf_classifier.feature_importances_\n",
        "\n",
        "# Print feature importances\n",
        "for i, feature in enumerate(X_train.columns):\n",
        "    print(f\"{feature}: {feature_importances[i]}\")\n",
        "Use code with caution\n",
        "This code snippet demonstrates how to retrieve and print the feature importances for each feature in your dataset.\n",
        "\n",
        "Visualizing Feature Importance\n",
        "\n",
        "You can visualize feature importances using various plotting techniques, such as bar charts or horizontal bar charts. This can help you easily identify the most important features in your model.\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a bar chart of feature importances\n",
        "plt.bar(X_train.columns, feature_importances)\n",
        "plt.xlabel(\"Features\")\n",
        "plt.ylabel(\"Importance\")\n",
        "plt.title(\"Feature Importance in Random Forest\")\n",
        "plt.xticks(rotation=90)  # Rotate x-axis labels for better readability\n",
        "plt.show()\n",
        "Use code with caution\n",
        "This code snippet demonstrates how to create a simple bar chart to visualize feature importances."
      ],
      "metadata": {
        "id": "XX7R4zGGVxfy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Explain the working principle of a Bagging Classifier2\n",
        "\n",
        "\n",
        "\n",
        "ans;-Bagging Classifier\n",
        "\n",
        "A Bagging Classifier is an ensemble learning method that combines the predictions of multiple base classifiers to improve the overall accuracy and robustness of the model. It is based on the concept of bootstrap aggregating, or bagging.\n",
        "\n",
        "Working Principle\n",
        "\n",
        "Bootstrapping: The Bagging Classifier starts by creating multiple subsets of the training data using bootstrapping. Bootstrapping involves randomly sampling data points from the training data with replacement. This means that some data points may be selected multiple times, while others may not be selected at all.\n",
        "\n",
        "Base Classifier Training: A base classifier (e.g., a decision tree) is trained on each bootstrapped subset of the data. These base classifiers are typically weak learners, meaning they have relatively high variance and may not perform well individually.\n",
        "\n",
        "Aggregation: Once all the base classifiers are trained, the Bagging Classifier aggregates their predictions to make a final prediction. For classification problems, the aggregation is typically done by majority voting. This means that the class that is predicted by the majority of the base classifiers is chosen as the final prediction.\n",
        "\n",
        "Steps Involved\n",
        "\n",
        "Here's a step-by-step breakdown of the Bagging Classifier's working principle:\n",
        "\n",
        "Create multiple bootstrapped subsets of the training data.\n",
        "Train a base classifier on each bootstrapped subset.\n",
        "Aggregate the predictions of the base classifiers to make a final prediction.\n",
        "Benefits of Bagging\n",
        "\n",
        "Reduced Variance: Bagging helps to reduce the variance of the model by combining the predictions of multiple base classifiers. This makes the model less sensitive to the specific training data used and leads to more stable and consistent predictions.\n",
        "\n",
        "Improved Accuracy: By combining the predictions of multiple base classifiers, bagging can improve the overall accuracy of the model. This is especially true when the base classifiers are weak learners.\n",
        "\n",
        "Reduced Overfitting: Bagging can help to reduce overfitting, which is a common problem with complex models. This is because the base classifiers are trained on different subsets of the data, which helps to prevent them from memorizing the training data too well.\n",
        "\n",
        "Illustrative Example\n",
        "\n",
        "Imagine you have a dataset with 100 data points. You want to train a Bagging Classifier with 10 base classifiers.\n",
        "\n",
        "The Bagging Classifier would create 10 bootstrapped subsets of the data, each containing 100 data points sampled with replacement.\n",
        "A base classifier (e.g., a decision tree) would be trained on each bootstrapped subset.\n",
        "When making a prediction on a new data point, the Bagging Classifier would obtain predictions from all 10 base classifiers.\n",
        "The final prediction would be the class that is predicted by the majority of the base classifiers."
      ],
      "metadata": {
        "id": "cXeNe8eXV7RJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. How do you evaluate a Bagging Classifierâ€™s performance\u0010\n",
        "\n",
        "\n",
        "\n",
        "ans:-Evaluating a Bagging Classifier's Performance\n",
        "\n",
        "Evaluating the performance of a Bagging Classifier is crucial to ensure that it is generalizing well to unseen data and making accurate predictions. Here are some common methods for evaluating its performance:\n",
        "\n",
        "Accuracy Score: For classification tasks, accuracy is a commonly used metric that measures the proportion of correctly classified instances out of the total number of instances.\n",
        "\n",
        "Precision and Recall: Precision measures the proportion of correctly predicted positive instances out of all instances predicted as positive. Recall measures the proportion of correctly predicted positive instances out of all actual positive instances. These metrics are particularly useful when dealing with imbalanced datasets.\n",
        "\n",
        "F1-Score: The F1-score is the harmonic mean of precision and recall, providing a balanced measure of both metrics. It is especially useful when you want to consider both false positives and false negatives.\n",
        "\n",
        "ROC AUC Score: The Receiver Operating Characteristic (ROC) Area Under the Curve (AUC) score is a measure of the classifier's ability to distinguish between positive and negative classes. It is particularly useful for binary classification problems.\n",
        "\n",
        "Cross-Validation: Cross-validation involves splitting the data into multiple folds and training the model on different combinations of folds. This helps to assess the model's performance on unseen data and reduce the impact of data variability.\n",
        "\n",
        "Out-of-Bag (OOB) Score: As discussed earlier, the OOB score is a specific evaluation method for Bagging Classifiers that leverages the bootstrapping process to estimate the model's generalization error without the need for a separate validation set.\n",
        "\n",
        "Example in scikit-learn\n",
        "\n",
        "Here's an example of how to evaluate a Bagging Classifier's performance using scikit-learn:\n",
        "\n",
        "\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Bagging Classifier\n",
        "bagging_classifier = BaggingClassifier()\n",
        "\n",
        "# Train the model\n",
        "bagging_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "y_pred = bagging_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the model's performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-Score:\", f1)\n",
        "print(\"ROC AUC Score:\", roc_auc)\n",
        "Use code with caution\n",
        "This code snippet demonstrates how to calculate various evaluation metrics for a Bagging Classifier using scikit-learn's built-in functions. You can choose the appropriate metrics based on your specific problem and dataset."
      ],
      "metadata": {
        "id": "t3cY7a8GWE12"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.How does a Bagging Regressor work\u0010\n",
        "\n",
        "\n",
        "\n",
        "ans:-Bagging Regressor\n",
        "\n",
        "A Bagging Regressor is an ensemble learning method used for regression tasks. It leverages the concept of bootstrap aggregating (bagging) to improve the accuracy and stability of regression models.\n",
        "\n",
        "Working Principle\n",
        "\n",
        "Bootstrapping: The Bagging Regressor starts by creating multiple subsets of the training data using bootstrapping. Bootstrapping involves randomly sampling data points from the training data with replacement. This means that some data points may be selected multiple times, while others may not be selected at all.\n",
        "\n",
        "Base Regressor Training: A base regressor (e.g., a decision tree regressor) is trained on each bootstrapped subset of the data. These base regressors are typically weak learners, meaning they have relatively high variance and may not perform well individually.\n",
        "\n",
        "Aggregation: Once all the base regressors are trained, the Bagging Regressor aggregates their predictions to make a final prediction. For regression problems, the aggregation is typically done by averaging the predictions of all the base regressors.\n",
        "\n",
        "Steps Involved\n",
        "\n",
        "Here's a step-by-step breakdown of the Bagging Regressor's working principle:\n",
        "\n",
        "Create multiple bootstrapped subsets of the training data.\n",
        "Train a base regressor on each bootstrapped subset.\n",
        "Average the predictions of the base regressors to make a final prediction.\n",
        "Benefits of Bagging for Regression\n",
        "\n",
        "Reduced Variance: Bagging helps to reduce the variance of the regression model by combining the predictions of multiple base regressors. This makes the model less sensitive to the specific training data used and leads to more stable and consistent predictions.\n",
        "\n",
        "Improved Accuracy: By combining the predictions of multiple base regressors, bagging can improve the overall accuracy of the regression model. This is especially true when the base regressors are weak learners.\n",
        "\n",
        "Reduced Overfitting: Bagging can help to reduce overfitting, which is a common problem with complex regression models. This is because the base regressors are trained on different subsets of the data, which helps to prevent them from memorizing the training data too well.\n",
        "\n",
        "Example in scikit-learn\n",
        "\n",
        "Here's an example of how to use a Bagging Regressor in scikit-learn:\n",
        "\n",
        "\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "# Create a Bagging Regressor with DecisionTreeRegressor as the base model\n",
        "bagging_regressor = BaggingRegressor(base_estimator=DecisionTreeRegressor(),\n",
        "                                    n_estimators=10,  # Number of base regressors\n",
        "                                    random_state=42)  # Random seed for reproducibility\n",
        "\n",
        "# Fit the Bagging Regressor to the training data\n",
        "bagging_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on new data\n",
        "y_pred = bagging_regressor.predict(X_test)\n",
        "Use code with caution\n",
        "This code snippet demonstrates how to create and use a Bagging Regressor with a decision tree regressor as the base model. You can adjust the number of base regressors (n_estimators) and other parameters as needed."
      ],
      "metadata": {
        "id": "Xf20pAJrWOua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What is the main advantage of ensemble techniques\u0010\n",
        "\n",
        "\n",
        "\n",
        "ans:-Main Advantage of Ensemble Techniques\n",
        "\n",
        "The main advantage of ensemble techniques is their ability to significantly improve the predictive performance of machine learning models. This improvement is achieved by combining the predictions of multiple individual models (base learners) to create a more robust and accurate overall model.\n",
        "\n",
        "Reasoning\n",
        "\n",
        "Ensemble techniques leverage the principle of \"wisdom of the crowd,\" where the collective knowledge of multiple models is often more accurate than the knowledge of any single model. By combining the predictions of diverse models, ensemble methods can effectively reduce errors, handle noise, and improve generalization.\n",
        "\n",
        "Key Benefits\n",
        "\n",
        "Improved Accuracy: Ensemble methods often achieve higher accuracy compared to individual models, especially when the base learners are diverse and have complementary strengths.\n",
        "Reduced Overfitting: Ensemble methods can help to reduce overfitting, which is a common problem in machine learning where a model performs well on the training data but poorly on unseen data. By combining multiple models, ensemble methods can mitigate the risk of overfitting and improve generalization.\n",
        "Increased Robustness: Ensemble methods are more robust to noise and outliers in the data compared to individual models. This is because the errors of individual models tend to cancel each other out when combined in an ensemble.\n",
        "Handling Complex Relationships: Ensemble methods can effectively capture complex relationships in the data that may be difficult for a single model to learn. This is because different models in the ensemble may focus on different aspects of the data, leading to a more comprehensive understanding of the underlying patterns.\n",
        "Example\n",
        "\n",
        "Consider a scenario where you want to predict whether a customer will churn. You could train multiple models, such as logistic regression, decision trees, and support vector machines, on the same data. Then, you could combine the predictions of these models using an ensemble method like bagging or boosting. The ensemble model is likely to achieve higher accuracy and be more robust to noise compared to any of the individual models."
      ],
      "metadata": {
        "id": "YcuiW_y1Wfiy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "10. What is the main challenge of ensemble methods\u0010\n",
        "\n",
        "\n",
        "ans:_Main Challenge of Ensemble Methods\n",
        "\n",
        "While ensemble methods offer significant advantages in terms of predictive performance, they also come with their own set of challenges. The main challenge of ensemble methods is the increased complexity compared to using a single model. This complexity arises from several factors:\n",
        "\n",
        "Computational Cost: Training and deploying multiple models can be computationally expensive, especially when dealing with large datasets or complex models. The increased computational cost can make ensemble methods impractical for some applications with limited resources.\n",
        "\n",
        "Interpretability: Ensemble models can be more difficult to interpret compared to single models. This is because the predictions of multiple models are combined, making it harder to understand the underlying relationships between the features and the target variable.\n",
        "\n",
        "Model Selection and Tuning: Choosing the right base learners and hyperparameters for the ensemble can be a challenging task. There are many different types of ensemble methods and a wide range of hyperparameters to tune. Selecting the optimal combination of base learners and hyperparameters often requires extensive experimentation and domain expertise.\n",
        "\n",
        "Data Requirements: Ensemble methods often require more data than single models to achieve their full potential. This is because multiple models need to be trained, and each model needs sufficient data to learn effectively.\n",
        "\n",
        "Diversity of Base Learners: For ensemble methods to be effective, the base learners should be diverse and have complementary strengths. Achieving diversity can be challenging, as it requires careful selection of base learners and potentially different training procedures.\n",
        "\n",
        "Addressing the Challenges\n",
        "\n",
        "While the complexity of ensemble methods can be a challenge, there are various strategies to mitigate these challenges:\n",
        "\n",
        "Using efficient algorithms: Choosing base learners that are computationally efficient can help reduce the overall computational cost of the ensemble.\n",
        "Feature importance analysis: Analyzing the feature importance of the ensemble can provide insights into the model's decision-making process and improve interpretability.\n",
        "Automated model selection and tuning: Using automated techniques, such as grid search or Bayesian optimization, can help find the optimal combination of base learners and hyperparameters.\n",
        "Data augmentation: Applying data augmentation techniques can help increase the effective size of the training data and improve the performance of the ensemble.\n",
        "Ensemble pruning: Removing redundant or less informative base learners from the ensemble can simplify the model and improve interpretability.\n",
        "By carefully considering these challenges and applying appropriate strategies, you can effectively leverage the power of ensemble methods while minimizing their drawbacks."
      ],
      "metadata": {
        "id": "ZZ-4s4NyWwp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Explain the key idea behind ensemble techniques2\n",
        "\n",
        "\n",
        "\n",
        "ans:-Key Idea Behind Ensemble Techniques\n",
        "\n",
        "The key idea behind ensemble techniques is to combine the predictions of multiple individual models (base learners) to improve the overall predictive performance. This is based on the principle that the collective knowledge of multiple models is often more accurate and robust than the knowledge of any single model.\n",
        "\n",
        "Reasoning\n",
        "\n",
        "Ensemble techniques leverage the \"wisdom of the crowd\" phenomenon, where the combined opinions of a group of individuals are often more accurate than the opinion of any single individual. In the context of machine learning, this translates to combining the predictions of multiple models to create a more accurate and robust overall model.\n",
        "\n",
        "How it Works\n",
        "\n",
        "Ensemble techniques typically involve the following steps:\n",
        "\n",
        "Training Base Learners: Multiple individual models (base learners) are trained on the same or different subsets of the training data. These base learners can be of the same type (e.g., all decision trees) or different types (e.g., a mix of decision trees, support vector machines, and logistic regression models).\n",
        "\n",
        "Combining Predictions: The predictions of the base learners are combined to produce a final prediction. This combination can be done in various ways, such as:\n",
        "\n",
        "Averaging: For regression problems, the predictions of the base learners can be averaged to produce the final prediction.\n",
        "Majority Voting: For classification problems, the class that is predicted by the majority of the base learners is chosen as the final prediction.\n",
        "Weighted Averaging: The predictions of the base learners can be weighted based on their individual performance or confidence levels.\n",
        "Benefits of Ensemble Techniques\n",
        "\n",
        "Improved Accuracy: By combining the predictions of multiple models, ensemble techniques can often achieve higher accuracy compared to individual models.\n",
        "Reduced Overfitting: Ensemble techniques can help to reduce overfitting, which is a common problem in machine learning where a model performs well on the training data but poorly on unseen data.\n",
        "Increased Robustness: Ensemble techniques are more robust to noise and outliers in the data compared to individual models.\n",
        "Handling Complex Relationships: Ensemble techniques can effectively capture complex relationships in the data that may be difficult for a single model to learn.\n",
        "Illustrative Example\n",
        "\n",
        "Imagine you have a group of doctors trying to diagnose a patient's illness. Each doctor may have their own expertise and perspective, and they may arrive at different diagnoses. However, by combining their opinions and considering all the available evidence, they are more likely to arrive at the correct diagnosis. This is analogous to how ensemble techniques work in machine learning."
      ],
      "metadata": {
        "id": "HYI_GiPIXFhK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. What is a Random Forest Classifier\u0010\n",
        "\n",
        "\n",
        "ans:-Random Forest Classifier\n",
        "\n",
        "A Random Forest Classifier is an ensemble learning method that belongs to the bagging family. It constructs a multitude of decision trees during training and outputs the class that is the mode of the classes (classification) or mean/average prediction (regression) of the individual trees.\n",
        "\n",
        "Key Characteristics\n",
        "\n",
        "Ensemble of Decision Trees: A Random Forest is essentially a collection of decision trees, where each tree is trained on a different subset of the data and uses a random subset of features for making decisions at each node.\n",
        "Bagging: It utilizes the bagging technique (bootstrap aggregating) to create diverse training sets for each tree. This involves randomly sampling data points with replacement from the original dataset.\n",
        "Feature Randomness: In addition to bagging, Random Forest also introduces feature randomness, where each tree only considers a random subset of features for splitting at each node. This further increases the diversity of the trees and helps to reduce overfitting.\n",
        "Robustness and Accuracy: By combining the predictions of multiple trees, Random Forest achieves higher accuracy and robustness compared to individual decision trees. It is less prone to overfitting and can handle noisy data effectively.\n",
        "Working Principle\n",
        "\n",
        "Bootstrapping: Create multiple bootstrapped samples of the training data.\n",
        "Tree Construction: For each bootstrapped sample, build a decision tree using a random subset of features at each node.\n",
        "Prediction: To make a prediction on a new data point, pass it through each tree in the forest and obtain individual predictions.\n",
        "Aggregation: Aggregate the predictions of all trees to produce the final prediction. For classification, this is typically done by majority voting (mode of the classes).\n",
        "Advantages of Random Forest Classifier\n",
        "\n",
        "High Accuracy: Random Forest often achieves high accuracy in classification tasks.\n",
        "Robustness: It is robust to noise and outliers in the data.\n",
        "Handles High-Dimensional Data: It can effectively handle datasets with a large number of features.\n",
        "Feature Importance: It provides insights into the importance of different features in the prediction process.\n",
        "Reduces Overfitting: The ensemble nature of Random Forest helps to reduce overfitting compared to individual decision trees.\n",
        "Example in scikit-learn\n",
        "\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Create a Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on new data\n",
        "y_pred = rf_classifier.predict(X_test)\n",
        "Use code with caution\n",
        "This code snippet demonstrates how to create and use a Random Forest Classifier in scikit-learn. You can adjust the number of trees (n_estimators) and other parameters as needed."
      ],
      "metadata": {
        "id": "BX7IR0W0XR10"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What are the main types of ensemble techniques\u0010\n",
        "\n",
        "\n",
        "\n",
        "ans:-Main Types of Ensemble Techniques\n",
        "\n",
        "Ensemble techniques can be broadly categorized into three main types:\n",
        "\n",
        "Bagging (Bootstrap Aggregating)\n",
        "\n",
        "Key Idea: Create multiple subsets of the training data using bootstrapping (random sampling with replacement). Train a base learner on each subset and combine their predictions through averaging (for regression) or voting (for classification).\n",
        "Examples: Random Forest, Bagging Classifier, Bagging Regressor\n",
        "Benefits: Reduces variance, improves accuracy, and handles high-dimensional data effectively.\n",
        "Boosting\n",
        "\n",
        "Key Idea: Train base learners sequentially, where each subsequent learner focuses on correcting the errors made by the previous learners. The final prediction is a weighted combination of the predictions from all learners.\n",
        "Examples: AdaBoost, Gradient Boosting, XGBoost, LightGBM\n",
        "Benefits: Achieves high accuracy, handles complex relationships, and can be effective with limited data.\n",
        "Stacking\n",
        "\n",
        "Key Idea: Train multiple base learners and use their predictions as input features to train a meta-learner that combines the predictions to produce the final prediction.\n",
        "Examples: Stacking Classifier, Stacking Regressor\n",
        "Benefits: Can potentially improve accuracy further by leveraging the strengths of different base learners and capturing higher-order interactions between them.\n",
        "Choosing the Right Ensemble Technique\n",
        "\n",
        "The choice of ensemble technique depends on various factors, including the specific problem, the dataset characteristics, and the desired performance goals. Here's a brief overview of when to consider each type:\n",
        "\n",
        "Bagging: Suitable for problems with high variance and when you want to reduce overfitting.\n",
        "Boosting: Effective for achieving high accuracy and handling complex relationships in the data.\n",
        "Stacking: Can be used to further improve accuracy by combining the predictions of multiple base learners in a more sophisticated way.\n",
        "It's often beneficial to experiment with different ensemble techniques and compare their performance to find the best approach for your specific task."
      ],
      "metadata": {
        "id": "kel0gDNBXdUW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. What is ensemble learning in machine learning\u0010\n",
        "\n",
        "\n",
        "\n",
        "ans:-Ensemble Learning\n",
        "\n",
        "Ensemble learning is a machine learning technique that combines the predictions of multiple individual models (base learners) to create a more accurate and robust overall model. This is based on the principle that the collective knowledge of multiple models is often more accurate than the knowledge of any single model.\n",
        "\n",
        "Key Idea\n",
        "\n",
        "The key idea behind ensemble learning is to leverage the \"wisdom of the crowd\" phenomenon, where the combined opinions of a group of individuals are often more accurate than the opinion of any single individual. In the context of machine learning, this translates to combining the predictions of multiple models to create a more accurate and robust overall model.\n",
        "\n",
        "How it Works\n",
        "\n",
        "Ensemble learning typically involves the following steps:\n",
        "\n",
        "Training Base Learners: Multiple individual models (base learners) are trained on the same or different subsets of the training data. These base learners can be of the same type (e.g., all decision trees) or different types (e.g., a mix of decision trees, support vector machines, and logistic regression models).\n",
        "\n",
        "Combining Predictions: The predictions of the base learners are combined to produce a final prediction. This combination can be done in various ways, such as:\n",
        "\n",
        "Averaging: For regression problems, the predictions of the base learners can be averaged to produce the final prediction.\n",
        "Majority Voting: For classification problems, the class that is predicted by the majority of the base learners is chosen as the final prediction.\n",
        "Weighted Averaging: The predictions of the base learners can be weighted based on their individual performance or confidence levels.\n",
        "Benefits of Ensemble Learning\n",
        "\n",
        "Improved Accuracy: By combining the predictions of multiple models, ensemble learning can often achieve higher accuracy compared to individual models.\n",
        "Reduced Overfitting: Ensemble learning can help to reduce overfitting, which is a common problem in machine learning where a model performs well on the training data but poorly on unseen data.\n",
        "Increased Robustness: Ensemble learning is more robust to noise and outliers in the data compared to individual models.\n",
        "Handling Complex Relationships: Ensemble learning can effectively capture complex relationships in the data that may be difficult for a single model to learn.\n",
        "Types of Ensemble Methods\n",
        "\n",
        "There are several different types of ensemble methods, including:\n",
        "\n",
        "Bagging (Bootstrap Aggregating): Random Forest, Bagging Classifier, Bagging Regressor\n",
        "Boosting: AdaBoost, Gradient Boosting, XGBoost, LightGBM\n",
        "Stacking: Stacking Classifier, Stacking Regressor\n",
        "Illustrative Example\n",
        "\n",
        "Imagine you have a group of doctors trying to diagnose a patient's illness. Each doctor may have their own expertise and perspective, and they may arrive at different diagnoses. However, by combining their opinions and considering all the available evidence, they are more likely to arrive at the correct diagnosis. This is analogous to how ensemble learning works in machine learning."
      ],
      "metadata": {
        "id": "4-uMuzxPXl5M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15.When should we avoid using ensemble methods\u0010\n",
        "\n",
        "\n",
        "ans:-When to Avoid Ensemble Methods\n",
        "\n",
        "While ensemble methods are powerful and often improve predictive performance, there are certain situations where they may not be the best choice:\n",
        "\n",
        "Simple Problems: For relatively simple problems where a single model can achieve satisfactory performance, using an ensemble method may be unnecessary and introduce unnecessary complexity.\n",
        "\n",
        "Limited Data: Ensemble methods typically require more data than single models to train effectively. With limited data, an ensemble may overfit and perform poorly on unseen data.\n",
        "\n",
        "Computational Constraints: Training and deploying multiple models can be computationally expensive. If computational resources are limited, using a simpler model may be more practical.\n",
        "\n",
        "Interpretability Requirements: Ensemble models can be more difficult to interpret than single models. If interpretability is a crucial requirement, a simpler model may be preferred.\n",
        "\n",
        "Low Diversity of Base Learners: For ensemble methods to be effective, the base learners should be diverse and have complementary strengths. If the base learners are too similar, the ensemble may not provide significant improvement over a single model.\n",
        "\n",
        "Time Constraints: Training multiple models can take longer than training a single model. If time is a constraint, using a simpler model may be necessary.\n",
        "\n",
        "Deployment Complexity: Deploying an ensemble model can be more complex than deploying a single model. This added complexity may not be justified if the performance gains are marginal.\n",
        "\n",
        "Alternatives to Ensemble Methods\n",
        "\n",
        "In situations where ensemble methods are not suitable, consider these alternatives:\n",
        "\n",
        "Feature Engineering: Focus on improving the quality and relevance of your features to improve the performance of a single model.\n",
        "Hyperparameter Tuning: Carefully tune the hyperparameters of a single model to optimize its performance.\n",
        "Regularization: Apply regularization techniques to prevent overfitting and improve the generalization of a single model.\n",
        "Using a More Complex Model: If a single model is underfitting, consider using a more complex model with greater capacity."
      ],
      "metadata": {
        "id": "5SNmPeaaXyvE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. How does Bagging help in reducing overfitting\u0010\n",
        "\n",
        "\n",
        "ans:-How Bagging Reduces Overfitting\n",
        "\n",
        "Bagging, or Bootstrap Aggregating, is an ensemble technique that helps reduce overfitting in machine learning models, particularly decision trees, by introducing diversity and reducing variance. Here's how it works:\n",
        "\n",
        "Bootstrapping: Bagging creates multiple subsets of the original training data by randomly sampling data points with replacement. Each subset, called a bootstrap sample, is the same size as the original dataset but contains some duplicate entries and omits some original data points.\n",
        "\n",
        "Training Multiple Models: A separate base model (often a decision tree) is trained on each bootstrap sample. This results in an ensemble of models, each with a slightly different perspective on the data.\n",
        "\n",
        "Aggregation: When making predictions, the outputs of all the individual models are combined, typically by averaging (for regression) or majority voting (for classification).\n",
        "\n",
        "Why This Reduces Overfitting:\n",
        "\n",
        "Reduced Variance: Individual decision trees are prone to high variance, meaning they can be sensitive to small changes in the training data. By averaging the predictions of multiple trees trained on different subsets of data, Bagging reduces this variance and creates a more stable model.\n",
        "\n",
        "Diversity of Models: Each tree in the ensemble is trained on a different bootstrap sample, leading to diversity in their structure and predictions. This diversity helps to avoid overfitting to specific patterns in the training data that might not generalize well to unseen data.\n",
        "\n",
        "Focus on Important Features: While not as pronounced as in Random Forests, Bagging can indirectly lead to models that focus on more important features. Trees trained on different bootstrap samples are likely to identify and prioritize different features, leading to a more robust overall model that is less swayed by irrelevant or noisy features.\n",
        "\n",
        "Analogy:\n",
        "\n",
        "Imagine you have a group of experts trying to make a prediction. Each expert is given a slightly different set of information about the problem. By combining their individual predictions, you are likely to get a more accurate and reliable answer than by relying on a single expert who might be biased by their limited information. Bagging works similarly by combining the predictions of multiple models trained on different subsets of the data, leading to a more robust and less overfit model."
      ],
      "metadata": {
        "id": "9VeoP-gaYEd7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. Why is Random Forest better than a single Decision Tree\u0010\n",
        "\n",
        "\n",
        "\n",
        "ans:-Why Random Forest Outperforms a Single Decision Tree\n",
        "\n",
        "Random Forest, an ensemble learning method, often outperforms a single Decision Tree due to its ability to reduce overfitting and improve prediction accuracy. Here's a breakdown of the key reasons:\n",
        "\n",
        "Reduced Overfitting:\n",
        "\n",
        "Decision Trees are prone to overfitting, where they learn the training data too well and fail to generalize to unseen data.\n",
        "Random Forest mitigates this by creating an ensemble of trees, each trained on a different subset of the data and using a random subset of features. This diversity prevents individual trees from memorizing the training data too closely, leading to a more robust and generalized model.\n",
        "Improved Accuracy:\n",
        "\n",
        "By combining the predictions of multiple trees, Random Forest reduces the impact of individual tree errors and produces a more accurate overall prediction.\n",
        "The aggregation of predictions from diverse trees helps to smooth out noise and capture a wider range of patterns in the data, leading to better performance on unseen data.\n",
        "Robustness to Noise and Outliers:\n",
        "\n",
        "Random Forest is less sensitive to noise and outliers in the data compared to a single Decision Tree.\n",
        "The ensemble nature of Random Forest helps to mitigate the influence of individual data points that might be erroneous or atypical, leading to a more stable and reliable model.\n",
        "Handling High-Dimensional Data:\n",
        "\n",
        "Random Forest can effectively handle datasets with a large number of features.\n",
        "The random feature selection at each node of the trees helps to reduce the dimensionality of the problem and focus on the most relevant features, improving computational efficiency and preventing overfitting.\n",
        "Feature Importance:\n",
        "\n",
        "Random Forest provides insights into the importance of different features in the prediction process.\n",
        "By analyzing the frequency and impact of features used for splitting across all trees, you can gain a better understanding of the underlying relationships in your data.\n",
        "Analogy:\n",
        "\n",
        "Imagine you need to make an important decision. Instead of relying on a single person's opinion, you consult multiple experts with diverse backgrounds and perspectives. By combining their insights, you are likely to make a more informed and reliable decision. Random Forest works similarly by combining the predictions of multiple trees, each with a different view of the data, leading to a more robust and accurate model."
      ],
      "metadata": {
        "id": "0Bo_cQ5VYNGb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18.What is the role of bootstrap sampling in Bagging\u0010\n",
        "\n",
        "\n",
        "\n",
        "ans:-Role of Bootstrap Sampling in Bagging\n",
        "\n",
        "Bootstrap sampling plays a crucial role in Bagging (Bootstrap Aggregating) by creating diverse training datasets for individual models in the ensemble. This diversity is key to Bagging's effectiveness in reducing overfitting and improving prediction accuracy.\n",
        "\n",
        "Here's how bootstrap sampling works within Bagging:\n",
        "\n",
        "Creating Bootstrap Samples:\n",
        "\n",
        "From the original training dataset, multiple subsets (bootstrap samples) are created by randomly selecting data points with replacement.\n",
        "Each bootstrap sample is the same size as the original dataset, but because of the replacement, some data points may appear multiple times, while others might be omitted.\n",
        "Training Base Models:\n",
        "\n",
        "A separate base model (e.g., a decision tree) is trained on each bootstrap sample.\n",
        "Since each sample is slightly different, the resulting models will have variations in their structure and learned patterns.\n",
        "Diversity and Reduced Overfitting:\n",
        "\n",
        "This diversity among the base models is the key benefit of bootstrap sampling.\n",
        "By training models on different subsets of the data, Bagging reduces the risk of overfitting to specific nuances or noise present in the original dataset.\n",
        "Each model learns a slightly different representation of the underlying relationships in the data.\n",
        "Aggregation for Robust Predictions:\n",
        "\n",
        "When making predictions, the outputs of all the individual models are combined, typically by averaging (for regression) or majority voting (for classification).\n",
        "This aggregation process further reduces variance and improves the overall prediction accuracy and robustness of the ensemble.\n",
        "Analogy:\n",
        "\n",
        "Imagine you have a group of students studying for an exam. Instead of all students using the same textbook, each student is given a different set of practice questions randomly selected from a larger pool. This ensures that each student learns different aspects of the material and has a unique perspective. When they come together to discuss the exam, they can share their knowledge and collectively arrive at a better understanding of the subject. Bootstrap sampling in Bagging works similarly by creating diverse training datasets, leading to a more robust and generalized ensemble model."
      ],
      "metadata": {
        "id": "H6p2HnqXYWEc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Real-World Applications of Ensemble Techniques\n",
        "\n",
        "Ensemble techniques are widely used in various domains due to their ability to improve prediction accuracy and model robustness. Here are some prominent real-world applications:\n",
        "\n",
        "Financial Modeling:\n",
        "\n",
        "Credit Risk Assessment: Ensemble methods are used to predict the likelihood of loan defaults, helping financial institutions make informed lending decisions.\n",
        "Fraud Detection: Ensembles can identify fraudulent transactions by combining patterns detected by different models, reducing financial losses.\n",
        "Algorithmic Trading: Ensemble models are used to predict stock prices and make automated trading decisions, potentially increasing profitability.\n",
        "Healthcare:\n",
        "\n",
        "Disease Diagnosis: Ensembles can assist in diagnosing diseases by combining insights from various medical data sources, such as imaging, lab results, and patient history.\n",
        "Personalized Medicine: Ensemble methods help tailor treatment plans based on individual patient characteristics and predicted responses to different therapies.\n",
        "Drug Discovery: Ensembles can accelerate drug discovery by predicting the effectiveness of potential drug candidates, reducing research time and costs.\n",
        "Marketing and Sales:\n",
        "\n",
        "Customer Churn Prediction: Ensemble models predict customer churn, allowing businesses to implement retention strategies.\n",
        "Targeted Advertising: Ensembles help personalize advertising campaigns by predicting customer preferences and responses to different ads.\n",
        "Sales Forecasting: Ensemble models can forecast future sales based on historical data and market trends, aiding in inventory management and resource allocation.\n",
        "Image and Object Recognition:\n",
        "\n",
        "Self-Driving Cars: Ensemble methods are crucial for object detection and scene understanding in autonomous vehicles, enabling safe navigation.\n",
        "Facial Recognition: Ensembles are used in facial recognition systems for security and authentication purposes.\n",
        "Medical Imaging Analysis: Ensemble models can assist in identifying tumors or other anomalies in medical images, improving diagnostic accuracy.\n",
        "Natural Language Processing (NLP):\n",
        "\n",
        "Sentiment Analysis: Ensembles can analyze text to determine the sentiment expressed, such as positive, negative, or neutral, for applications like social media monitoring.\n",
        "Machine Translation: Ensemble methods improve the accuracy of machine translation systems by combining predictions from different translation models.\n",
        "Spam Filtering: Ensembles can effectively filter spam emails by identifying patterns indicative of unwanted messages.\n",
        "These are just a few examples, and the applications of ensemble techniques are continuously expanding as the field of machine learning evolves. The ability of ensembles to combine the strengths of multiple models makes them valuable tools for solving complex real-world problems across various industries."
      ],
      "metadata": {
        "id": "fovldrQwYfyX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20.What is the difference between Bagging and Boosting?\n",
        "\n",
        "\n",
        "\n",
        "ans:-Bagging vs. Boosting\n",
        "\n",
        "Bagging and Boosting are both ensemble techniques that combine multiple models to improve prediction accuracy, but they differ in their approach:\n",
        "\n",
        "Bagging (Bootstrap Aggregating)\n",
        "\n",
        "Focus: Reducing variance and overfitting.\n",
        "Method:\n",
        "Creates multiple subsets of the training data using bootstrapping (random sampling with replacement).\n",
        "Trains a separate base model (often a decision tree) on each subset.\n",
        "Combines predictions by averaging (regression) or majority voting (classification).\n",
        "Model Independence: Base models are trained independently and in parallel.\n",
        "Example: Random Forest.\n",
        "Boosting\n",
        "\n",
        "Focus: Reducing bias and improving accuracy.\n",
        "Method:\n",
        "Trains base models sequentially, where each subsequent model focuses on correcting errors made by previous models.\n",
        "Assigns weights to data points, giving higher weight to misclassified instances.\n",
        "Combines predictions using a weighted average.\n",
        "Model Dependence: Base models are trained iteratively, with each model building upon the previous ones.\n",
        "Example: AdaBoost, Gradient Boosting, XGBoost.\n",
        "Here's a table summarizing the key differences:\n",
        "\n",
        "Feature\tBagging\tBoosting\n",
        "Goal\tReduce variance, overfitting\tReduce bias, improve accuracy\n",
        "Model Training\tParallel, independent\tSequential, dependent\n",
        "Data Sampling\tBootstrapping\tWeighted data points\n",
        "Prediction Combination\tAveraging, majority voting\tWeighted average\n",
        "Base Model\tTypically Decision Trees\tCan be various models\n",
        "Example\tRandom Forest\tAdaBoost, Gradient Boosting\n",
        "In simpler terms:\n",
        "\n",
        "Bagging: Like having multiple independent experts who provide their opinions, and the final decision is based on averaging or voting.\n",
        "Boosting: Like having a team of experts where each expert focuses on improving the areas where previous experts made mistakes.\n",
        "Choosing between Bagging and Boosting:\n",
        "\n",
        "Bagging is generally preferred when the base model has high variance (e.g., Decision Trees), and the goal is to reduce overfitting.\n",
        "Boosting is preferred when the base model has high bias (e.g., weak learners), and the goal is to improve accuracy.\n",
        "Ultimately, the best choice depends on the specific problem and the characteristics of the dataset. It's often beneficial to experiment with both techniques and compare their performance."
      ],
      "metadata": {
        "id": "NlautRnDY2Qc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "practical"
      ],
      "metadata": {
        "id": "OiNvygGIZAmW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21.Train a Bagging Classifier using Decision Trees on a sample dataset and print model accuracy2\n",
        "\n",
        "\n",
        "\n",
        "ans:-"
      ],
      "metadata": {
        "id": "yXIlJ2n_ZEDM"
      }
    },
    {
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Mf1L8W6jZK7J"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1BbFPFrBZLOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "0qsdtd3tZMyX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "bagging_classifier = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=10, random_state=42)\n",
        "bagging_classifier.fit(X_train, y_train)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "wznYHx7JZNc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "y_pred = bagging_classifier.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "vNhK61QsZN5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the Bagging Classifier\n",
        "bagging_classifier = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=10, random_state=42)\n",
        "bagging_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred = bagging_classifier.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "tP_JfFfvZOcR",
        "outputId": "4f56e4be-8304-4304-8783-76a5fd97e651"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "BaggingClassifier.__init__() got an unexpected keyword argument 'base_estimator'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-b7aea508d967>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Create and train the Bagging Classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mbagging_classifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBaggingClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDecisionTreeClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mbagging_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: BaggingClassifier.__init__() got an unexpected keyword argument 'base_estimator'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. Train a Bagging Regressor using Decision Trees and evaluate using Mean Squared Error (MSE)2\n",
        "\n",
        "\n",
        "\n",
        "ans:-"
      ],
      "metadata": {
        "id": "d_7i4jTKZZUU"
      }
    },
    {
      "source": [
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.datasets import load_boston"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Tox6rNprZiHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "boston = load_boston()  # Load the Boston Housing dataset\n",
        "X, y = boston.data, boston.target"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "UA8YVRiYZi2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "nsm46eprZjwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "bagging_regressor = BaggingRegressor(base_estimator=DecisionTreeRegressor(), n_estimators=10, random_state=42)\n",
        "bagging_regressor.fit(X_train, y_train)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "ic-Oe0j3ZkHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "y_pred = bagging_regressor.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error: {mse}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "RXnV2KfKZklA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.datasets import load_boston\n",
        "\n",
        "# Load the Boston Housing dataset\n",
        "boston = load_boston()\n",
        "X, y = boston.data, boston.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the Bagging Regressor\n",
        "bagging_regressor = BaggingRegressor(base_estimator=DecisionTreeRegressor(), n_estimators=10, random_state=42)\n",
        "bagging_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and evaluate MSE\n",
        "y_pred = bagging_regressor.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error: {mse}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7IRbQleHZk-z",
        "outputId": "b3532619-f0c1-4908-8018-7d95bfb6efc7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "\n`load_boston` has been removed from scikit-learn since version 1.2.\n\nThe Boston housing prices dataset has an ethical problem: as\ninvestigated in [1], the authors of this dataset engineered a\nnon-invertible variable \"B\" assuming that racial self-segregation had a\npositive impact on house prices [2]. Furthermore the goal of the\nresearch that led to the creation of this dataset was to study the\nimpact of air quality but it did not give adequate demonstration of the\nvalidity of this assumption.\n\nThe scikit-learn maintainers therefore strongly discourage the use of\nthis dataset unless the purpose of the code is to study and educate\nabout ethical issues in data science and machine learning.\n\nIn this special case, you can fetch the dataset from the original\nsource::\n\n    import pandas as pd\n    import numpy as np\n\n    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n    target = raw_df.values[1::2, 2]\n\nAlternative datasets include the California housing dataset and the\nAmes housing dataset. You can load the datasets as follows::\n\n    from sklearn.datasets import fetch_california_housing\n    housing = fetch_california_housing()\n\nfor the California housing dataset and::\n\n    from sklearn.datasets import fetch_openml\n    housing = fetch_openml(name=\"house_prices\", as_frame=True)\n\nfor the Ames housing dataset.\n\n[1] M Carlisle.\n\"Racist data destruction?\"\n<https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>\n\n[2] Harrison Jr, David, and Daniel L. Rubinfeld.\n\"Hedonic housing prices and the demand for clean air.\"\nJournal of environmental economics and management 5.1 (1978): 81-102.\n<https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-0755a0fd6519>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_boston\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Load the Boston Housing dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/datasets/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    159\u001b[0m             \"\"\"\n\u001b[1;32m    160\u001b[0m         )\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: \n`load_boston` has been removed from scikit-learn since version 1.2.\n\nThe Boston housing prices dataset has an ethical problem: as\ninvestigated in [1], the authors of this dataset engineered a\nnon-invertible variable \"B\" assuming that racial self-segregation had a\npositive impact on house prices [2]. Furthermore the goal of the\nresearch that led to the creation of this dataset was to study the\nimpact of air quality but it did not give adequate demonstration of the\nvalidity of this assumption.\n\nThe scikit-learn maintainers therefore strongly discourage the use of\nthis dataset unless the purpose of the code is to study and educate\nabout ethical issues in data science and machine learning.\n\nIn this special case, you can fetch the dataset from the original\nsource::\n\n    import pandas as pd\n    import numpy as np\n\n    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n    target = raw_df.values[1::2, 2]\n\nAlternative datasets include the California housing dataset and the\nAmes housing dataset. You can load the datasets as follows::\n\n    from sklearn.datasets import fetch_california_housing\n    housing = fetch_california_housing()\n\nfor the California housing dataset and::\n\n    from sklearn.datasets import fetch_openml\n    housing = fetch_openml(name=\"house_prices\", as_frame=True)\n\nfor the Ames housing dataset.\n\n[1] M Carlisle.\n\"Racist data destruction?\"\n<https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>\n\n[2] Harrison Jr, David, and Daniel L. Rubinfeld.\n\"Hedonic housing prices and the demand for clean air.\"\nJournal of environmental economics and management 5.1 (1978): 81-102.\n<https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>\n",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. Train a Random Forest Classifier on the Breast Cancer dataset and print feature importance scores2\n",
        "\n",
        "\n",
        "\n",
        "ans:-"
      ],
      "metadata": {
        "id": "gIk__DQOZms2"
      }
    },
    {
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "import pandas as pd"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "U4KSaUleZuEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "breast_cancer = load_breast_cancer()\n",
        "X, y = breast_cancer.data, breast_cancer.target\n",
        "feature_names = breast_cancer.feature_names"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "9eZAHBoXZuZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "27bnGcvqZu_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)  # You can adjust n_estimators\n",
        "rf_classifier.fit(X_train, y_train)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "ScN9ljbkZvif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "feature_importances = pd.DataFrame({'feature': feature_names, 'importance': rf_classifier.feature_importances_})\n",
        "feature_importances = feature_importances.sort_values('importance', ascending=False)\n",
        "print(feature_importances)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "TnW0x9uoZwCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X, y = breast_cancer.data, breast_cancer.target\n",
        "feature_names = breast_cancer.feature_names\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Get and print feature importance scores\n",
        "feature_importances = pd.DataFrame({'feature': feature_names, 'importance': rf_classifier.feature_importances_})\n",
        "feature_importances = feature_importances.sort_values('importance', ascending=False)\n",
        "print(feature_importances)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wsVoDm1ZwZ9",
        "outputId": "d9f04294-176f-4169-f7cc-72a46707251b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                    feature  importance\n",
            "23               worst area    0.153892\n",
            "27     worst concave points    0.144663\n",
            "7       mean concave points    0.106210\n",
            "20             worst radius    0.077987\n",
            "6            mean concavity    0.068001\n",
            "22          worst perimeter    0.067115\n",
            "2            mean perimeter    0.053270\n",
            "0               mean radius    0.048703\n",
            "3                 mean area    0.047555\n",
            "26          worst concavity    0.031802\n",
            "13               area error    0.022407\n",
            "21            worst texture    0.021749\n",
            "25        worst compactness    0.020266\n",
            "10             radius error    0.020139\n",
            "5          mean compactness    0.013944\n",
            "1              mean texture    0.013591\n",
            "12          perimeter error    0.011303\n",
            "24         worst smoothness    0.010644\n",
            "28           worst symmetry    0.010120\n",
            "16          concavity error    0.009386\n",
            "4           mean smoothness    0.007285\n",
            "19  fractal dimension error    0.005321\n",
            "15        compactness error    0.005253\n",
            "29  worst fractal dimension    0.005210\n",
            "11            texture error    0.004724\n",
            "14         smoothness error    0.004271\n",
            "18           symmetry error    0.004018\n",
            "9    mean fractal dimension    0.003886\n",
            "8             mean symmetry    0.003770\n",
            "17     concave points error    0.003513\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. Train a Random Forest Regressor and compare its performance with a single Decision Tree2\n",
        "\n",
        "\n",
        "\n",
        "ans:-"
      ],
      "metadata": {
        "id": "q3CDW8KZZyCr"
      }
    },
    {
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.datasets import load_boston"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Giw2-YqtZ3tG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "boston = load_boston()\n",
        "X, y = boston.data, boston.target"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Uc4lQp2DZ4AX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "_SYKju7dZ4WC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Random Forest Regressor\n",
        "rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)  # You can adjust n_estimators\n",
        "rf_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Decision Tree Regressor\n",
        "dt_regressor = DecisionTreeRegressor(random_state=42)\n",
        "dt_regressor.fit(X_train, y_train)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "ftLcfJAhZ4sO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Random Forest\n",
        "rf_predictions = rf_regressor.predict(X_test)\n",
        "rf_mse = mean_squared_error(y_test, rf_predictions)\n",
        "\n",
        "# Decision Tree\n",
        "dt_predictions = dt_regressor.predict(X_test)\n",
        "dt_mse = mean_squared_error(y_test, dt_predictions)\n",
        "\n",
        "print(f\"Random Forest MSE: {rf_mse}\")\n",
        "print(f\"Decision Tree MSE: {dt_mse}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "UqbKSrHYZ5EN",
        "outputId": "519a4da1-3109-4c9e-fb76-f445d4727188"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'rf_regressor' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-23512817c0ce>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Random Forest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrf_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrf_regressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mrf_mse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrf_predictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Decision Tree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'rf_regressor' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KK0sXLqSZPHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Random Forest\n",
        "rf_predictions = rf_regressor.predict(X_test)\n",
        "rf_mse = mean_squared_error(y_test, rf_predictions)\n",
        "\n",
        "# Decision Tree\n",
        "dt_predictions = dt_regressor.predict(X_test)\n",
        "dt_mse = mean_squared_error(y_test, dt_predictions)\n",
        "\n",
        "print(f\"Random Forest MSE: {rf_mse}\")\n",
        "print(f\"Decision Tree MSE: {dt_mse}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "BvZYb2VLZ6aZ",
        "outputId": "34068262-8f1a-4f16-b25c-bf93b48183a4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'rf_regressor' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-23512817c0ce>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Random Forest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrf_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrf_regressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mrf_mse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrf_predictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Decision Tree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'rf_regressor' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "25.Compute the Out-of-Bag (OOB) Score for a Random Forest Classifier2\n",
        "\n",
        "\n",
        "\n",
        "ans:-"
      ],
      "metadata": {
        "id": "zExneHZ2Z8f3"
      }
    },
    {
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_breast_cancer"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "EmfnU1HbaF_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "breast_cancer = load_breast_cancer()\n",
        "X, y = breast_cancer.data, breast_cancer.target"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "m8YrfzrhaGPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "SnedZhsMaGi_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "rf_classifier = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=42)  # Enable oob_score\n",
        "rf_classifier.fit(X_train, y_train)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "LFJPOv91aHJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "oob_score = rf_classifier.oob_score_\n",
        "print(f\"OOB Score: {oob_score}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "id": "7RAf6GbeaHmO",
        "outputId": "f1f59501-e784-4d07-a7f2-dc150aaa9a67"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'RandomForestClassifier' object has no attribute 'oob_score_'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-c96b638164fc>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moob_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrf_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moob_score_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"OOB Score: {oob_score}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'RandomForestClassifier' object has no attribute 'oob_score_'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hMXxIua8Z6qZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X, y = breast_cancer.data, breast_cancer.target\n",
        "\n",
        "# Split data into training and testing sets (optional)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the Random Forest Classifier with OOB score enabled\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=42)\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Get and print the OOB score\n",
        "oob_score = rf_classifier.oob_score_\n",
        "print(f\"OOB Score: {oob_score}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6fWJzUuaJro",
        "outputId": "04470b27-28e8-48bf-f0a2-ce74fd94819c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OOB Score: 0.9560439560439561\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "26. Train a Bagging Classifier using SVM as a base estimator and print accuracy2\n",
        "\n",
        "\n",
        "ans:-"
      ],
      "metadata": {
        "id": "JlyGuaFeaK88"
      }
    },
    {
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_breast_cancer  # Using Breast Cancer dataset for this example\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "fLV_vsYgacNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "breast_cancer = load_breast_cancer()\n",
        "X, y = breast_cancer.data, breast_cancer.target"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "T495IZQDaczH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "7psb3zD7adjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "bagging_classifier = BaggingClassifier(base_estimator=SVC(), n_estimators=10, random_state=42)\n",
        "bagging_classifier.fit(X_train, y_train)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "py3e8mhUad8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "bagging_classifier = BaggingClassifier(base_estimator=SVC(), n_estimators=10, random_state=42)\n",
        "bagging_classifier.fit(X_train, y_train)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "lN84rMdpaeFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "bagging_classifier = BaggingClassifier(base_estimator=SVC(), n_estimators=10, random_state=42)\n",
        "bagging_classifier.fit(X_train, y_train)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "LrUbp9yfae4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "y_pred = bagging_classifier.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "6-fhfz8lafOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X, y = breast_cancer.data, breast_cancer.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the Bagging Classifier with SVM base estimator\n",
        "bagging_classifier = BaggingClassifier(base_estimator=SVC(), n_estimators=10, random_state=42)\n",
        "bagging_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred = bagging_classifier.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "xG8JiAwuafuZ",
        "outputId": "b3e1f231-54d6-4be0-f558-8c6a8bf6a050"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "BaggingClassifier.__init__() got an unexpected keyword argument 'base_estimator'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-dc3a8dd3d682>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Create and train the Bagging Classifier with SVM base estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mbagging_classifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBaggingClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mbagging_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: BaggingClassifier.__init__() got an unexpected keyword argument 'base_estimator'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "27. Train a Random Forest Classifier with different numbers of trees and compare accuracy2\n",
        "\n",
        "\n",
        "\n",
        "ans:-"
      ],
      "metadata": {
        "id": "BA0UmqEwahGd"
      }
    },
    {
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_breast_cancer"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "EGO7oIAiaryc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "breast_cancer = load_breast_cancer()\n",
        "X, y = breast_cancer.data, breast_cancer.target"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "RZoDJfZAasFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "wV8wIV_jask5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "n_estimators_list = [10, 50, 100, 200, 500]  # List of number of trees to try\n",
        "accuracy_scores = []\n",
        "\n",
        "for n_estimators in n_estimators_list:\n",
        "    rf_classifier = RandomForestClassifier(n_estimators=n_estimators, random_state=42)\n",
        "    rf_classifier.fit(X_train, y_train)\n",
        "    y_pred = rf_classifier.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracy_scores.append(accuracy)\n",
        "    print(f\"Accuracy with {n_estimators} trees: {accuracy}\")\n",
        "\n",
        "# Create a DataFrame to store the results\n",
        "results = pd.DataFrame({'n_estimators': n_estimators_list, 'accuracy': accuracy_scores})\n",
        "\n",
        "# Plot the results\n",
        "plt.plot(results['n_estimators'], results['accuracy'], marker='o')\n",
        "plt.xlabel('Number of Trees (n_estimators)')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy vs. Number of Trees in Random Forest')\n",
        "plt.grid(True)\n",
        "plt.show()  # To display the plot in Colab, you might need to use plt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "BcLu54Nqas5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X, y = breast_cancer.data, breast_cancer.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train and evaluate for different numbers of trees\n",
        "n_estimators_list = [10, 50, 100, 200, 500]\n",
        "accuracy_scores = []\n",
        "\n",
        "for n_estimators in n_estimators_list:\n",
        "    rf_classifier = RandomForestClassifier(n_estimators=n_estimators, random_state=42)\n",
        "    rf_classifier.fit(X_train, y_train)\n",
        "    y_pred = rf_classifier.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracy_scores.append(accuracy)\n",
        "    print(f\"Accuracy with {n_estimators} trees: {accuracy}\")\n",
        "\n",
        "# Create a DataFrame to store the results\n",
        "results = pd.DataFrame({'n_estimators': n_estimators_list, 'accuracy': accuracy_scores})\n",
        "\n",
        "# Plot the results\n",
        "plt.plot(results['n_estimators'], results['accuracy'], marker='o')\n",
        "plt.xlabel('Number of Trees (n_estimators)')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy vs. Number of Trees in Random Forest')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 563
        },
        "id": "bjNzqW-qatwr",
        "outputId": "79296ec5-f036-45a9-9015-403c048d6a12"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with 10 trees: 0.956140350877193\n",
            "Accuracy with 50 trees: 0.9649122807017544\n",
            "Accuracy with 100 trees: 0.9649122807017544\n",
            "Accuracy with 200 trees: 0.9649122807017544\n",
            "Accuracy with 500 trees: 0.9649122807017544\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "28. Train a Bagging Classifier using Logistic Regression as a base estimator and print AUC score2\n",
        "\n",
        "\n",
        "\n",
        "ans:-"
      ],
      "metadata": {
        "id": "RI-ZoIAMa2uU"
      }
    },
    {
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.datasets import load_breast_cancer"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "aPAqFwkBa7_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "29. Train a Random Forest Regressor and analyze feature importance scores2\n",
        "\n",
        "\n",
        "\n",
        "ans:-"
      ],
      "metadata": {
        "id": "4XegSCsxa-VQ"
      }
    },
    {
      "source": [
        "#1. Import necessary libraries:\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_boston"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "I4fI5cSbbQrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "#2. Load the sample dataset:\n",
        "boston = load_boston()\n",
        "X, y = boston.data, boston.target\n",
        "feature_names = boston.feature_names"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "F3xeUd_gbThI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "#Split data into training and testing sets:\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "0XOIRkdpbYrp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "#4. Create and train the Random Forest Regressor:\n",
        "rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)  # You can adjust n_estimators\n",
        "rf_regressor.fit(X_train, y_train)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "_MJ71wYibew0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "#5. Get and analyze feature importance scores:\n",
        "feature_importances = pd.DataFrame({'feature': feature_names, 'importance': rf_regressor.feature_importances_})\n",
        "feature_importances = feature_importances.sort_values('importance', ascending=False)\n",
        "print(feature_importances)\n",
        "\n",
        "# Plot feature importances\n",
        "plt.bar(feature_importances['feature'], feature_importances['importance'])\n",
        "plt.xlabel('Features')\n",
        "plt.ylabel('Importance')\n",
        "plt.title('Feature Importance in Random Forest Regressor')\n",
        "plt.xticks(rotation=90)  # Rotate x-axis labels for better readability\n",
        "plt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "RUcUCYAvbj6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_boston\n",
        "\n",
        "# Load the Boston Housing dataset\n",
        "boston = load_boston()\n",
        "X, y = boston.data, boston.target\n",
        "feature_names = boston.feature_names\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the Random Forest Regressor\n",
        "rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Get and analyze feature importance scores\n",
        "feature_importances = pd.DataFrame({'feature': feature_names, 'importance': rf_regressor.feature_importances_})\n",
        "feature_importances = feature_importances.sort_values('importance', ascending=False)\n",
        "print(feature_importances)\n",
        "\n",
        "# Plot feature importances\n",
        "plt.bar(feature_importances['feature'], feature_importances['importance'])\n",
        "plt.xlabel('Features')\n",
        "plt.ylabel('Importance')\n",
        "plt.title('Feature Importance in Random Forest Regressor')\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "28iP3sxSbnaL",
        "outputId": "b1c97309-7f95-4143-d0a5-5bc0c912b780"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "\n`load_boston` has been removed from scikit-learn since version 1.2.\n\nThe Boston housing prices dataset has an ethical problem: as\ninvestigated in [1], the authors of this dataset engineered a\nnon-invertible variable \"B\" assuming that racial self-segregation had a\npositive impact on house prices [2]. Furthermore the goal of the\nresearch that led to the creation of this dataset was to study the\nimpact of air quality but it did not give adequate demonstration of the\nvalidity of this assumption.\n\nThe scikit-learn maintainers therefore strongly discourage the use of\nthis dataset unless the purpose of the code is to study and educate\nabout ethical issues in data science and machine learning.\n\nIn this special case, you can fetch the dataset from the original\nsource::\n\n    import pandas as pd\n    import numpy as np\n\n    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n    target = raw_df.values[1::2, 2]\n\nAlternative datasets include the California housing dataset and the\nAmes housing dataset. You can load the datasets as follows::\n\n    from sklearn.datasets import fetch_california_housing\n    housing = fetch_california_housing()\n\nfor the California housing dataset and::\n\n    from sklearn.datasets import fetch_openml\n    housing = fetch_openml(name=\"house_prices\", as_frame=True)\n\nfor the Ames housing dataset.\n\n[1] M Carlisle.\n\"Racist data destruction?\"\n<https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>\n\n[2] Harrison Jr, David, and Daniel L. Rubinfeld.\n\"Hedonic housing prices and the demand for clean air.\"\nJournal of environmental economics and management 5.1 (1978): 81-102.\n<https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-b06829ade47a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_boston\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Load the Boston Housing dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/datasets/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    159\u001b[0m             \"\"\"\n\u001b[1;32m    160\u001b[0m         )\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: \n`load_boston` has been removed from scikit-learn since version 1.2.\n\nThe Boston housing prices dataset has an ethical problem: as\ninvestigated in [1], the authors of this dataset engineered a\nnon-invertible variable \"B\" assuming that racial self-segregation had a\npositive impact on house prices [2]. Furthermore the goal of the\nresearch that led to the creation of this dataset was to study the\nimpact of air quality but it did not give adequate demonstration of the\nvalidity of this assumption.\n\nThe scikit-learn maintainers therefore strongly discourage the use of\nthis dataset unless the purpose of the code is to study and educate\nabout ethical issues in data science and machine learning.\n\nIn this special case, you can fetch the dataset from the original\nsource::\n\n    import pandas as pd\n    import numpy as np\n\n    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n    target = raw_df.values[1::2, 2]\n\nAlternative datasets include the California housing dataset and the\nAmes housing dataset. You can load the datasets as follows::\n\n    from sklearn.datasets import fetch_california_housing\n    housing = fetch_california_housing()\n\nfor the California housing dataset and::\n\n    from sklearn.datasets import fetch_openml\n    housing = fetch_openml(name=\"house_prices\", as_frame=True)\n\nfor the Ames housing dataset.\n\n[1] M Carlisle.\n\"Racist data destruction?\"\n<https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>\n\n[2] Harrison Jr, David, and Daniel L. Rubinfeld.\n\"Hedonic housing prices and the demand for clean air.\"\nJournal of environmental economics and management 5.1 (1978): 81-102.\n<https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>\n",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "30.Train an ensemble model using both Bagging and Random Forest and compare accuracy.\n",
        "\n",
        "\n",
        "\n",
        "as:-"
      ],
      "metadata": {
        "id": "yo8m2SxXbpKf"
      }
    },
    {
      "source": [
        "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier  # Base estimator for Bagging\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_breast_cancer"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "33-uDCCZcXfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "RTxPAMd7cXzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "breast_cancer = load_breast_cancer()\n",
        "X, y = breast_cancer.data, breast_cancer.target"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "ao_y-9l0cYCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Bagging Classifier\n",
        "bagging_classifier = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=100, random_state=42)\n",
        "bagging_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_classifier.fit(X_train, y_train)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "ppOvrMuWcYXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Bagging Classifier\n",
        "bagging_predictions = bagging_classifier.predict(X_test)\n",
        "bagging_accuracy = accuracy_score(y_test, bagging_predictions)\n",
        "\n",
        "# Random Forest Classifier\n",
        "rf_predictions = rf_classifier.predict(X_test)\n",
        "rf_accuracy = accuracy_score(y_test, rf_predictions)\n",
        "\n",
        "print(f\"Bagging Classifier Accuracy: {bagging_accuracy}\")\n",
        "print(f\"Random Forest Classifier Accuracy: {rf_accuracy}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "fgvtm9BQcY1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X, y = breast_cancer.data, breast_cancer.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the models\n",
        "bagging_classifier = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=100, random_state=42)\n",
        "bagging_classifier.fit(X_train, y_train)\n",
        "\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "bagging_predictions = bagging_classifier.predict(X_test)\n",
        "bagging_accuracy = accuracy_score(y_test, bagging_predictions)\n",
        "\n",
        "rf_predictions = rf_classifier.predict(X_test)\n",
        "rf_accuracy = accuracy_score(y_test, rf_predictions)\n",
        "\n",
        "print(f\"Bagging Classifier Accuracy: {bagging_accuracy}\")\n",
        "print(f\"Random Forest Classifier Accuracy: {rf_accuracy}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "bAUlxoRUcZSy",
        "outputId": "1c6c7568-399d-40a5-ced3-807beae2ef5b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "BaggingClassifier.__init__() got an unexpected keyword argument 'base_estimator'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-afd82ea4442b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Create and train the models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mbagging_classifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBaggingClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDecisionTreeClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mbagging_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: BaggingClassifier.__init__() got an unexpected keyword argument 'base_estimator'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "31.Train a Random Forest Classifier and tune hyperparameters using GridSearchCV=\n",
        "\n",
        "\n",
        "ans:-"
      ],
      "metadata": {
        "id": "XHqTv7iectGU"
      }
    },
    {
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_breast_cancer"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "33uvMscFcy15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "breast_cancer = load_breast_cancer()\n",
        "X, y = breast_cancer.data, breast_cancer.target"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "0h5giHDlczIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "U0Wnxh1yczby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],  # Number of trees in the forest\n",
        "    'max_depth': [None, 5, 10],  # Maximum depth of the trees\n",
        "    'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split an internal node\n",
        "    'min_samples_leaf': [1, 2, 4]  # Minimum number of samples required to be at a leaf node\n",
        "}"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Rjh2EyA0cz_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "rf_classifier = RandomForestClassifier(random_state=42)\n",
        "grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, scoring='accuracy', cv=5)  # 5-fold cross-validation\n",
        "grid_search.fit(X_train, y_train)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "gmEVNanSc0Y-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Best Hyperparameters: {best_params}\")\n",
        "print(f\"Accuracy on Test Set: {accuracy}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "gl1okc16c3Vo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X, y = breast_cancer.data, breast_cancer.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [None, 5, 10],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# Create and train the model using GridSearchCV\n",
        "rf_classifier = RandomForestClassifier(random_state=42)\n",
        "grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, scoring='accuracy', cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best hyperparameters and evaluate on the test set\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Best Hyperparameters: {best_params}\")\n",
        "print(f\"Accuracy on Test Set: {accuracy}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "id": "QCFomTiMc3y-",
        "outputId": "dbd16df4-a3fd-4599-e069-2c5c76362173"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-de1715af68e0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mrf_classifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mgrid_search\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrf_classifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# Get the best hyperparameters and evaluate on the test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1022\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1569\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1570\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1571\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1572\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    968\u001b[0m                     )\n\u001b[1;32m    969\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 970\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    971\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    972\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         )\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1917\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1920\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    864\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 866\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    485\u001b[0m             \u001b[0;31m# parallel_backend contexts set at a higher level,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             \u001b[0;31m# since correctness does not rely on using threads.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m             trees = Parallel(\n\u001b[0m\u001b[1;32m    488\u001b[0m                 \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         )\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1917\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1920\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"balanced\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         tree._fit(\n\u001b[0m\u001b[1;32m    190\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    470\u001b[0m             )\n\u001b[1;32m    471\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 472\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing_values_in_feature_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "32. Train a Bagging Regressor with different numbers of base estimators and compare performance=\n",
        "\n",
        "\n",
        "\n",
        "ans:-"
      ],
      "metadata": {
        "id": "PJv_3SzIdF6l"
      }
    },
    {
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor  # Base estimator\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.datasets import load_boston"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "pkWZF_BodME1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "boston = load_boston()\n",
        "X, y = boston.data, boston.target"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "FKobsokrdMZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "44NQdU5KdM_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "n_estimators_list = [10, 50, 100, 200, 500]  # List of number of base estimators to try\n",
        "mse_scores = []\n",
        "\n",
        "for n_estimators in n_estimators_list:\n",
        "    bagging_regressor = BaggingRegressor(base_estimator=DecisionTreeRegressor(), n_estimators=n_estimators, random_state=42)\n",
        "    bagging_regressor.fit(X_train, y_train)\n",
        "    y_pred = bagging_regressor.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    mse_scores.append(mse)\n",
        "    print(f\"MSE with {n_estimators} base estimators: {mse}\")\n",
        "\n",
        "# Create a DataFrame to store the results\n",
        "results = pd.DataFrame({'n_estimators': n_estimators_list, 'mse': mse_scores})\n",
        "\n",
        "# Plot the results\n",
        "plt.plot(results['n_estimators'], results['mse'], marker='o')\n",
        "plt.xlabel('Number of Base Estimators (n_estimators)')\n",
        "plt.ylabel('Mean Squared Error (MSE)')\n",
        "plt"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Tc2ynmWhdNOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "n_estimators_list = [10, 50, 100, 200, 500]  # List of number of base estimators to try\n",
        "mse_scores = []\n",
        "\n",
        "for n_estimators in n_estimators_list:\n",
        "    bagging_regressor = BaggingRegressor(base_estimator=DecisionTreeRegressor(), n_estimators=n_estimators, random_state=42)\n",
        "    bagging_regressor.fit(X_train, y_train)\n",
        "    y_pred = bagging_regressor.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    mse_scores.append(mse)\n",
        "    print(f\"MSE with {n_estimators} base estimators: {mse}\")\n",
        "\n",
        "# Create a DataFrame to store the results\n",
        "results = pd.DataFrame({'n_estimators': n_estimators_list, 'mse': mse_scores})\n",
        "\n",
        "# Plot the results\n",
        "plt.plot(results['n_estimators'], results['mse'], marker='o')\n",
        "plt.xlabel('Number of Base Estimators (n_estimators)')\n",
        "plt.ylabel('Mean Squared Error (MSE)')\n",
        "plt.title('MSE vs. Number of Base Estimators in Bagging Regressor')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "xlApSPbrdP6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.datasets import load_boston\n",
        "\n",
        "# Load the Boston Housing dataset\n",
        "boston = load_boston()\n",
        "X, y = boston.data, boston.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train and evaluate for different numbers of base estimators\n",
        "n_estimators_list = [10, 50, 100, 200, 500]\n",
        "mse_scores = []\n",
        "\n",
        "for n_estimators in n_estimators_list:\n",
        "    bagging_regressor = BaggingRegressor(base_estimator=DecisionTreeRegressor(), n_estimators=n_estimators, random_state=42)\n",
        "    bagging_regressor.fit(X_train, y_train)\n",
        "    y_pred = bagging_regressor.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    mse_scores.append(mse)\n",
        "    print(f\"MSE with {n_estimators} base estimators: {mse}\")\n",
        "\n",
        "# Create a DataFrame to store the results\n",
        "results = pd.DataFrame({'n_estimators': n_estimators_list, 'mse': mse_scores})\n",
        "\n",
        "# Plot the results\n",
        "plt.plot(results['n_estimators'], results['mse'], marker='o')\n",
        "plt.xlabel('Number of Base Estimators (n_estimators)')\n",
        "plt.ylabel('Mean Squared Error (MSE)')\n",
        "plt.title('MSE vs. Number of Base Estimators in Bagging Regressor')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xWD2A-aJdQqr",
        "outputId": "1483da87-08c1-4fa9-ccda-c4cacf6d55de"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "\n`load_boston` has been removed from scikit-learn since version 1.2.\n\nThe Boston housing prices dataset has an ethical problem: as\ninvestigated in [1], the authors of this dataset engineered a\nnon-invertible variable \"B\" assuming that racial self-segregation had a\npositive impact on house prices [2]. Furthermore the goal of the\nresearch that led to the creation of this dataset was to study the\nimpact of air quality but it did not give adequate demonstration of the\nvalidity of this assumption.\n\nThe scikit-learn maintainers therefore strongly discourage the use of\nthis dataset unless the purpose of the code is to study and educate\nabout ethical issues in data science and machine learning.\n\nIn this special case, you can fetch the dataset from the original\nsource::\n\n    import pandas as pd\n    import numpy as np\n\n    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n    target = raw_df.values[1::2, 2]\n\nAlternative datasets include the California housing dataset and the\nAmes housing dataset. You can load the datasets as follows::\n\n    from sklearn.datasets import fetch_california_housing\n    housing = fetch_california_housing()\n\nfor the California housing dataset and::\n\n    from sklearn.datasets import fetch_openml\n    housing = fetch_openml(name=\"house_prices\", as_frame=True)\n\nfor the Ames housing dataset.\n\n[1] M Carlisle.\n\"Racist data destruction?\"\n<https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>\n\n[2] Harrison Jr, David, and Daniel L. Rubinfeld.\n\"Hedonic housing prices and the demand for clean air.\"\nJournal of environmental economics and management 5.1 (1978): 81-102.\n<https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-cf74540c51bc>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_boston\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Load the Boston Housing dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/datasets/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    159\u001b[0m             \"\"\"\n\u001b[1;32m    160\u001b[0m         )\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: \n`load_boston` has been removed from scikit-learn since version 1.2.\n\nThe Boston housing prices dataset has an ethical problem: as\ninvestigated in [1], the authors of this dataset engineered a\nnon-invertible variable \"B\" assuming that racial self-segregation had a\npositive impact on house prices [2]. Furthermore the goal of the\nresearch that led to the creation of this dataset was to study the\nimpact of air quality but it did not give adequate demonstration of the\nvalidity of this assumption.\n\nThe scikit-learn maintainers therefore strongly discourage the use of\nthis dataset unless the purpose of the code is to study and educate\nabout ethical issues in data science and machine learning.\n\nIn this special case, you can fetch the dataset from the original\nsource::\n\n    import pandas as pd\n    import numpy as np\n\n    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n    target = raw_df.values[1::2, 2]\n\nAlternative datasets include the California housing dataset and the\nAmes housing dataset. You can load the datasets as follows::\n\n    from sklearn.datasets import fetch_california_housing\n    housing = fetch_california_housing()\n\nfor the California housing dataset and::\n\n    from sklearn.datasets import fetch_openml\n    housing = fetch_openml(name=\"house_prices\", as_frame=True)\n\nfor the Ames housing dataset.\n\n[1] M Carlisle.\n\"Racist data destruction?\"\n<https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>\n\n[2] Harrison Jr, David, and Daniel L. Rubinfeld.\n\"Hedonic housing prices and the demand for clean air.\"\nJournal of environmental economics and management 5.1 (1978): 81-102.\n<https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>\n",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "33. Train a Random Forest Classifier and analyze misclassified samples=\n",
        "\n",
        "\n",
        "\n",
        "ans:-\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "3RBX6KT-dWkn"
      }
    },
    {
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_breast_cancer"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "EIbczWK1dYkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "breast_cancer = load_breast_cancer()\n",
        "X, y = breast_cancer.data, breast_cancer.target\n",
        "feature_names = breast_cancer.feature_names"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "_d5mzjmTdY6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "_Ktd2J_2dZrx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)  # You can adjust n_estimators\n",
        "rf_classifier.fit(X_train, y_train)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "di1Qsen0daIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "y_pred = rf_classifier.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "kqULZSRIdafX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "misclassified_indices = [i for i in range(len(y_test)) if y_test[i] != y_pred[i]]\n",
        "misclassified_samples = pd.DataFrame(X_test[misclassified_indices], columns=feature_names)\n",
        "misclassified_samples['Actual_Class'] = y_test[misclassified_indices]\n",
        "misclassified_samples['Predicted_Class'] = y_pred[misclassified_indices]\n",
        "\n",
        "print(\"\\nMisclassified Samples:\")\n",
        "print(misclassified_samples)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "fEjrSnCadb-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X, y = breast_cancer.data, breast_cancer.target\n",
        "feature_names = breast_cancer.feature_names\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred = rf_classifier.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "# Analyze misclassified samples\n",
        "misclassified_indices = [i for i in range(len(y_test)) if y_test[i] != y_pred[i]]\n",
        "misclassified_samples = pd.DataFrame(X_test[misclassified_indices], columns=feature_names)\n",
        "misclassified_samples['Actual_Class'] = y_test[misclassified_indices]\n",
        "misclassified_samples['Predicted_Class'] = y_pred[misclassified_indices]\n",
        "\n",
        "print(\"\\nMisclassified Samples:\")\n",
        "print(misclassified_samples)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRg9ksdOdcoV",
        "outputId": "7960f144-a6f0-4076-f935-cb2d6ea7be31"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9649122807017544\n",
            "\n",
            "Misclassified Samples:\n",
            "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
            "0        13.34         15.86           86.49      520.0          0.10780   \n",
            "1        13.80         15.79           90.43      584.1          0.10070   \n",
            "2        13.96         17.05           91.43      602.4          0.10960   \n",
            "3        14.48         21.46           94.25      648.2          0.09444   \n",
            "\n",
            "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
            "0           0.15350         0.11690              0.06987         0.1942   \n",
            "1           0.12800         0.07789              0.05069         0.1662   \n",
            "2           0.12790         0.09789              0.05246         0.1908   \n",
            "3           0.09947         0.12040              0.04938         0.2075   \n",
            "\n",
            "   mean fractal dimension  ...  worst perimeter  worst area  worst smoothness  \\\n",
            "0                 0.06902  ...            96.66       614.9            0.1536   \n",
            "1                 0.06566  ...           110.30       812.4            0.1411   \n",
            "2                 0.06130  ...           108.10       826.0            0.1512   \n",
            "3                 0.05636  ...           108.40       808.9            0.1306   \n",
            "\n",
            "   worst compactness  worst concavity  worst concave points  worst symmetry  \\\n",
            "0             0.4791           0.4858                0.1708          0.3527   \n",
            "1             0.3542           0.2779                0.1383          0.2589   \n",
            "2             0.3262           0.3209                0.1374          0.3068   \n",
            "3             0.1976           0.3349                0.1225          0.3020   \n",
            "\n",
            "   worst fractal dimension  Actual_Class  Predicted_Class  \n",
            "0                  0.10160             1                0  \n",
            "1                  0.10300             0                1  \n",
            "2                  0.07957             0                1  \n",
            "3                  0.06846             0                1  \n",
            "\n",
            "[4 rows x 32 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "34. Train a Bagging Classifier and compare its performance with a single Decision Tree Classifier=\n",
        "\n",
        "\n",
        "ans:-"
      ],
      "metadata": {
        "id": "BIlq5HDPdfOi"
      }
    },
    {
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_breast_cancer"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "vMTxXdFadnsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "breast_cancer = load_breast_cancer()\n",
        "X, y = breast_cancer.data, breast_cancer.target"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "GkMuMYfVdn9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "pADSJUBBdoRl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Bagging Classifier\n",
        "bagging_classifier = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=100, random_state=42)\n",
        "bagging_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Decision Tree Classifier\n",
        "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
        "dt_classifier.fit(X_train, y_train)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "if3JvIemdoli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Bagging Classifier\n",
        "bagging_predictions = bagging_classifier.predict(X_test)\n",
        "bagging_accuracy = accuracy_score(y_test, bagging_predictions)\n",
        "\n",
        "# Decision Tree Classifier\n",
        "dt_predictions = dt_classifier.predict(X_test)\n",
        "dt_accuracy = accuracy_score(y_test, dt_predictions)\n",
        "\n",
        "print(f\"Bagging Classifier Accuracy: {bagging_accuracy}\")\n",
        "print(f\"Decision Tree Classifier Accuracy: {dt_accuracy}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "i3w9Muq9dpCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X, y = breast_cancer.data, breast_cancer.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the models\n",
        "bagging_classifier = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=100, random_state=42)\n",
        "bagging_classifier.fit(X_train, y_train)\n",
        "\n",
        "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
        "dt_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "bagging_predictions = bagging_classifier.predict(X_test)\n",
        "bagging_accuracy = accuracy_score(y_test, bagging_predictions)\n",
        "\n",
        "dt_predictions = dt_classifier.predict(X_test)\n",
        "dt_accuracy = accuracy_score(y_test, dt_predictions)\n",
        "\n",
        "print(f\"Bagging Classifier Accuracy: {bagging_accuracy}\")\n",
        "print(f\"Decision Tree Classifier Accuracy: {dt_accuracy}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "HM4vQtB-dpiA",
        "outputId": "986cc19a-86e5-495b-919c-6cb600596163"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "BaggingClassifier.__init__() got an unexpected keyword argument 'base_estimator'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-8bacc2f33415>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Create and train the models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mbagging_classifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBaggingClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDecisionTreeClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mbagging_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: BaggingClassifier.__init__() got an unexpected keyword argument 'base_estimator'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "35. Train a Random Forest Classifier and visualize the confusion matrix=\n",
        "\n",
        "\n",
        "\n",
        "ans:-"
      ],
      "metadata": {
        "id": "HI3P5L21drWL"
      }
    },
    {
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.datasets import load_breast_cancer"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "KHjPObsQdzlt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "breast_cancer = load_breast_cancer()\n",
        "X, y = breast_cancer.data, breast_cancer.target"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "dT2zfyLUdz4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "vXctVlySd0WP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)  # You can adjust n_estimators\n",
        "rf_classifier.fit(X_train, y_train)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "CBFHIKoqd05h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "y_pred = rf_classifier.predict(X_test)\n",
        "cm = confusion_matrix(y_test, y_pred)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "nSPCFAYod1ST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
        "plt.xlabel(\"Predicted Labels\")\n",
        "plt.ylabel(\"True Labels\")\n",
        "plt.title(\"Confusion Matrix for Random Forest Classifier\")\n",
        "plt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "yiZ8iASfd1tD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X, y = breast_cancer.data, breast_cancer.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and create the confusion matrix\n",
        "y_pred = rf_classifier.predict(X_test)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualize the confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
        "plt.xlabel(\"Predicted Labels\")\n",
        "plt.ylabel(\"True Labels\")\n",
        "plt.title(\"Confusion Matrix for Random Forest Classifier\")\n",
        "plt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "h0EnhhMsd2Io",
        "outputId": "ed39fe5f-9933-4877-d09a-211325fcd7e1"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAqsAAAIjCAYAAAAk+FJEAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAPYlJREFUeJzt3XlcVdX+//H3AeWAIKCCImk4m5RpDpmimWVRlllqDl0VHNPUTNTKX4NDpeWQms2W6TXr22DpTb2JU1k55WxlDjnlPKLiAArr94cPzuUI6AGxs5LX8/E4j4dn7bX3/uzN5vhmnT04jDFGAAAAgIV8vF0AAAAAkBPCKgAAAKxFWAUAAIC1CKsAAACwFmEVAAAA1iKsAgAAwFqEVQAAAFiLsAoAAABrEVYBAABgLcIqkEdbt27Vfffdp5CQEDkcDs2cOTNfl79z5045HA5NmTIlX5f7T3bXXXfprrvuyrflJScnq1u3boqIiJDD4dDTTz+db8v+p+A4s5sNP59y5copPj7erS27z78pU6bI4XBo586dXqkT1y/CKv7R/vzzTz3xxBOqUKGC/P39FRwcrJiYGE2YMEFnz569puuOi4vTxo0b9eqrr2ratGmqU6fONV3f3yk+Pl4Oh0PBwcHZ7setW7fK4XDI4XBozJgxuV7+vn37NHToUK1bty4fqs27ESNGaMqUKerVq5emTZumjh07XtP1lStXzrXfHA6HAgMDdfvtt+vf//73NV3vP82l+ynz69y5c94uL4ulS5dq6NChSkpKytV833//vVq2bKmIiAj5+fmpZMmSat68ub7++utrU2g+up4//2CfQt4uAMirOXPm6LHHHpPT6VSnTp10yy23KDU1VT/99JMGDRqk3377TR988ME1WffZs2e1bNkyPf/88+rTp881WUdUVJTOnj2rwoULX5PlX0mhQoV05swZffvtt2rTpo3btOnTp8vf3z/PwWHfvn0aNmyYypUrp5o1a3o8X2JiYp7Wl5NFixbpjjvu0JAhQ/J1uZdTs2ZNDRgwQJK0f/9+ffjhh4qLi1NKSoq6d+/+t9Vhu8z7KTM/Pz8vVHN5S5cu1bBhwxQfH6/Q0FCP5hkyZIiGDx+uypUr64knnlBUVJSOHj2quXPnqlWrVpo+fboef/zxa1u4hzZv3iwfn/+NbeX0+dexY0e1a9dOTqfTG2XiOkZYxT/Sjh071K5dO0VFRWnRokUqXbq0a1rv3r21bds2zZkz55qt//Dhw5Lk8X9MeeFwOOTv73/Nln8lTqdTMTEx+uyzz7KE1U8//VQPPvigZsyY8bfUcubMGRUpUiTfg8qhQ4cUHR2db8u7cOGC0tPTL1vnDTfcoA4dOrjex8fHq0KFCho3bhxhNZNL91N+SU9PV2pqqld/t7766isNHz5crVu31qeffur2B+mgQYM0b948nT9/3mv1XerS8JnT55+vr698fX3zbb2nT59WYGBgvi0P/2AG+Afq2bOnkWR+/vlnj/qfP3/eDB8+3FSoUMH4+fmZqKgoM3jwYHPu3Dm3flFRUebBBx80P/74o6lbt65xOp2mfPnyZurUqa4+Q4YMMZLcXlFRUcYYY+Li4lz/zixjnswSExNNTEyMCQkJMYGBgaZKlSpm8ODBruk7duwwkszHH3/sNt/ChQtNw4YNTZEiRUxISIh5+OGHze+//57t+rZu3Wri4uJMSEiICQ4ONvHx8eb06dNX3F9xcXEmMDDQTJkyxTidTnP8+HHXtJUrVxpJZsaMGUaSGT16tGva0aNHzYABA8wtt9xiAgMDTdGiRc39999v1q1b5+qzePHiLPsv83Y2btzY3HzzzWbVqlWmUaNGJiAgwPTr1881rXHjxq5lderUyTidzizbf99995nQ0FCzd+/ebLcvpxp27NhhjDHm4MGDpkuXLqZkyZLG6XSaW2+91UyZMsVtGRk/n9GjR5tx48aZChUqGB8fH7N27doc92vG8XWpOnXqGD8/P7e2JUuWmNatW5uyZcsaPz8/U6ZMGfP000+bM2fOuPXL+Fnt2bPHtGjRwgQGBpqwsDAzYMAAc+HCBbe+x48fN3FxcSY4ONiEhISYTp06mbVr1171cbZ582bzr3/9ywQHB5uwsDDzwgsvmPT0dLN7927z8MMPm6JFi5pSpUqZMWPG5LhvPNlPmSUnJ5uEhARTpkwZ4+fnZ6pUqWJGjx5t0tPT3fpJMr179zaffPKJiY6ONoUKFTLffPONMcaYPXv2mM6dO5uSJUsaPz8/Ex0dbT766KMs63rzzTdNdHS0CQgIMKGhoaZ27dpm+vTpbvsgp2MpOzfddJMpXry4OXny5BX3RXafA+vXrzdxcXGmfPnyxul0mlKlSpnOnTubI0eOuM178uRJ069fPxMVFWX8/PxMeHi4adq0qVm9erWrz5YtW0zLli1NqVKljNPpNDfccINp27atSUpKcvWJiooycXFxOW5vxmfexx9/nO22z50713UsBQUFmWbNmplff/3VrU/Gcbxt2zbzwAMPmKCgINOiRYsr7h8UDIys4h/p22+/VYUKFdSgQQOP+nfr1k1Tp05V69atNWDAAK1YsUIjR47Upk2b9M0337j13bZtm1q3bq2uXbsqLi5OkydPVnx8vGrXrq2bb75ZLVu2VGhoqPr376/27durWbNmCgoKylX9v/32mx566CHdeuutGj58uJxOp7Zt26aff/75svMtWLBADzzwgCpUqKChQ4fq7NmzmjhxomJiYrRmzRqVK1fOrX+bNm1Uvnx5jRw5UmvWrNGHH36okiVL6vXXX/eozpYtW6pnz576+uuv1aVLF0kXR1Vvuukm1apVK0v/7du3a+bMmXrsscdUvnx5HTx4UO+//74aN26s33//XZGRkapWrZqGDx+ul156ST169FCjRo0kye1nefToUT3wwANq166dOnTooFKlSmVb34QJE7Ro0SLFxcVp2bJl8vX11fvvv6/ExERNmzZNkZGR2c5XrVo1TZs2Tf3791eZMmVcXzeHh4fr7Nmzuuuuu7Rt2zb16dNH5cuX15dffqn4+HglJSWpX79+bsv6+OOPde7cOfXo0UNOp1PFixf3aN9muHDhgvbs2aNixYq5tX/55Zc6c+aMevXqpRIlSmjlypWaOHGi9uzZoy+//NKtb1pammJjY1WvXj2NGTNGCxYs0NixY1WxYkX16tVLkmSMUYsWLfTTTz+pZ8+eqlatmr755hvFxcVlqSm3x1nbtm1VrVo1vfbaa5ozZ45eeeUVFS9eXO+//77uvvtuvf7665o+fboGDhyounXr6s4777zifjl//ryOHDni1lakSBEVKVJExhg9/PDDWrx4sbp27aqaNWtq3rx5GjRokPbu3atx48a5zbdo0SJ98cUX6tOnj8LCwlSuXDkdPHhQd9xxhxwOh/r06aPw8HD997//VdeuXXXy5EnXxXaTJk3SU089pdatW6tfv346d+6cNmzYoBUrVujxxx9Xy5YttWXLFn322WcaN26cwsLCJF08lrKzdetW/fHHH+rSpYuKFi16xf2Qnfnz52v79u3q3LmzIiIiXKc8/fbbb1q+fLkcDockqWfPnvrqq6/Up08fRUdH6+jRo/rpp5+0adMm1apVS6mpqYqNjVVKSor69u2riIgI7d27V7Nnz1ZSUpJCQkKyrDu3n3/Tpk1TXFycYmNj9frrr+vMmTN699131bBhQ61du9btWLpw4YJiY2PVsGFDjRkzRkWKFMnT/sF1yNtpGcitEydOGEke/9W9bt06I8l069bNrX3gwIFGklm0aJGrLSoqykgyS5YscbUdOnTIOJ1OM2DAAFdb5lG1zDwdWR03bpyRZA4fPpxj3dmNqNSsWdOULFnSHD161NW2fv164+PjYzp16pRlfV26dHFb5qOPPmpKlCiR4zozb0dgYKAxxpjWrVube+65xxhjTFpamomIiDDDhg3Ldh+cO3fOpKWlZdkOp9Nphg8f7mr75Zdfsh3NM+bi6Kkk895772U7LfPIqjHGzJs3z0gyr7zyitm+fbsJCgoyjzzyyBW30ZjsR/DGjx9vJJlPPvnE1Zaammrq169vgoKCXKNhGdsfHBxsDh065PH67rvvPnP48GFz+PBhs3HjRtOxY0fX6F9ml46gGmPMyJEjjcPhMLt27XK1xcXFGUlu+9cYY2677TZTu3Zt1/uZM2caSWbUqFGutgsXLphGjRpd9XHWo0cPt2WWKVPGOBwO89prr7najx8/bgICAlwjdFfaT8pmtHLIkCFu2/LKK6+4zde6dWvjcDjMtm3bXG2SjI+Pj/ntt9/c+nbt2tWULl06y2hku3btTEhIiGv/t2jRwtx8882XrXf06NFXHE3NMGvWLCPJjBs37op9jcn+cyC7Y+Ozzz7L8tkVEhKS5bjKLGNU/csvv7xsDZlHVjPXdOnn36Ujq6dOnTKhoaGme/fubv0OHDhgQkJC3NozjuPnnnvusrWgYOJuAPjHOXnypCR5PCoxd+5cSVJCQoJbe8Zo2qXntkZHR7tG+6SLIyRVq1bV9u3b81zzpTLO9Zo1a5bS09M9mmf//v1at26d4uPj3Ubvbr31Vt17772u7cysZ8+ebu8bNWqko0ePuvahJx5//HF9//33OnDggBYtWqQDBw7keOGH0+l0XYiRlpamo0ePKigoSFWrVtWaNWs8XqfT6VTnzp096nvffffpiSee0PDhw9WyZUv5+/vr/fff93hdl5o7d64iIiLUvn17V1vhwoX11FNPKTk5WT/88INb/1atWuU4ipadxMREhYeHKzw8XNWrV9e0adPUuXNnjR492q1fQECA69+nT5/WkSNH1KBBAxljtHbt2izLze5nnfmYnTt3rgoVKuQaaZUunmPYt29ft/nycpx169bNbZl16tSRMUZdu3Z1tYeGhubq96hevXqaP3++26tTp06ubfH19dVTTz3lNs+AAQNkjNF///tft/bGjRu7nZtsjNGMGTPUvHlzGWN05MgR1ys2NlYnTpxwHa+hoaHas2ePfvnlF4/qvpLcfn5lJ/Oxce7cOR05ckR33HGHJLn9noWGhmrFihXat29ftsvJGDmdN2+ezpw5k+d6cjJ//nwlJSWpffv2bvvY19dX9erV0+LFi7PMk/n4BDIQVvGPExwcLEk6deqUR/137dolHx8fVapUya09IiJCoaGh2rVrl1v7jTfemGUZxYoV0/Hjx/NYcVZt27ZVTEyMunXrplKlSqldu3b64osvLhtcM+qsWrVqlmnVqlXTkSNHdPr0abf2S7cl46vm3GxLs2bNVLRoUX3++eeaPn266tatm2VfZkhPT9e4ceNUuXJlOZ1OhYWFKTw8XBs2bNCJEyc8XucNN9yQq4upxowZo+LFi2vdunV68803VbJkSY/nvdSuXbtUuXJlt6ufpYv7OGN6ZuXLl8/V8jNC2HfffacxY8YoNDRUx48fz7K9u3fvdgXGoKAghYeHq3HjxpKUZV/6+/tnCcyXHrO7du1S6dKls3xle+nxlB/HWUhIiPz9/V1fiWdu9/TYCwsLU9OmTd1eFSpUcNUYGRmZJfB5+jM6fPiwkpKS9MEHH7j+cMh4ZfyRdOjQIUnSs88+q6CgIN1+++2qXLmyevfufcXTdS4nt59f2Tl27Jj69eunUqVKKSAgQOHh4a5tzHxsjBo1Sr/++qvKli2r22+/XUOHDnX7Y6F8+fJKSEjQhx9+qLCwMMXGxurtt9/O1e/q5WzdulWSdPfdd2fZz4mJia59nKFQoUIqU6ZMvqwb1xfOWcU/TnBwsCIjI/Xrr7/mar6M87iuJKerWY0xeV5HWlqa2/uAgAAtWbJEixcv1pw5c/Tdd9/p888/1913363ExMR8u6L2arYlg9PpVMuWLTV16lRt375dQ4cOzbHviBEj9OKLL6pLly56+eWXVbx4cfn4+Ojpp5/2eARZch858sTatWtd//Ft3LjRbVT0WsttrRkhTJJiY2N100036aGHHtKECRNco/9paWm69957dezYMT377LO66aabFBgYqL179yo+Pj7LvszPK7DzIrv158exl18u/Rll7L8OHTpke86udHEkWboYgDdv3qzZs2fru+++04wZM/TOO+/opZde0rBhw3Jdy0033STp4nGaV23atNHSpUs1aNAg1axZU0FBQUpPT9f999/vdmy0adNGjRo10jfffKPExESNHj1ar7/+ur7++ms98MADkqSxY8cqPj5es2bNUmJiop566imNHDlSy5cvv+rgmFHLtGnTFBERkWV6oULuESTzNzNAZoRV/CM99NBD+uCDD7Rs2TLVr1//sn2joqKUnp6urVu3ukZeJOngwYNKSkpSVFRUvtVVrFixbG8MfulIjyT5+Pjonnvu0T333KM33nhDI0aM0PPPP6/Fixe7wsyl2yFdvOfhpf744w+FhYVds9u8PP7445o8ebJ8fHzUrl27HPt99dVXatKkiT766CO39qSkJLdRNk//cPDE6dOn1blzZ0VHR6tBgwYaNWqUHn30UdWtWzdPy4uKitKGDRuUnp7u9h/nH3/84Zqenx588EE1btxYI0aM0BNPPKHAwEBt3LhRW7Zs0dSpU11ffUsXv1bNq6ioKC1cuFDJycluo6uXHk/ePM48FRUVpQULFujUqVNuo6ue/ozCw8NVtGhRpaWlZfu7dqnAwEC1bdtWbdu2VWpqqlq2bKlXX31VgwcPlr+/f66O5ypVqqhq1aqaNWuWJkyYkOuLM48fP66FCxdq2LBheumll1ztGaOYlypdurSefPJJPfnkkzp06JBq1aqlV1991RVWJal69eqqXr26XnjhBS1dulQxMTF677339Morr+SqtktVrFhRklSyZEmP9jOQE/6EwT/SM888o8DAQHXr1k0HDx7MMv3PP//UhAkTJF38GluSxo8f79bnjTfekHQxLOSXihUr6sSJE9qwYYOrbf/+/VnuOHDs2LEs82bcHD8lJSXbZZcuXVo1a9bU1KlT3QLxr7/+qsTERNd2XgtNmjTRyy+/rLfeeivbEZIMvr6+WUbOvvzyS+3du9etLSPs5PaJP9l59tlntXv3bk2dOlVvvPGGypUr57rJfl40a9ZMBw4c0Oeff+5qu3DhgiZOnKigoCDXV/H56dlnn9XRo0c1adIkSf8blcy8L40xrmM6L5o1a6YLFy7o3XffdbWlpaVp4sSJbv28eZx5qlmzZkpLS9Nbb73l1j5u3Dg5HA63IJYdX19ftWrVSjNmzMj2G5qM+4hKF+9MkZmfn5+io6NljHHdCzW3x/OwYcN09OhRdevWTRcuXMgyPTExUbNnz86xdinrCPWln29paWlZvs4vWbKkIiMjXb8bJ0+ezLL+6tWry8fHJ8+/P5nFxsYqODhYI0aMyPa+sZn3M3A5jKziH6lixYr69NNPXbfMyfwEq6VLl7puNSRJNWrUUFxcnD744AMlJSWpcePGWrlypaZOnapHHnlETZo0ybe62rVrp2effVaPPvqonnrqKddtWqpUqeJ24cPw4cO1ZMkSPfjgg4qKitKhQ4f0zjvvqEyZMmrYsGGOyx89erQeeOAB1a9fX127dnXdUigkJOSyX89fLR8fH73wwgtX7PfQQw9p+PDh6ty5sxo0aKCNGzdq+vTprnMNM1SsWFGhoaF67733VLRoUQUGBqpevXq5Pv9z0aJFeueddzRkyBDXrbQ+/vhj3XXXXXrxxRc1atSoXC1Pknr06KH3339f8fHxWr16tcqVK6evvvpKP//8s8aPH39VF8bk5IEHHtAtt9yiN954Q71799ZNN92kihUrauDAgdq7d6+Cg4M1Y8aMqzpvunnz5oqJidFzzz2nnTt3Kjo6Wl9//XW25yd66zjzVPPmzdWkSRM9//zz2rlzp2rUqKHExETNmjVLTz/9tGtE73Jee+01LV68WPXq1VP37t0VHR2tY8eOac2aNVqwYIHrD8r77rtPERERiomJUalSpbRp0ya99dZbevDBB13HQu3atSVJzz//vNq1a6fChQurefPmOY5At23b1vWo0rVr16p9+/auJ1h99913WrhwoT799NNs5w0ODtadd96pUaNG6fz587rhhhuUmJioHTt2uPU7deqUypQpo9atW6tGjRoKCgrSggUL9Msvv2js2LGSLv7+9OnTR4899piqVKmiCxcuaNq0aa4wf7WCg4P17rvvqmPHjqpVq5batWun8PBw7d69W3PmzFFMTEyWPziAbHnlHgRAPtmyZYvp3r27KVeunPHz8zNFixY1MTExZuLEiW43/D9//rwZNmyYKV++vClcuLApW7bsZR8KcKlLb5mU061bjLl4s/9bbrnF+Pn5mapVq5pPPvkky62rFi5caFq0aGEiIyONn5+fiYyMNO3btzdbtmzJso5Lb++0YMECExMTYwICAkxwcLBp3rx5jjdrv/TWWDndtPtSmW9dlZOcbl01YMAAU7p0aRMQEGBiYmLMsmXLsr3l1KxZs1w3ac+8nRkPBchO5uWcPHnSREVFmVq1apnz58+79evfv7/x8fExy5Ytu+w25PTzPnjwoOncubMJCwszfn5+pnr16ll+Dpc7BnK7PmOMmTJlitt++P33303Tpk1NUFCQCQsLM927dzfr16/Pckzk9LPK7kEUR48eNR07dnQ9FKBjx445PhTgao6znGq63M82M08eCnDq1CnTv39/ExkZaQoXLmwqV6582YcCZOfgwYOmd+/epmzZsqZw4cImIiLC3HPPPeaDDz5w9Xn//ffNnXfeaUqUKGGcTqepWLGiGTRokDlx4oTbsl5++WVzww03GB8fH49vY5XxOVCyZElTqFAhEx4ebpo3b25mzZrl6pPd58CePXvMo48+akJDQ01ISIh57LHHzL59+9xu75WSkmIGDRpkatSoYYoWLWoCAwNNjRo1zDvvvONazvbt202XLl1MxYoVjb+/vylevLhp0qSJWbBggVudeb11VYbFixeb2NhYExISYvz9/U3FihVNfHy8WbVqlauPJ585KLgcxnjhbHcAAADAA5yzCgAAAGsRVgEAAGAtwioAAACsRVgFAACAtQirAAAAsBZhFQAAANYirAIAAMBa1+UTrNpOXevtEgAgX33Ytoa3SwCAfFXU37MxU0ZWAQAAYC3CKgAAAKxFWAUAAIC1CKsAAACwFmEVAAAA1iKsAgAAwFqEVQAAAFiLsAoAAABrEVYBAABgLcIqAAAArEVYBQAAgLUIqwAAALAWYRUAAADWIqwCAADAWoRVAAAAWIuwCgAAAGsRVgEAAGAtwioAAACsRVgFAACAtQirAAAAsBZhFQAAANYirAIAAMBahFUAAABYi7AKAAAAaxFWAQAAYC3CKgAAAKxFWAUAAIC1CKsAAACwFmEVAAAA1iKsAgAAwFqEVQAAAFiLsAoAAABrEVYBAABgLcIqAAAArEVYBQAAgLUIqwAAALAWYRUAAADWIqwCAADAWoRVAAAAWIuwCgAAAGsRVgEAAGAtwioAAACsRVgFAACAtQirAAAAsBZhFQAAANYirAIAAMBahFUAAABYi7AKAAAAaxFWAQAAYC3CKgAAAKxFWAUAAIC1CKsAAACwFmEVAAAA1iKsAgAAwFqEVQAAAFiLsAoAAABrEVYBAABgLcIqAAAArEVYBQAAgLUIqwAAALAWYRUAAADWIqwCAADAWoRVAAAAWIuwCgAAAGsRVgEAAGAtwioAAACsRVgFAACAtQirAAAAsBZhFQAAANYirAIAAMBahFUAAABYi7AKAAAAaxFWAQAAYC3CKgAAAKxFWAUAAIC1CKsAAACwFmEVAAAA1iKsAgAAwFqEVQAAAFiLsAoAAABrEVYBAABgLcIqAAAArEVYBQAAgLUIqwAAALAWYRUAAADWIqwCAADAWoRVAAAAWIuwCgAAAGsRVgEAAGAtwioAAACsRVgFAACAtQirAAAAsBZhFQAAANYirAIAAMBahFUAAABYi7AKAAAAaxFWAQAAYC3CKgAAAKxFWAUAAIC1CKsAAACwFmEVAAAA1iKsAgAAwFqEVQAAAFiLsAoAAABrEVYBAABgLcIqAAAArEVYBQAAgLUIqwAAALAWYRUAAADWKuTtAoB/kha3lNLjtSM19/dDmvrLXklSYR+HOta9QQ3KFVNhX4fW7zulj5b/pRPnLni5WgDwzFdffKavvvg/7d938XOtQsVK6vbEk4ppeKeXKwMYWQU8VrFEETWtUkK7jp11a+90+w2qXSZE437YoaHfbVWxgMIa0KS8l6oEgNwrWTJCffolaNpnX+nfn36pOrffoQH9+ujPbVu9XRpAWAU84Szkoz6NovTBsr+UnPq/EdOAwj66u1IJ/XvVXv12IFk7jp3Vuz/vUtWSQaocVsSLFQOA5+68q4kaNmqsG6PKKapcefXu+7SKFCmijRvWe7s0wLunARw5ckSTJ0/WsmXLdODAAUlSRESEGjRooPj4eIWHh3uzPMCla70yWrv3pDbuP6VHby3laq9QoogK+fpo475TrrZ9J1N0ODlVlUsGauuRM94oFwDyLC0tTQsSv9PZs2d0a42a3i4H8F5Y/eWXXxQbG6siRYqoadOmqlKliiTp4MGDevPNN/Xaa69p3rx5qlOnzmWXk5KSopSUFLe2tPOp8i3sd81qR8HSoFyoypcoov83e3OWaaEBhXU+LV1nzqe5tZ84d16h/oX/rhIB4Kpt27pFnTu2V2pqigKKFNHocRNVoWIlb5cFeC+s9u3bV4899pjee+89ORwOt2nGGPXs2VN9+/bVsmXLLruckSNHatiwYW5t0S166JZHe+Z7zSh4ShQprLjby+jV+dt0Pt14uxwAuGaiypXTp198reTkZC2cP09DXxysDz76N4EVXucwxnjlf+CAgACtXbtWN910U7bT//jjD9122206e/ZsttMzZDey2uWLTYysIl/UKRuiQXdXUFqmoOrr41C6MTJGGjF/m16MrazOn25wG119q9XNmrvpkOb+ftgbZeM69GHbGt4uAQXMkz0664YyN+r5l4ZduTOQB0X9Pbt0ymsjqxEREVq5cmWOYXXlypUqVapUttMyczqdcjqdbm0EVeSXX/ef0sBZm9zaesXcqL0nUvSfXw/qyOlUXUhL1y2lg7Ry9wlJUulgp8KD/LT10GlvlAwA+SI93ej8+VRvlwF4L6wOHDhQPXr00OrVq3XPPfe4gunBgwe1cOFCTZo0SWPGjPFWeYAk6dyFdP2VdC5LW3LKBVf7om1H1aluGZ1OTdOZ1DR1rldGmw8lc3EVgH+Mtya8oQYNGykiIlJnzpzWd3Nna/WqlZr47iRvlwZ4L6z27t1bYWFhGjdunN555x2lpV38CtXX11e1a9fWlClT1KZNG2+VB3js3yv3ytSVEu4qr0I+Dm3Yd0ofLv/L22UBgMeOHTuqIS88pyOHDysoqKgqV6miie9O0h31Y7xdGuC9c1YzO3/+vI4cOSJJCgsLU+HCV3cVddupa/OjLACwBuesArjeWH/OamaFCxdW6dKlvV0GAAAALMMTrAAAAGAtwioAAACsRVgFAACAtQirAAAAsBZhFQAAANYirAIAAMBahFUAAABYi7AKAAAAaxFWAQAAYC3CKgAAAKxFWAUAAIC1CKsAAACwFmEVAAAA1iKsAgAAwFqEVQAAAFiLsAoAAABrEVYBAABgLcIqAAAArEVYBQAAgLUIqwAAALAWYRUAAADWIqwCAADAWoRVAAAAWIuwCgAAAGsRVgEAAGAtwioAAACsRVgFAACAtQirAAAAsBZhFQAAANYirAIAAMBahFUAAABYi7AKAAAAaxFWAQAAYC3CKgAAAKxFWAUAAIC1CKsAAACwFmEVAAAA1iKsAgAAwFqEVQAAAFiLsAoAAABrEVYBAABgLcIqAAAArEVYBQAAgLUIqwAAALAWYRUAAADWIqwCAADAWoRVAAAAWIuwCgAAAGsRVgEAAGAtwioAAACsRVgFAACAtQirAAAAsBZhFQAAANYirAIAAMBahFUAAABYi7AKAAAAaxFWAQAAYC3CKgAAAKxFWAUAAIC1CKsAAACwFmEVAAAA1iKsAgAAwFqEVQAAAFiLsAoAAABrEVYBAABgLcIqAAAArEVYBQAAgLUIqwAAALAWYRUAAADWIqwCAADAWoRVAAAAWIuwCgAAAGvlOqxOnTpVc+bMcb1/5plnFBoaqgYNGmjXrl35WhwAAAAKtlyH1REjRiggIECStGzZMr399tsaNWqUwsLC1L9//3wvEAAAAAVXodzO8Ndff6lSpUqSpJkzZ6pVq1bq0aOHYmJidNddd+V3fQAAACjAcj2yGhQUpKNHj0qSEhMTde+990qS/P39dfbs2fytDgAAAAVarkdW7733XnXr1k233XabtmzZombNmkmSfvvtN5UrVy6/6wMAAEABluuR1bffflv169fX4cOHNWPGDJUoUUKStHr1arVv3z7fCwQAAEDB5TDGGG8Xkd/aTl3r7RIAIF992LaGt0sAgHxV1N+zMVOPTgPYsGGDxyu+9dZbPe4LAAAAXI5HYbVmzZpyOBzKaRA2Y5rD4VBaWlq+FggAAICCy6OwumPHjmtdBwAAAJCFR2E1KirqWtcBAAAAZJHruwFI0rRp0xQTE6PIyEjXI1bHjx+vWbNm5WtxAAAAKNhyHVbfffddJSQkqFmzZkpKSnKdoxoaGqrx48fnd30AAAAowHIdVidOnKhJkybp+eefl6+vr6u9Tp062rhxY74WBwAAgIIt12F1x44duu2227K0O51OnT59Ol+KAgAAAKQ8hNXy5ctr3bp1Wdq/++47VatWLT9qAgAAACR5eDeAzBISEtS7d2+dO3dOxhitXLlSn332mUaOHKkPP/zwWtQIAACAAirXYbVbt24KCAjQCy+8oDNnzujxxx9XZGSkJkyYoHbt2l2LGgEAAFBAOUxOj6XywJkzZ5ScnKySJUvmZ01Xre3Utd4uAQDy1Ydta3i7BADIV0X9PTsbNdcjqxkOHTqkzZs3S7r4uNXw8PC8LgoAAADIVq4vsDp16pQ6duyoyMhINW7cWI0bN1ZkZKQ6dOigEydOXIsaAQAAUEDlOqx269ZNK1as0Jw5c5SUlKSkpCTNnj1bq1at0hNPPHEtagQAAEABletzVgMDAzVv3jw1bNjQrf3HH3/U/fffb8W9VjlnFcD1hnNWAVxvPD1nNdcjqyVKlFBISEiW9pCQEBUrViy3iwMAAABylOuw+sILLyghIUEHDhxwtR04cECDBg3Siy++mK/FAQAAoGDz6G4At912mxwOh+v91q1bdeONN+rGG2+UJO3evVtOp1OHDx/mvFUAAADkG4/C6iOPPHKNywAAAACy8iisDhky5FrXAQAAAGSR63NWAQAAgL9Lrp9glZaWpnHjxumLL77Q7t27lZqa6jb92LFj+VYcAAAACrZcj6wOGzZMb7zxhtq2basTJ04oISFBLVu2lI+Pj4YOHXoNSgQAAEBBleuwOn36dE2aNEkDBgxQoUKF1L59e3344Yd66aWXtHz58mtRIwAAAAqoXIfVAwcOqHr16pKkoKAgnThxQpL00EMPac6cOflbHQAAAAq0XIfVMmXKaP/+/ZKkihUrKjExUZL0yy+/yOl05m91AAAAKNByHVYfffRRLVy4UJLUt29fvfjii6pcubI6deqkLl265HuBAAAAKLgcxhhzNQtYvny5li5dqsqVK6t58+b5VddVaTt1rbdLAIB89WHbGt4uAQDyVVF/z8ZMr/o+q3fccYcSEhJUr149jRgx4moXBwAAALhc9chqhvXr16tWrVpKS0vLj8VdlXMXvF0BAOSvYnX7eLsEAMhXZ9e+5VE/nmAFAAAAaxFWAQAAYC3CKgAAAKxVyNOOCQkJl51++PDhqy4GAAAAyMzjsLp27ZVvB3XnnXdeVTEAAABAZh6H1cWLF1/LOgAAAIAsOGcVAAAA1iKsAgAAwFqEVQAAAFiLsAoAAABrEVYBAABgrTyF1R9//FEdOnRQ/fr1tXfvXknStGnT9NNPP+VrcQAAACjYch1WZ8yYodjYWAUEBGjt2rVKSUmRJJ04cUIjRozI9wIBAABQcOU6rL7yyit67733NGnSJBUuXNjVHhMTozVr1uRrcQAAACjYch1WN2/enO2TqkJCQpSUlJQfNQEAAACS8hBWIyIitG3btiztP/30kypUqJAvRQEAAABSHsJq9+7d1a9fP61YsUIOh0P79u3T9OnTNXDgQPXq1eta1AgAAIACqlBuZ3juueeUnp6ue+65R2fOnNGdd94pp9OpgQMHqm/fvteiRgAAABRQDmOMycuMqamp2rZtm5KTkxUdHa2goKD8ri3Pzl3wdgUAkL+K1e3j7RIAIF+dXfuWR/1yPbKawc/PT9HR0XmdHQAAALiiXIfVJk2ayOFw5Dh90aJFV1UQAAAAkCHXYbVmzZpu78+fP69169bp119/VVxcXH7VBQAAAOQ+rI4bNy7b9qFDhyo5OfmqCwIAAAAy5PrWVTnp0KGDJk+enF+LAwAAAPIvrC5btkz+/v75tTgAAAAg96cBtGzZ0u29MUb79+/XqlWr9OKLL+ZbYQAAAECuw2pISIjbex8fH1WtWlXDhw/Xfffdl2+FAQAAALkKq2lpaercubOqV6+uYsWKXauaAAAAAEm5PGfV19dX9913n5KSkq5ROQAAAMD/5PoCq1tuuUXbt2+/FrUAAAAAbnIdVl955RUNHDhQs2fP1v79+3Xy5Em3FwAAAJBfHMYY40nH4cOHa8CAASpatOj/Zs702FVjjBwOh9LS0vK/ylw6d8HbFQBA/ipWt4+3SwCAfHV27Vse9fM4rPr6+mr//v3atGnTZfs1btzYoxVfS4RVANcbwiqA642nYdXjuwFkZFobwigAAAAKhlyds5r5a38AAADgWsvVfVarVKlyxcB67NixqyoIAAAAyJCrsDps2LAsT7ACAAAArpVchdV27dqpZMmS16oWAAAAwI3H56xyvioAAAD+bh6HVQ/vcAUAAADkG49PA0hPT7+WdQAAAABZ5PpxqwAAAMDfhbAKAAAAaxFWAQAAYC3CKgAAAKxFWAUAAIC1CKsAAACwFmEVAAAA1iKsAgAAwFqEVQAAAFiLsAoAAABrEVYBAABgLcIqAAAArEVYBQAAgLUIqwAAALAWYRUAAADWIqwCAADAWoRVAAAAWIuwCgAAAGsRVgEAAGAtwioAAACsRVgFAACAtQirAAAAsBZhFQAAANYirAIAAMBahFUAAABYi7AKAAAAaxFWAQAAYC3CKgAAAKxFWAUAAIC1CKsAAACwFmEVAAAA1iKsAgAAwFqEVQAAAFiLsAoAAABrEVYBAABgLcIqAAAArEVYBQAAgLUIqwAAALAWYRUAAADWIqwCAADAWoRVAAAAWIuwCgAAAGsRVgEAAGAtwioAAACsRVgFAACAtQirAAAAsBZhFQAAANYirAIAAMBahFUAAABYi7AKAAAAaxFWAQAAYC3CKgAAAKxFWAUAAIC1CKsAAACwFmEVAAAA1iKsAgAAwFqEVQAAAFiLsAoAAABrEVYBAABgLcIqAAAArEVYBQAAgLUIqwAAALAWYRUAAADWIqwCAADAWoRVAAAAWIuwCgAAAGsRVgEAAGAtwioAAACsVcjbBQD/NKtX/aIpkz/Spt9/1eHDhzXuzbd19z1NvV0WAHjsjznDFBVZIkv7e58vUf/XvpDTr5BeS2ipx2Jry+lXSAuWbVK/EZ/r0LFTXqgWBR1hFcils2fPqGrVqnqkZSsl9Ovj7XIAINcadhgtXx+H6310pUjNfa+vvp6/VpI0amArPdDwZv3rmY90Mvmsxj3XRv83tpvu7jzOWyWjACOsArnUsFFjNWzU2NtlAECeHTme7PZ+YOdb9Ofuw/px9VYFB/kr/pH6iv9/U/TDL1skST2GfKL137yo26uX08qNO71QMQoyzlkFAKAAK1zIV+2a1dXUWcskSbdVu1F+hQtp0fLNrj5bdh7U7v3HVO/W8t4qEwWY1WH1r7/+UpcuXS7bJyUlRSdPnnR7paSk/E0VAgDwz/Zwk1sVWjRAn3y7QpIUUSJYKanndSL5rFu/Q0dPqlSJYG+UiALO6rB67NgxTZ069bJ9Ro4cqZCQELfX6NdH/k0VAgDwzxb3SAPN+/l37T98wtulANny6jmr//nPfy47ffv27VdcxuDBg5WQkODWZnydV1UXAAAFwY2li+nuelXVbuAkV9uBoyfl9CuskKAAt9HVkiWCdfDoSW+UiQLOq2H1kUcekcPhkDEmxz4OhyPHaZLkdDrldLqH03MX8qU8AACuax0frq9Dx07pvz/+5mpbu2m3Us9fUJN6VTVz4TpJUuWokrqxdHGt2LDDS5WiIPPqaQClS5fW119/rfT09Gxfa9as8WZ5QLbOnD6tPzZt0h+bNkmS9u7Zoz82bdL+ffu8XBkAeM7hcKhTizs0ffYKpaWlu9pPJp/TlJnL9PqAlrqzTmXdVq2sPhjWQcvXb+dOAPAKr46s1q5dW6tXr1aLFi2ynX6lUVfAG3777Vd169zJ9X7MqIvnSD/c4lG9POI1b5UFALlyd72qurF0cU2duTzLtGfGzFB6utFnY7pdfCjA0k3qN/JzL1QJSA7jxTT4448/6vTp07r//vuznX769GmtWrVKjRvn7p6WnAYA4HpTrC4PoABwfTm79i2P+nk1rF4rhFUA1xvCKoDrjadh1epbVwEAAKBgI6wCAADAWoRVAAAAWIuwCgAAAGsRVgEAAGAtwioAAACsRVgFAACAtQirAAAAsBZhFQAAANYirAIAAMBahFUAAABYi7AKAAAAaxFWAQAAYC3CKgAAAKxFWAUAAIC1CKsAAACwFmEVAAAA1iKsAgAAwFqEVQAAAFiLsAoAAABrEVYBAABgLcIqAAAArEVYBQAAgLUIqwAAALAWYRUAAADWIqwCAADAWoRVAAAAWIuwCgAAAGsRVgEAAGAtwioAAACsRVgFAACAtQirAAAAsBZhFQAAANYirAIAAMBahFUAAABYi7AKAAAAaxFWAQAAYC3CKgAAAKxFWAUAAIC1CKsAAACwFmEVAAAA1iKsAgAAwFqEVQAAAFiLsAoAAABrEVYBAABgLcIqAAAArEVYBQAAgLUIqwAAALAWYRUAAADWIqwCAADAWoRVAAAAWIuwCgAAAGsRVgEAAGAtwioAAACsRVgFAACAtQirAAAAsBZhFQAAANYirAIAAMBahFUAAABYi7AKAAAAaxFWAQAAYC3CKgAAAKxFWAUAAIC1CKsAAACwFmEVAAAA1iKsAgAAwFqEVQAAAFiLsAoAAABrEVYBAABgLcIqAAAArEVYBQAAgLUIqwAAALAWYRUAAADWIqwCAADAWoRVAAAAWIuwCgAAAGsRVgEAAGAtwioAAACsRVgFAACAtQirAAAAsBZhFQAAANYirAIAAMBahFUAAABYi7AKAAAAaxFWAQAAYC3CKgAAAKxFWAUAAIC1CKsAAACwFmEVAAAA1iKsAgAAwFqEVQAAAFiLsAoAAABrEVYBAABgLcIqAAAArEVYBQAAgLUIqwAAALAWYRUAAADWIqwCAADAWoRVAAAAWIuwCgAAAGsRVgEAAGAtwioAAACsRVgFAACAtQirAAAAsBZhFQAAANYirAIAAMBahFUAAABYi7AKAAAAaxFWAQAAYC2HMcZ4uwjgnyglJUUjR47U4MGD5XQ6vV0OAFw1PtdgI8IqkEcnT55USEiITpw4oeDgYG+XAwBXjc812IjTAAAAAGAtwioAAACsRVgFAACAtQirQB45nU4NGTKEixAAXDf4XIONuMAKAAAA1mJkFQAAANYirAIAAMBahFUAAABYi7AKAAAAaxFWgTx6++23Va5cOfn7+6tevXpauXKlt0sCgDxZsmSJmjdvrsjISDkcDs2cOdPbJQEuhFUgDz7//HMlJCRoyJAhWrNmjWrUqKHY2FgdOnTI26UBQK6dPn1aNWrU0Ntvv+3tUoAsuHUVkAf16tVT3bp19dZbb0mS0tPTVbZsWfXt21fPPfecl6sDgLxzOBz65ptv9Mgjj3i7FEASI6tArqWmpmr16tVq2rSpq83Hx0dNmzbVsmXLvFgZAADXH8IqkEtHjhxRWlqaSpUq5dZeqlQpHThwwEtVAQBwfSKsAgAAwFqEVSCXwsLC5Ovrq4MHD7q1Hzx4UBEREV6qCgCA6xNhFcglPz8/1a5dWwsXLnS1paena+HChapfv74XKwMA4PpTyNsFAP9ECQkJiouLU506dXT77bdr/PjxOn36tDp37uzt0gAg15KTk7Vt2zbX+x07dmjdunUqXry4brzxRi9WBnDrKiDP3nrrLY0ePVoHDhxQzZo19eabb6pevXreLgsAcu37779XkyZNsrTHxcVpypQpf39BQCaEVQAAAFiLc1YBAABgLcIqAAAArEVYBQAAgLUIqwAAALAWYRUAAADWIqwCAADAWoRVAAAAWIuwCgAAAGsRVgEgl+Lj4/XII4+43t911116+umn//Y6vv/+ezkcDiUlJV2zdVy6rXnxd9QJ4PpFWAVwXYiPj5fD4ZDD4ZCfn58qVaqk4cOH68KFC9d83V9//bVefvllj/r+3cGtXLlyGj9+/N+yLgC4Fgp5uwAAyC/333+/Pv74Y6WkpGju3Lnq3bu3ChcurMGDB2fpm5qaKj8/v3xZb/HixfNlOQCArBhZBXDdcDqdioiIUFRUlHr16qWmTZvqP//5j6T/fZ396quvKjIyUlWrVpUk/fXXX2rTpo1CQ0NVvHhxtWjRQjt37nQtMy0tTQkJCQoNDVWJEiX0zDPPyBjjtt5LTwNISUnRs88+q7Jly8rpdKpSpUr66KOPtHPnTjVp0kSSVKxYMTkcDsXHx0uS0tPTNXLkSJUvX14BAQGqUaOGvvrqK7f1zJ07V1WqVFFAQICaNGniVmdepKWlqWvXrq51Vq1aVRMmTMi277BhwxQeHq7g4GD17NlTqamprmme1J7Zrl271Lx5cxUrVkyBgYG6+eabNXfu3KvaFgDXL0ZWAVy3AgICdPToUdf7hQsXKjg4WPPnz5cknT9/XrGxsapfv75+/PFHFSpUSK+88oruv/9+bdiwQX5+fho7dqymTJmiyZMnq1q1aho7dqy++eYb3X333Tmut1OnTlq2bJnefPNN1ahRQzt27NCRI0dUtmxZzZgxQ61atdLmzZsVHBysgIAASdLIkSP1ySef6L333lPlypW1ZMkSdejQQeHh4WrcuLH++usvtWzZUr1791aPHj20atUqDRgw4Kr2T3p6usqUKaMvv/xSJUqU0NKlS9WjRw+VLl1abdq0cdtv/v7++v7777Vz50517txZJUqU0KuvvupR7Zfq3bu3UlNTtWTJEgUGBur3339XUFDQVW0LgOuYAYDrQFxcnGnRooUxxpj09HQzf/5843Q6zcCBA13TS5UqZVJSUlzzTJs2zVStWtWkp6e72lJSUkxAQICZN2+eMcaY0qVLm1GjRrmmnz9/3pQpU8a1LmOMady4senXr58xxpjNmzcbSWb+/PnZ1rl48WIjyRw/ftzVdu7cOVOkSBGzdOlSt75du3Y17du3N8YYM3jwYBMdHe02/dlnn82yrEtFRUWZcePG5Tj9Ur179zatWrVyvY+LizPFixc3p0+fdrW9++67JigoyKSlpXlU+6XbXL16dTN06FCPawJQsDGyCuC6MXv2bAUFBen8+fNKT0/X448/rqFDh7qmV69e3e081fXr12vbtm0qWrSo23LOnTunP//8UydOnND+/ftVr14917RChQqpTp06WU4FyLBu3Tr5+vpmO6KYk23btunMmTO699573dpTU1N12223SZI2bdrkVock1a9f3+N15OTtt9/W5MmTtXv3bp09e1apqamqWbOmW58aNWqoSJEibutNTk7WX3/9peTk5CvWfqmnnnpKvXr1UmJiopo2bapWrVrp1ltvveptAXB9IqwCuG40adJE7777rvz8/BQZGalChdw/4gIDA93eJycnq3bt2po+fXqWZYWHh+ephoyv9XMjOTlZkjRnzhzdcMMNbtOcTmee6vDE//3f/2ngwIEaO3as6tevr6JFi2r06NFasWKFx8vIS+3dunVTbGys5syZo8TERI0cOVJjx45V3759874xAK5bhFUA143AwEBVqlTJ4/61atXS559/rpIlSyo4ODjbPqVLl9aKFSt05513SpIuXLig1atXq1atWtn2r169utLT0/XDDz+oadOmWaZnjOympaW52qKjo+V0OrV79+4cR2SrVavmulgsw/Lly6+8kZfx888/q0GDBnryySddbX/++WeWfuvXr9fZs2ddQXz58uUKCgpS2bJlVbx48SvWnp2yZcuqZ8+e6tmzpwYPHqxJkyYRVgFki7sBACiw/vWvfyksLEwtWrTQjz/+qB07duj777/XU089pT179kiS+vXrp9dee00zZ87UH3/8oSeffPKy90gtV66c4uLi1KVLF82cOdO1zC+++EKSFBUVJYfDodmzZ+vw4cNKTk5W0aJFNXDgQPXv319Tp07Vn3/+qTVr1mjixImaOnWqJKlnz57aunWrBg0apM2bN+vTTz/VlClTPNrOvXv3at26dW6v48ePq3Llylq1apXmzZunLVu26MUXX9Qvv/ySZf7U1FR17dpVv//+u+bOnashQ4aoT58+8vHx8aj2Sz399NOaN2+eduzYoTVr1mjx4sWqVq2aR9sCoADy9kmzAJAfMl9glZvp+/fvN506dTJhYWHG6XSaChUqmO7du5sTJ04YYy5eUNWvXz8THBxsQkNDTUJCgunUqVOOF1gZY8zZs2dN//79TenSpY2fn5+pVKmSmTx5smv68OHDTUREhHE4HCYuLs4Yc/GisPHjx5uqVauawoULm/DwcBMbG2t++OEH13zffvutqVSpknE6naZRo0Zm8uTJHl1gJSnLa9q0aebcuXMmPj7ehISEmNDQUNOrVy/z3HPPmRo1amTZby+99JIpUaKECQoKMt27dzfnzp1z9blS7ZdeYNWnTx9TsWJF43Q6TXh4uOnYsaM5cuRIjtsAoGBzGJPDVQIAAACAl3EaAAAAAKxFWAUAAIC1CKsAAACwFmEVAAAA1iKsAgAAwFqEVQAAAFiLsAoAAABrEVYBAABgLcIqAAAArEVYBQAAgLUIqwAAALDW/wdG1tQgwWlmXwAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "36. Train a Stacking Classifier using Decision Trees, SVM, and Logistic Regression, and compare accuracy=\\\n",
        "\n",
        "\n",
        "ans:-"
      ],
      "metadata": {
        "id": "Zk0XTkV0d4Ep"
      }
    },
    {
      "source": [
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_breast_cancer"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "4iSr3GdDeJG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "breast_cancer = load_breast_cancer()\n",
        "X, y = breast_cancer.data, breast_cancer.target"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "f_aPJU6MeKBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "t5_DjrgFeKbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "estimators = [\n",
        "    ('dt', DecisionTreeClassifier(random_state=42)),\n",
        "    ('svm', SVC(random_state=42)),\n",
        "    ('lr', LogisticRegression(random_state=42))\n",
        "]\n",
        "final_estimator = LogisticRegression(random_state=42)  # You can choose a different final estimator"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Fek3zqH7eK4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "stacking_classifier = StackingClassifier(estimators=estimators, final_estimator=final_estimator)\n",
        "stacking_classifier.fit(X_train, y_train)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "j0YmscTieLSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Stacking Classifier\n",
        "stacking_predictions = stacking_classifier.predict(X_test)\n",
        "stacking_accuracy = accuracy_score(y_test, stacking_predictions)\n",
        "\n",
        "# Individual models\n",
        "dt_predictions = estimators[0][1].predict(X_test)\n",
        "svm_predictions = estimators[1][1].predict(X_test)\n",
        "# Stacking Classifier\n",
        "stacking_predictions = stacking_classifier.predict(X_test)\n",
        "stacking_accuracy = accuracy_score(y_test, stacking_predictions)\n",
        "\n",
        "# Individual models\n",
        "dt_predictions = estimators[0][1].predict(X_test)\n",
        "svm_predictions = estimators[1][1].predict(X_test)\n",
        "lr_predictions = estimators[2][1].predict(X_test)\n",
        "\n",
        "dt_accuracy = accuracy_score(y_test, dt_predictions)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "rb_9jkN9eLnp",
        "outputId": "260526a9-536b-4a6e-87f0-b01ea930803d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'stacking_classifier' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-079ae22996f6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Stacking Classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mstacking_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstacking_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mstacking_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacking_predictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Individual models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'stacking_classifier' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "37. Train a Random Forest Classifier and print the top 5 most important features=\n",
        "\n",
        "\n",
        "\n",
        "ans:-"
      ],
      "metadata": {
        "id": "I_fuskgRePyF"
      }
    },
    {
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_breast_cancer  # Or use your own dataset"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "-IK1EZQeeU09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "breast_cancer = load_breast_cancer()\n",
        "X, y = breast_cancer.data, breast_cancer.target\n",
        "feature_names = breast_cancer.feature_names  # Get feature names"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "cQmUUhP3eVNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Le8fdrRpeVx3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)  # You can adjust n_estimators\n",
        "rf_classifier.fit(X_train, y_train)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "NYpA7mxleWq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "importances = rf_classifier.feature_importances_\n",
        "indices = importances.argsort()[::-1]  # Get indices of features sorted by importance (descending)\n",
        "\n",
        "print(\"Top 5 most important features:\")\n",
        "for i in range(5):\n",
        "    print(f\"{feature_names[indices[i]]}: {importances[indices[i]]}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Yx3IqCRWeX0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X, y = breast_cancer.data, breast_cancer.target\n",
        "feature_names = breast_cancer.feature_names\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Get and print the top 5 most important features\n",
        "importances = rf_classifier.feature_importances_\n",
        "indices = importances.argsort()[::-1]\n",
        "\n",
        "print(\"Top 5 most important features:\")\n",
        "for i in range(5):\n",
        "    print(f\"{feature_names[indices[i]]}: {importances[indices[i]]}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddlRGrrSeYQx",
        "outputId": "39b07228-c3c8-4cf6-97a7-8c72f74573fa"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 most important features:\n",
            "worst area: 0.15389236463205394\n",
            "worst concave points: 0.14466326620735528\n",
            "mean concave points: 0.10620998844591638\n",
            "worst radius: 0.07798687515738047\n",
            "mean concavity: 0.06800084191430111\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "38.Train a Bagging Classifier and evaluate performance using Precision, Recall, and F1-score=\n",
        "\n",
        "\n",
        "\n",
        "ans:-"
      ],
      "metadata": {
        "id": "6tmk18OMeaZX"
      }
    },
    {
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier  # Base estimator for Bagging\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.datasets import load_breast_cancer"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "mzUAEqIDegGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "breast_cancer = load_breast_cancer()\n",
        "X, y = breast_cancer.data, breast_cancer.target"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Go__GOjeegf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "znpmiYf2eg_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "bagging_classifier = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=100, random_state=42)\n",
        "bagging_classifier.fit(X_train, y_train)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "y4rVsbBMehbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "y_pred = bagging_classifier.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1-score: {f1}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "9ZXI8MD-eh6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X, y = breast_cancer.data, breast_cancer.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the Bagging Classifier\n",
        "bagging_classifier = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=100, random_state=42)\n",
        "bagging_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and evaluate performance\n",
        "y_pred = bagging_classifier.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1-score: {f1}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "rQWP6HQvekhh",
        "outputId": "aacbc1cb-44fa-4ac1-bf61-aeace7e544f2"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "BaggingClassifier.__init__() got an unexpected keyword argument 'base_estimator'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-1ffc26cb96cb>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Create and train the Bagging Classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mbagging_classifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBaggingClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDecisionTreeClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mbagging_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: BaggingClassifier.__init__() got an unexpected keyword argument 'base_estimator'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "39. Train a Random Forest Classifier and analyze the effect of max_depth on accuracy=\n",
        "\n",
        "\n",
        "ans:-"
      ],
      "metadata": {
        "id": "qdUorWjFemBB"
      }
    },
    {
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_breast_cancer"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "afLkTv4BerJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "breast_cancer = load_breast_cancer()\n",
        "X, y = breast_cancer.data, breast_cancer.target"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "h2L_DTGUerrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "J9TDOl2wesi0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "max_depth_values = range(1, 21)  # Explore max_depth from 1 to 20\n",
        "accuracy_scores = []\n",
        "\n",
        "for max_depth in max_depth_values:\n",
        "    rf_classifier = RandomForestClassifier(n_estimators=100, max_depth=max_depth, random_state=42)\n",
        "    rf_classifier.fit(X_train, y_train)\n",
        "    y_pred = rf_classifier.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracy_scores.append(accuracy)\n",
        "    print(f\"Accuracy with max_depth={max_depth}: {accuracy}\")\n",
        "\n",
        "# Create a DataFrame to store the results\n",
        "results = pd.DataFrame({'max_depth': max_depth_values, 'accuracy': accuracy_scores})\n",
        "\n",
        "# Plot the results\n",
        "plt.plot(results['max_depth'], results['accuracy'], marker='o')\n",
        "plt.xlabel('Max Depth (max_depth)')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy vs. Max Depth in Random Forest Classifier')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 835
        },
        "id": "mR7C9IINesym",
        "outputId": "01dd397a-4117-4051-9cd9-96d58064022e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with max_depth=1: 0.956140350877193\n",
            "Accuracy with max_depth=2: 0.9649122807017544\n",
            "Accuracy with max_depth=3: 0.9649122807017544\n",
            "Accuracy with max_depth=4: 0.9649122807017544\n",
            "Accuracy with max_depth=5: 0.9649122807017544\n",
            "Accuracy with max_depth=6: 0.9649122807017544\n",
            "Accuracy with max_depth=7: 0.9649122807017544\n",
            "Accuracy with max_depth=8: 0.9649122807017544\n",
            "Accuracy with max_depth=9: 0.9649122807017544\n",
            "Accuracy with max_depth=10: 0.9649122807017544\n",
            "Accuracy with max_depth=11: 0.9649122807017544\n",
            "Accuracy with max_depth=12: 0.9649122807017544\n",
            "Accuracy with max_depth=13: 0.9649122807017544\n",
            "Accuracy with max_depth=14: 0.9649122807017544\n",
            "Accuracy with max_depth=15: 0.9649122807017544\n",
            "Accuracy with max_depth=16: 0.9649122807017544\n",
            "Accuracy with max_depth=17: 0.9649122807017544\n",
            "Accuracy with max_depth=18: 0.9649122807017544\n",
            "Accuracy with max_depth=19: 0.9649122807017544\n",
            "Accuracy with max_depth=20: 0.9649122807017544\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "40. Train a Bagging Regressor using different base estimators (DecisionTree and KNeighbors) and compare\n",
        "performance=\n",
        "\n",
        "ans:-"
      ],
      "metadata": {
        "id": "tWG6ODgVevaY"
      }
    },
    {
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.datasets import load_boston"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "0Tbpr-Xze1Ae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "boston = load_boston()\n",
        "X, y = boston.data, boston.target"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "WzBw_y8Xe1UJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "base_estimators = [DecisionTreeRegressor(), KNeighborsRegressor()]\n",
        "estimator_names = ['Decision Tree', 'KNeighbors']\n",
        "mse_scores = []\n",
        "\n",
        "for estimator, name in zip(base_estimators, estimator_names):\n",
        "    bagging_regressor = BaggingRegressor(base_estimator=estimator, n_estimators=100, random_state=42)\n",
        "    bagging_regressor.fit(X_train, y_train)\n",
        "    y_pred = bagging_regressor.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    mse_scores.append(mse)\n",
        "    print(f\"MSE with {name} base estimator: {mse}\")\n",
        "\n",
        "# Create a DataFrame to store the results\n",
        "results = pd.DataFrame({'Base Estimator': estimator_names, 'MSE': mse_scores})\n",
        "\n",
        "# Plot the results\n",
        "plt.bar(results['Base Estimator'], results['MSE'])\n",
        "plt.xlabel('Base Estimator')\n",
        "plt.ylabel('Mean Squared Error (MSE)')\n",
        "plt.title('Comparison of Base Estimators in Bagging Regressor')\n",
        "plt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "X289x7OYe2LN",
        "outputId": "b9871cf9-79d7-44cd-f6c2-3cc814f86d2e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'KNeighborsRegressor' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-daea4c793741>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbase_estimators\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mDecisionTreeRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKNeighborsRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mestimator_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Decision Tree'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'KNeighbors'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmse_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimators\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'KNeighborsRegressor' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r9Fuw0XCaJ7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "base_estimators = [DecisionTreeRegressor(), KNeighborsRegressor()]\n",
        "estimator_names = ['Decision Tree', 'KNeighbors']\n",
        "mse_scores = []\n",
        "\n",
        "for estimator, name in zip(base_estimators, estimator_names):\n",
        "    bagging_regressor = BaggingRegressor(base_estimator=estimator, n_estimators=100, random_state=42)\n",
        "    bagging_regressor.fit(X_train, y_train)\n",
        "    y_pred = bagging_regressor.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    mse_scores.append(mse)\n",
        "    print(f\"MSE with {name} base estimator: {mse}\")\n",
        "\n",
        "# Create a DataFrame to store the results\n",
        "results = pd.DataFrame({'Base Estimator': estimator_names, 'MSE': mse_scores})\n",
        "\n",
        "# Plot the results\n",
        "plt.bar(results['Base Estimator'], results['MSE'])\n",
        "plt.xlabel('Base Estimator')\n",
        "plt.ylabel('Mean Squared Error (MSE)')\n",
        "plt.title('Comparison of Base Estimators in Bagging Regressor')\n",
        "plt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "JkJ4R202e5Rg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "base_estimators = [DecisionTreeRegressor(), KNeighborsRegressor()]\n",
        "estimator_names = ['Decision Tree', 'KNeighbors']\n",
        "mse_scores = []\n",
        "\n",
        "for estimator, name in zip(base_estimators, estimator_names):\n",
        "    bagging_regressor = BaggingRegressor(base_estimator=estimator, n_estimators=100, random_state=42)\n",
        "    bagging_regressor.fit(X_train, y_train)\n",
        "    y_pred = bagging_regressor.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    mse_scores.append(mse)\n",
        "    print(f\"MSE with {name} base estimator: {mse}\")\n",
        "\n",
        "# Create a DataFrame to store the results\n",
        "results = pd.DataFrame({'Base Estimator': estimator_names, 'MSE': mse_scores})\n",
        "\n",
        "# Plot the results\n",
        "plt.bar(results['Base Estimator'], results['MSE'])\n",
        "plt.xlabel('Base Estimator')\n",
        "plt.ylabel('Mean Squared Error (MSE)')\n",
        "plt.title('Comparison of Base Estimators in Bagging Regressor')\n",
        "plt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "sT5RDD6De57O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "41. Train a Random Forest Classifier and evaluate its performance using ROC-AUC Score=\n",
        "\n",
        "\n",
        "ans:-"
      ],
      "metadata": {
        "id": "L3mLlJ6CdSYV"
      }
    },
    {
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "from sklearn.datasets import load_breast_cancer"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "7FtmwCdsfB5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "breast_cancer = load_breast_cancer()\n",
        "X, y = breast_cancer.data, breast_cancer.target"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "73_MlgSvfCQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "tahvB_AIfCvN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)  # You can adjust n_estimators\n",
        "rf_classifier.fit(X_train, y_train)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "BuhQ4W1MfC8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "fpr, tpr, thresholds = roc_curve(y_test, y_probs)\n",
        "plt.plot(fpr, tpr, label=f\"ROC Curve (AUC = {roc_auc:.2f})\")\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')  # Diagonal line\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve for Random Forest Classifier\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "F5HKmYxXfEbj",
        "outputId": "23163ddf-8b99-4baa-a149-3166faad9c26"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'roc_curve' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-7af6bfe8f775>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresholds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"ROC Curve (AUC = {roc_auc:.2f})\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinestyle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'--'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gray'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Diagonal line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"False Positive Rate\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"True Positive Rate\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'roc_curve' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "42. Train a Bagging Classifier and evaluate its performance using cross-validatio.\n",
        "\n",
        "\n",
        "ans:-"
      ],
      "metadata": {
        "id": "ZN-lbIFXfGCI"
      }
    },
    {
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier  # You can choose a different base estimator\n",
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "from sklearn.datasets import load_breast_cancer"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Rq1Bi3LlfLMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "breast_cancer = load_breast_cancer()\n",
        "X, y = breast_cancer.data, breast_cancer.target"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "XcCQvpMifLhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "bagging_classifier = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=100, random_state=42)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "cRrtDDlOfMjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "bagging_classifier = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=100, random_state=42)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Wb7aL4o8fM8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "print(f\"Cross-validation scores: {cv_scores}\")\n",
        "print(f\"Mean accuracy: {cv_scores.mean()}\")\n",
        "print(f\"Standard deviation: {cv_scores.std()}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "hwmSkrOvfNmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X, y = breast_cancer.data, breast_cancer.target\n",
        "\n",
        "# Create the Bagging Classifier\n",
        "bagging_classifier = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=100, random_state=42)\n",
        "\n",
        "# Perform cross-validation\n",
        "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "cv_scores = cross_val_score(bagging_classifier, X, y, cv=kfold, scoring='accuracy')\n",
        "\n",
        "# Print the results\n",
        "print(f\"Cross-validation scores: {cv_scores}\")\n",
        "print(f\"Mean accuracy: {cv_scores.mean()}\")\n",
        "print(f\"Standard deviation: {cv_scores.std()}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "OjfkybmlfOGo",
        "outputId": "67a69d5e-0559-4f89-e662-4551e5b36e46"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "BaggingClassifier.__init__() got an unexpected keyword argument 'base_estimator'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-7086758c2d8c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Create the Bagging Classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mbagging_classifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBaggingClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDecisionTreeClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Perform cross-validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: BaggingClassifier.__init__() got an unexpected keyword argument 'base_estimator'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "44. Train a Stacking Classifier with Random Forest and Logistic Regression and compare accuracy=\n",
        "\n",
        "\n",
        "\n",
        "ans:-"
      ],
      "metadata": {
        "id": "dUDVJr0jfPdy"
      }
    },
    {
      "source": [
        "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_breast_cancer\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "sgqdi5TlftkW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "breast_cancer = load_breast_cancer()\n",
        "X, y = breast_cancer.data, breast_cancer.target"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "4b62EDi5ft11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "lK2rJKIFfu-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "lr_classifier = LogisticRegression(random_state=42, max_iter=1000)\n",
        "\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "lr_classifier.fit(X_train, y_train)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "HFEUZc2zfvWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "estimators = [('rf', rf_classifier), ('lr', lr_classifier)]\n",
        "stacking_classifier = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression(random_state=42))\n",
        "stacking_classifier.fit(X_train, y_train)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "jNDcdF8bfvs8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "estimators = [('rf', rf_classifier), ('lr', lr_classifier)]\n",
        "stacking_classifier = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression(random_state=42))\n",
        "stacking_classifier.fit(X_train, y_train)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "skv1kimmfx8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "y_pred_rf = rf_classifier.predict(X_test)\n",
        "y_pred_lr = lr_classifier.predict(X_test)\n",
        "y_pred_stacking = stacking_classifier.predict(X_test)\n",
        "\n",
        "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
        "accuracy_lr = accuracy_score(y_test, y_pred_lr)\n",
        "accuracy_stacking = accuracy_score(y_test, y_pred_stacking)\n",
        "\n",
        "print(f\"Random Forest Accuracy: {accuracy_rf}\")\n",
        "print(f\"Logistic Regression Accuracy: {accuracy_lr}\")\n",
        "print(f\"Stacking Classifier Accuracy: {accuracy_stacking}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "fEQtdHlofyeU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X, y = breast_cancer.data, breast_cancer.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the individual models\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "lr_classifier = LogisticRegression(random_state=42, max_iter=1000)\n",
        "\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "lr_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Create and train the Stacking Classifier\n",
        "estimators = [('rf', rf_classifier), ('lr', lr_classifier)]\n",
        "stacking_classifier = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression(random_state=42))\n",
        "stacking_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred_rf = rf_classifier.predict(X_test)\n",
        "y_pred_lr = lr_classifier.predict(X_test)\n",
        "y_pred_stacking = stacking_classifier.predict(X_test)\n",
        "\n",
        "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
        "accuracy_lr = accuracy_score(y_test, y_pred_lr)\n",
        "accuracy_stacking = accuracy_score(y_test, y_pred_stacking)\n",
        "\n",
        "print(f\"Random Forest Accuracy: {accuracy_rf}\")\n",
        "print(f\"Logistic Regression Accuracy: {accuracy_lr}\")\n",
        "print(f\"Stacking Classifier Accuracy: {accuracy_stacking}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bq4DUPLpfy_d",
        "outputId": "0d3bbf0d-2ec4-44d7-eb58-2151dfd6839f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Accuracy: 0.9649122807017544\n",
            "Logistic Regression Accuracy: 0.956140350877193\n",
            "Stacking Classifier Accuracy: 0.9649122807017544\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "puq115kUfFBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "45. Train a Bagging Regressor with different levels of bootstrap samples and compare performance.\n",
        "\n",
        "\n",
        "ans:-"
      ],
      "metadata": {
        "id": "q_TlJ8nSf1-x"
      }
    },
    {
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor  # You can choose a different base estimator\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.datasets import load_boston"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "uI-VXovCf-NF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "boston = load_boston()\n",
        "X, y = boston.data, boston.target"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "QyXvjpXmgAEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "cbWPLmvJgAWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "bootstrap_sample_sizes = [0.5, 0.7, 0.9, 1.0]  # Explore different proportions of the training data\n",
        "mse_scores = []\n",
        "\n",
        "for sample_size in bootstrap_sample_sizes:\n",
        "    bagging_regressor = BaggingRegressor(base_estimator=DecisionTreeRegressor(),\n",
        "                                         n_estimators=100,\n",
        "                                         max_samples=sample_size,\n",
        "                                         random_state=42)\n",
        "    bagging_regressor.fit(X_train, y_train)\n",
        "    y_pred = bagging_regressor.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    mse_scores.append(mse)\n",
        "    print(f\"MSE with bootstrap sample size {sample_size}: {mse}\")\n",
        "\n",
        "# Create a DataFrame to store the results\n",
        "results = pd.DataFrame({'Bootstrap Sample Size': bootstrap_sample_sizes, 'MSE': mse_scores})\n",
        "\n",
        "# Plot the results\n",
        "plt.plot(results['Bootstrap Sample Size'], results['MSE'], marker='o')\n",
        "plt.xlabel('Bootstrap Sample Size (max_samples)')\n",
        "plt.ylabel('Mean Squared Error (MSE)')\n",
        "plt.title('Performance of Bagging Regressor with Different Bootstrap Sample Sizes')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "Z66A9gK4gA5-",
        "outputId": "cf5020cb-ff91-498e-c1e7-a15b958c8e79"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "BaggingRegressor.__init__() got an unexpected keyword argument 'base_estimator'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-83ff96746d56>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msample_size\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbootstrap_sample_sizes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     bagging_regressor = BaggingRegressor(base_estimator=DecisionTreeRegressor(), \n\u001b[0m\u001b[1;32m      6\u001b[0m                                          \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                                          \u001b[0mmax_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: BaggingRegressor.__init__() got an unexpected keyword argument 'base_estimator'"
          ]
        }
      ]
    },
    {
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.datasets import load_boston\n",
        "\n",
        "# Load the Boston Housing dataset\n",
        "boston = load_boston()\n",
        "X, y = boston.data, boston.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train and evaluate with different bootstrap sample sizes\n",
        "bootstrap_sample_sizes = [0.5, 0.7, 0.9, 1.0]\n",
        "mse_scores = []\n",
        "\n",
        "for sample_size in bootstrap_sample_sizes:\n",
        "    bagging_regressor = BaggingRegressor(base_estimator=DecisionTreeRegressor(),\n",
        "                                         n_estimators=100,\n",
        "                                         max_samples=sample_size,\n",
        "                                         random_state=42)\n",
        "    bagging_regressor.fit(X_train, y_train)\n",
        "    y_pred = bagging_regressor.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    mse_scores.append(mse)\n",
        "    print(f\"MSE with bootstrap sample size {sample_size}: {mse}\")\n",
        "\n",
        "# Create a DataFrame to store the results\n",
        "results = pd.DataFrame({'Bootstrap Sample Size': bootstrap_sample_sizes, 'MSE': mse_scores})\n",
        "\n",
        "# Plot the results\n",
        "plt.plot(results['Bootstrap Sample Size'], results['MSE'], marker='o')\n",
        "plt.xlabel('Bootstrap Sample Size (max_samples)')\n",
        "plt.ylabel('Mean Squared Error (MSE)')\n",
        "plt.title('Performance of Bagging Regressor with Different Bootstrap Sample Sizes')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ocHbHMTOgEMH",
        "outputId": "674314b9-7030-4bfe-a29b-407536216cc1"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "\n`load_boston` has been removed from scikit-learn since version 1.2.\n\nThe Boston housing prices dataset has an ethical problem: as\ninvestigated in [1], the authors of this dataset engineered a\nnon-invertible variable \"B\" assuming that racial self-segregation had a\npositive impact on house prices [2]. Furthermore the goal of the\nresearch that led to the creation of this dataset was to study the\nimpact of air quality but it did not give adequate demonstration of the\nvalidity of this assumption.\n\nThe scikit-learn maintainers therefore strongly discourage the use of\nthis dataset unless the purpose of the code is to study and educate\nabout ethical issues in data science and machine learning.\n\nIn this special case, you can fetch the dataset from the original\nsource::\n\n    import pandas as pd\n    import numpy as np\n\n    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n    target = raw_df.values[1::2, 2]\n\nAlternative datasets include the California housing dataset and the\nAmes housing dataset. You can load the datasets as follows::\n\n    from sklearn.datasets import fetch_california_housing\n    housing = fetch_california_housing()\n\nfor the California housing dataset and::\n\n    from sklearn.datasets import fetch_openml\n    housing = fetch_openml(name=\"house_prices\", as_frame=True)\n\nfor the Ames housing dataset.\n\n[1] M Carlisle.\n\"Racist data destruction?\"\n<https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>\n\n[2] Harrison Jr, David, and Daniel L. Rubinfeld.\n\"Hedonic housing prices and the demand for clean air.\"\nJournal of environmental economics and management 5.1 (1978): 81-102.\n<https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-edfcfb8e39a1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_boston\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Load the Boston Housing dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/datasets/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    159\u001b[0m             \"\"\"\n\u001b[1;32m    160\u001b[0m         )\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: \n`load_boston` has been removed from scikit-learn since version 1.2.\n\nThe Boston housing prices dataset has an ethical problem: as\ninvestigated in [1], the authors of this dataset engineered a\nnon-invertible variable \"B\" assuming that racial self-segregation had a\npositive impact on house prices [2]. Furthermore the goal of the\nresearch that led to the creation of this dataset was to study the\nimpact of air quality but it did not give adequate demonstration of the\nvalidity of this assumption.\n\nThe scikit-learn maintainers therefore strongly discourage the use of\nthis dataset unless the purpose of the code is to study and educate\nabout ethical issues in data science and machine learning.\n\nIn this special case, you can fetch the dataset from the original\nsource::\n\n    import pandas as pd\n    import numpy as np\n\n    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n    target = raw_df.values[1::2, 2]\n\nAlternative datasets include the California housing dataset and the\nAmes housing dataset. You can load the datasets as follows::\n\n    from sklearn.datasets import fetch_california_housing\n    housing = fetch_california_housing()\n\nfor the California housing dataset and::\n\n    from sklearn.datasets import fetch_openml\n    housing = fetch_openml(name=\"house_prices\", as_frame=True)\n\nfor the Ames housing dataset.\n\n[1] M Carlisle.\n\"Racist data destruction?\"\n<https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>\n\n[2] Harrison Jr, David, and Daniel L. Rubinfeld.\n\"Hedonic housing prices and the demand for clean air.\"\nJournal of environmental economics and management 5.1 (1978): 81-102.\n<https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>\n",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y_KxegBigBic"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}