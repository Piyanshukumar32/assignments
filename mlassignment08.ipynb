{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GYwJNhkA7HjG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Can we use Bagging for regression problems\n",
        "\n",
        "\n",
        "ans:-Yes, Bagging can be used for regression problems.\n",
        "\n",
        "Bagging, or Bootstrap Aggregating, is an ensemble method that can be used for both regression and classification problems. In the context of regression, bagging involves creating multiple subsets of the training data using bootstrapping (sampling with replacement). A regression model is then trained on each subset, and the predictions from these models are averaged to produce the final prediction.\n",
        "\n",
        "Here's how bagging works for regression:\n",
        "\n",
        "Create multiple subsets of the training data using bootstrapping. This involves randomly sampling data points from the training data with replacement to create multiple subsets of the same size as the original training data.\n",
        "Train a regression model on each subset. Any regression model can be used, such as decision trees, linear regression, or support vector machines.\n",
        "Average the predictions from the individual models. For a given input, each model will produce a prediction. These predictions are then averaged to produce the final prediction.\n",
        "Bagging helps to reduce the variance of the regression model and improve its overall accuracy. By averaging the predictions from multiple models, bagging reduces the impact of any single model's errors. This can lead to a more robust and stable model that is less prone to overfitting.\n",
        "\n",
        "Here's an example of how to use bagging for regression in Python using the BaggingRegressor class from scikit-learn:\n",
        "\n",
        "\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "# Create a BaggingRegressor object\n",
        "bagging_regressor = BaggingRegressor(base_estimator=DecisionTreeRegressor(), n_estimators=10, random_state=42)\n",
        "\n",
        "# Fit the model to the training data\n",
        "bagging_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = bagging_regressor.predict(X_test)\n"
      ],
      "metadata": {
        "id": "d9OqsZFF7NoV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2 What is the difference between multiple model training and single model training\n",
        "\n",
        "\n",
        "ans:-In single model training, you train one model on your entire dataset to solve a specific problem. This approach is straightforward and often works well for simple tasks. However, it can be prone to overfitting, especially when the dataset is complex or has noisy data.\n",
        "\n",
        "In multiple model training, you train several models on different subsets of the data or using different algorithms. These models are then combined to make a final prediction. This approach is also known as ensemble learning and can offer several advantages:\n",
        "\n",
        "Improved accuracy: By combining the predictions of multiple models, you can often achieve higher accuracy than with a single model. This is because different models may capture different aspects of the data or make different types of errors.\n",
        "Reduced overfitting: Multiple models can help to reduce overfitting by averaging out the predictions of individual models, which may have overfit to the training data.\n",
        "Increased robustness: Ensemble models can be more robust to noisy data or outliers, as the predictions of individual models are less likely to be affected by these issues.\n",
        "Better generalization: By training models on different subsets of the data, you can improve the generalization ability of the ensemble model, meaning it is more likely to perform well on unseen data.\n",
        "Here's a table summarizing the key differences:\n",
        "\n",
        "Feature\tSingle Model Training\tMultiple Model Training\n",
        "Number of models\tOne\tMultiple\n",
        "Complexity\tSimpler\tMore complex\n",
        "Accuracy\tCan be lower\tOften higher\n",
        "Overfitting\tMore prone\tLess prone\n",
        "Robustness\tLower\tHigher\n",
        "Generalization\tCan be lower\tOften better\n",
        "When to use multiple model training:\n",
        "\n",
        "When you want to achieve the highest possible accuracy.\n",
        "When you have a complex dataset with noisy data or outliers.\n",
        "When you want to improve the generalization ability of your model.\n",
        "When to use single model training:\n",
        "\n",
        "When you have a simple task or a small dataset.\n",
        "When you want a model that is easy to understand and interpret.\n",
        "When you have limited computational resources."
      ],
      "metadata": {
        "id": "mcU8-dx67eR2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " 3.Explain the concept of feature randomness in Random Forest\n",
        "\n",
        " ans:-Okay, let's explore the concept of feature randomness in Random Forest.\n",
        "\n",
        "Feature randomness, also known as feature bagging or the random subspace method, is a crucial aspect of the Random Forest algorithm. It involves randomly selecting a subset of features at each node of a decision tree during the tree-building process. This random selection of features for each split is what makes Random Forest different from bagging with decision trees, where all features are considered for each split.\n",
        "\n",
        "Here's a breakdown of how feature randomness works in Random Forest:\n",
        "\n",
        "Building Individual Decision Trees: In a Random Forest, multiple decision trees are built. For each tree, a bootstrap sample (random sampling with replacement) of the training data is taken.\n",
        "\n",
        "Feature Subset Selection: At each node of a decision tree, instead of considering all features for the best split, a random subset of features is selected. The size of this subset is typically denoted by max_features and is a hyperparameter that can be tuned.\n",
        "\n",
        "Splitting the Node: The best split point is determined using only the selected subset of features, further introducing randomness into the tree-building process.\n",
        "\n",
        "Repeating the Process: Steps 2 and 3 are repeated for each node until the tree reaches a stopping criterion (e.g., maximum depth or minimum samples per leaf).\n",
        "\n",
        "Ensemble Prediction: The predictions from all individual trees are then aggregated (usually by averaging) to produce the final prediction of the Random Forest.\n",
        "\n",
        "Benefits of Feature Randomness\n",
        "\n",
        "Reduced Correlation between Trees: By using different feature subsets for each tree, the correlation between individual trees in the forest is reduced. This is important because highly correlated trees tend to make similar errors, diminishing the benefits of ensembling.\n",
        "Improved Generalization: Feature randomness helps the model generalize better to unseen data by reducing the risk of overfitting to specific features in the training set.\n",
        "Handling High-Dimensional Data: It allows the algorithm to effectively handle datasets with a large number of features, even if some features are irrelevant or redundant.\n",
        "Example Let's say you have a dataset with 10 features. If max_features is set to 3, then at each node of a decision tree, only 3 random features will be considered for the split. This process is repeated for each tree in the fores"
      ],
      "metadata": {
        "id": "JyFk3OXP7qZB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.What is OOB (Out-of-Bag) Score.\n",
        "\n",
        "ans:-OOB Score The OOB (Out-of-Bag) Score is a method for evaluating the performance of a Random Forest model without the need for a separate validation set. It leverages the way Random Forest uses bootstrapping during the training process.\n",
        "\n",
        "How it works:\n",
        "\n",
        "Bootstrapping: When training a Random Forest, each decision tree is built using a bootstrap sample of the training data. This means that some data points are selected multiple times, while others are left out (out-of-bag).\n",
        "\n",
        "OOB Samples: For each tree in the forest, the data points that were not included in its bootstrap sample are considered as its OOB samples.\n",
        "\n",
        "Prediction and Evaluation: Each tree can make predictions on its OOB samples. By aggregating these predictions (e.g., averaging for regression, majority voting for classification), we can obtain an OOB prediction for each data point in the original training set.\n",
        "\n",
        "OOB Score: The OOB score is then calculated by comparing the OOB predictions to the actual target values for the training data. It essentially represents the model's performance on unseen data, similar to cross-validation.\n",
        "\n",
        "Benefits of using OOB Score:\n",
        "\n",
        "No need for a separate validation set: This saves data and computational resources.\n",
        "Efficient estimation of performance: It's calculated during training, providing a readily available performance metric.\n",
        "Similar to cross-validation: It gives a good estimate of how the model will generalize to new data.\n",
        "In Scikit-learn: When using Random Forest in scikit-learn (e.g., RandomForestClassifier or RandomForestRegressor), you can enable OOB scoring by setting the oob_score parameter to True during model initialization. After training, you can access the OOB score using the oob_score_ attribute of the model.\n",
        "\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf_classifier = RandomForestClassifier(oob_score=True, random_state=42)\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "oob_score = rf_classifier.oob_score_\n",
        "print(\"OOB Score:\", oob_score)"
      ],
      "metadata": {
        "id": "hlYjMdIQ73ob"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\t5.How can you measure the importance of features in a Random Forest model\n",
        "\n",
        "  ans:_Okay, let's discuss how to measure the importance of features in a Random Forest model.\n",
        "\n",
        "Random Forest models offer a built-in way to assess feature importance, which helps you understand which features contribute the most to the model's predictive power. Here's how it works:\n",
        "\n",
        "Feature Importance based on Mean Decrease in Impurity\n",
        "\n",
        "Tree Traversal: During the construction of each decision tree in the Random Forest, the algorithm calculates the decrease in impurity (e.g., Gini impurity or entropy) achieved by splitting on each feature.\n",
        "\n",
        "Importance Calculation: For each feature, the total decrease in impurity across all trees in the forest is accumulated. This accumulated value represents the importance of the feature.\n",
        "\n",
        "Normalization: The feature importances are then normalized to sum up to 1, making them easier to interpret as relative contributions.\n",
        "\n",
        "Accessing Feature Importance in Scikit-learn:\n",
        "\n",
        "In Scikit-learn, you can access the feature importances using the feature_importances_ attribute of the trained Random Forest model (RandomForestClassifier or RandomForestRegressor).\n",
        "\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Assuming you have your features in X and target in y\n",
        "rf_classifier = RandomForestClassifier(random_state=42)\n",
        "rf_classifier.fit(X, y)\n",
        "\n",
        "# Access feature importances\n",
        "feature_importances = rf_classifier.feature_importances_\n",
        "\n",
        "# Print or visualize the feature importances\n",
        "for i, feature_name in enumerate(X.columns):  # Assuming X is a pandas DataFrame\n",
        "    print(f\"Feature {feature_name}: Importance = {feature_importances[i]}\")\n",
        "\n",
        "# You can also create visualizations (e.g., bar plots) to better understand the feature importances.\n",
        "Use code with caution\n",
        "Interpreting Feature Importance:\n",
        "\n",
        "Higher values of feature importance indicate that the feature plays a more significant role in the model's predictions. Features with low importance might be less relevant or redundant and could potentially be removed to simplify the model.\n",
        "\n",
        "Important Considerations:\n",
        "\n",
        "Feature Scaling: Feature importance can be influenced by the scale of the features. It's generally recommended to scale features before training the Random Forest to avoid bias towards features with larger values.\n",
        "Correlated Features: If you have highly correlated features, the importance might be distributed among them. In such cases, consider removing redundant features or using feature selection techniques."
      ],
      "metadata": {
        "id": "Z-L0l6JA8NMh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Explain the working principle of a Bagging Classifier\n",
        "\n",
        "\n",
        "ans:-Bagging Classifier A Bagging Classifier is an ensemble learning method that combines the predictions of multiple base classifiers (typically decision trees) to improve the overall predictive accuracy and robustness of the model. It's based on the concept of bootstrap aggregating, or bagging.\n",
        "\n",
        "Working Principle:\n",
        "\n",
        "Bootstrap Sampling:\n",
        "\n",
        "The Bagging Classifier starts by creating multiple subsets of the training data using bootstrapping.\n",
        "Bootstrapping involves randomly sampling data points from the original training dataset with replacement. This means that some data points may appear multiple times in a single subset, while others may be left out.\n",
        "These subsets are typically the same size as the original training dataset.\n",
        "Training Base Classifiers:\n",
        "\n",
        "A base classifier (e.g., a decision tree) is trained independently on each of the bootstrap samples.\n",
        "Since each sample is slightly different, the resulting base classifiers will have some variations in their learned patterns.\n",
        "Aggregation of Predictions:\n",
        "\n",
        "When making predictions on new data, each base classifier makes its own prediction.\n",
        "The Bagging Classifier then aggregates these predictions to produce the final prediction.\n",
        "For classification tasks, this is usually done through majority voting, where the class predicted by the majority of base classifiers is chosen as the final prediction.\n",
        "How Bagging Improves Performance:\n",
        "\n",
        "Reduces Variance: By averaging the predictions of multiple base classifiers, bagging reduces the variance of the model. This means that the model is less sensitive to the specific training data used and is more likely to generalize well to unseen data.\n",
        "Improves Stability: Bagging makes the model more stable by reducing the impact of outliers or noisy data points. Individual base classifiers may be affected by these points, but their influence is minimized when the predictions are aggregated.\n",
        "Handles Complex Relationships: Bagging can handle complex relationships in the data by allowing individual base classifiers to learn different aspects of the data.\n",
        "In Scikit-learn:\n",
        "\n",
        "You can use the BaggingClassifier class from sklearn.ensemble to implement a Bagging Classifier in Python.\n",
        "\n",
        "\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Create a BaggingClassifier with 10 Decision Tree base classifiers\n",
        "bagging_classifier = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=10, random_state=42)\n",
        "\n",
        "# Fit the model to the training data\n",
        "bagging_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = bagging_classifier.predict(X_test)"
      ],
      "metadata": {
        "id": "BIxmCuXJ8inh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.How do you evaluate a Bagging Classifier’s performance\n",
        "\n",
        "\n",
        "ans:-Evaluating the performance of a Bagging Classifier is essential to understand how well it generalizes to unseen data and to compare it with other models. Here are some common methods for evaluation:\n",
        "\n",
        "1. Accuracy:\n",
        "\n",
        "Accuracy is the most straightforward metric, representing the percentage of correctly classified instances.\n",
        "It's calculated as: (Number of correct predictions) / (Total number of predictions)\n",
        "While simple, accuracy can be misleading for imbalanced datasets where one class is much more prevalent than others.\n",
        "2. Precision and Recall:\n",
        "\n",
        "Precision measures the proportion of correctly predicted positive instances out of all instances predicted as positive.\n",
        "Recall measures the proportion of correctly predicted positive instances out of all actual positive instances.\n",
        "These metrics are particularly useful when the cost of false positives or false negatives is different.\n",
        "3. F1-Score:\n",
        "\n",
        "The F1-score is the harmonic mean of precision and recall, providing a balanced measure of both.\n",
        "It's calculated as: 2 * (Precision * Recall) / (Precision + Recall)\n",
        "The F1-score is a good overall metric when you want to consider both precision and recall.\n",
        "4. Confusion Matrix:\n",
        "\n",
        "A confusion matrix provides a detailed breakdown of the model's predictions, showing the counts of true positives, true negatives, false positives, and false negatives.\n",
        "It helps to visualize the performance of the classifier and identify areas where it might be making mistakes.\n",
        "5. ROC Curve and AUC:\n",
        "\n",
        "The Receiver Operating Characteristic (ROC) curve plots the true positive rate against the false positive rate at various classification thresholds.\n",
        "The Area Under the Curve (AUC) summarizes the ROC curve, providing a single value representing the classifier's overall performance.\n",
        "ROC curves and AUC are useful for evaluating the classifier's ability to discriminate between classes.\n",
        "6. Cross-Validation:\n",
        "\n",
        "Cross-validation involves splitting the data into multiple folds and training the model on different combinations of folds.\n",
        "This helps to assess the model's performance on different subsets of the data and get a more robust estimate of its generalization ability.\n",
        "In Scikit-learn:\n",
        "\n",
        "You can use various functions from sklearn.metrics to calculate these evaluation metrics. For example:\n",
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score\n",
        "\n",
        "# Assuming you have your predictions in y_pred and true labels in y_true\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred)\n",
        "recall = recall_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "confusion_mat = confusion_matrix(y_true, y_pred)\n",
        "auc = roc_auc_score(y_true, y_pred)\n",
        "Use code with caution\n",
        "Choosing the Right Metric:\n",
        "\n",
        "The choice of evaluation metric depends on the specific problem and the relative importance of different types of errors. Consider the context and the desired outcome when selecting the most appropriate metric for your Bagging Classifier.\n"
      ],
      "metadata": {
        "id": "_ZrJD6pR8vcv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.How does a Bagging Regressor work\n",
        "\n",
        "\n",
        "ans:-Bagging Regressor A Bagging Regressor is an ensemble learning method used for regression tasks. It builds upon the concept of bootstrap aggregating (bagging) to improve the accuracy and stability of regression models.\n",
        "\n",
        "Working Principle:\n",
        "\n",
        "Bootstrap Sampling:\n",
        "\n",
        "Similar to the Bagging Classifier, the Bagging Regressor starts by creating multiple subsets of the training data using bootstrapping.\n",
        "Bootstrapping involves randomly sampling data points from the original training dataset with replacement. This means that some data points may appear multiple times in a single subset, while others may be left out.\n",
        "These subsets are typically the same size as the original training dataset.\n",
        "Training Base Regressors:\n",
        "\n",
        "A base regressor (e.g., a decision tree regressor, linear regression) is trained independently on each of the bootstrap samples.\n",
        "Since each sample is slightly different, the resulting base regressors will have some variations in their learned patterns and predictions.\n",
        "Aggregation of Predictions:\n",
        "\n",
        "When making predictions on new data, each base regressor makes its own prediction.\n",
        "The Bagging Regressor then aggregates these predictions to produce the final prediction.\n",
        "For regression tasks, this is typically done by averaging the predictions of all base regressors.\n",
        "How Bagging Improves Regression Performance:\n",
        "\n",
        "Reduces Variance: By averaging the predictions of multiple base regressors, bagging reduces the variance of the model. This makes the model less sensitive to the specific training data used and more likely to generalize well to unseen data.\n",
        "Improves Stability: Bagging makes the model more stable by reducing the impact of outliers or noisy data points. Individual base regressors may be affected by these points, but their influence is minimized when the predictions are aggregated.\n",
        "Handles Complex Relationships: Bagging can handle complex relationships in the data by allowing individual base regressors to learn different aspects of the data and their contributions to the target variable.\n",
        "In Scikit-learn:\n",
        "\n",
        "You can use the BaggingRegressor class from sklearn.ensemble to implement a Bagging Regressor in Python.\n",
        "\n",
        "\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "# Create a BaggingRegressor with 10 Decision Tree base regressors\n",
        "bagging_regressor = BaggingRegressor(base_estimator=DecisionTreeRegressor(), n_estimators=10, random_state=42)\n",
        "\n",
        "# Fit the model to the training data\n",
        "bagging_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = bagging_regressor.predict(X_test)"
      ],
      "metadata": {
        "id": "VNboUKqi87GF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.What is the main advantage of ensemble techniques\n",
        "\n",
        "ans:-Bagging Regressor A Bagging Regressor is an ensemble learning method used for regression tasks. It builds upon the concept of bootstrap aggregating (bagging) to improve the accuracy and stability of regression models.\n",
        "\n",
        "Working Principle:\n",
        "\n",
        "Bootstrap Sampling:\n",
        "\n",
        "Similar to the Bagging Classifier, the Bagging Regressor starts by creating multiple subsets of the training data using bootstrapping.\n",
        "Bootstrapping involves randomly sampling data points from the original training dataset with replacement. This means that some data points may appear multiple times in a single subset, while others may be left out.\n",
        "These subsets are typically the same size as the original training dataset.\n",
        "Training Base Regressors:\n",
        "\n",
        "A base regressor (e.g., a decision tree regressor, linear regression) is trained independently on each of the bootstrap samples.\n",
        "Since each sample is slightly different, the resulting base regressors will have some variations in their learned patterns and predictions.\n",
        "Aggregation of Predictions:\n",
        "\n",
        "When making predictions on new data, each base regressor makes its own prediction.\n",
        "The Bagging Regressor then aggregates these predictions to produce the final prediction.\n",
        "For regression tasks, this is typically done by averaging the predictions of all base regressors.\n",
        "How Bagging Improves Regression Performance:\n",
        "\n",
        "Reduces Variance: By averaging the predictions of multiple base regressors, bagging reduces the variance of the model. This makes the model less sensitive to the specific training data used and more likely to generalize well to unseen data.\n",
        "Improves Stability: Bagging makes the model more stable by reducing the impact of outliers or noisy data points. Individual base regressors may be affected by these points, but their influence is minimized when the predictions are aggregated.\n",
        "Handles Complex Relationships: Bagging can handle complex relationships in the data by allowing individual base regressors to learn different aspects of the data and their contributions to the target variable.\n",
        "In Scikit-learn:\n",
        "\n",
        "You can use the BaggingRegressor class from sklearn.ensemble to implement a Bagging Regressor in Python.\n",
        "\n",
        "\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "# Create a BaggingRegressor with 10 Decision Tree base regressors\n",
        "bagging_regressor = BaggingRegressor(base_estimator=DecisionTreeRegressor(), n_estimators=10, random_state=42)\n",
        "\n",
        "# Fit the model to the training data\n",
        "bagging_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = bagging_regressor.predict(X_test)"
      ],
      "metadata": {
        "id": "kia9gp_n9Ot-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5g69yvzo9XYN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.What is the main challenge of ensemble methods\n",
        "\n",
        "\n",
        "ans:-While ensemble methods offer significant advantages in terms of accuracy and robustness, they also come with certain challenges. Here's a breakdown of the main challenge:\n",
        "\n",
        "Increased Computational Complexity:\n",
        "\n",
        "Multiple Models: Ensemble methods involve training and maintaining multiple individual models (base learners), which can significantly increase computational cost and resource requirements compared to using a single model.\n",
        "Training Time: Training multiple models takes longer than training a single model, especially for complex base learners or large datasets.\n",
        "Memory Usage: Storing and managing multiple models can require more memory, potentially exceeding available resources.\n",
        "Prediction Time: Predicting with an ensemble involves getting predictions from all base learners and then aggregating them, which can add to the prediction time.\n",
        "Other Challenges:\n",
        "\n",
        "Interpretability: Ensemble models, particularly complex ones like Random Forests or stacked models, can be more difficult to interpret than single models. Understanding the relationships between features and predictions can be challenging.\n",
        "Model Selection and Tuning: Choosing the right base learners, ensemble method, and hyperparameters can be a complex process that requires experimentation and expertise.\n",
        "Data Requirements: Ensembles often require larger datasets to train effectively compared to single models.\n",
        "Mitigation Strategies:\n",
        "\n",
        "Feature Selection: Using feature selection techniques to reduce the number of features can decrease computational complexity.\n",
        "Pruning: Techniques like tree pruning can help simplify base learners and reduce their size.\n",
        "Parallel Computing: Utilizing parallel computing resources can speed up the training and prediction process.\n",
        "Model Selection Strategies: Applying systematic model selection strategies can help identify the best ensemble configuration.\n",
        "Feature Engineering: Effective feature engineering can improve the quality of data and reduce the need for complex models."
      ],
      "metadata": {
        "id": "VD6hTyQm9cZg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11.2 Explain the key idea behind ensemble techniques\n",
        "\n",
        "ans:-Ensemble techniques in machine learning are based on the principle of \"wisdom of the crowd.\" The core idea is that combining the predictions of multiple individual models (base learners) can often lead to better overall performance than using a single model alone.\n",
        "\n",
        "Here's a breakdown of the key concepts:\n",
        "\n",
        "Diversity: Ensemble methods work best when the base learners are diverse, meaning they make different types of errors or capture different aspects of the data. This diversity can be achieved by using different algorithms, training on different subsets of the data, or introducing randomness into the model building process.\n",
        "\n",
        "Aggregation: The predictions of the individual base learners are combined using an aggregation function, such as averaging for regression or majority voting for classification. This aggregation process helps to reduce the impact of individual model errors and improve the overall prediction accuracy.\n",
        "\n",
        "Bias-Variance Trade-off: Ensemble methods aim to strike a balance between bias and variance in the model. Bias refers to the error introduced by simplifying assumptions about the data, while variance refers to the sensitivity of the model to fluctuations in the training data. By combining multiple models, ensemble methods can often reduce both bias and variance, leading to improved generalization performance.\n",
        "\n",
        "In simpler terms: Imagine you have a group of experts with different backgrounds and perspectives. Instead of relying on the opinion of a single expert, you would likely get a more accurate and robust decision by combining their insights. Ensemble techniques apply this same principle to machine learning models.\n",
        "\n",
        "Benefits of Ensemble Techniques:\n",
        "\n",
        "Improved Accuracy: By combining the predictions of multiple models, ensemble methods often achieve higher accuracy than individual models.\n",
        "Reduced Overfitting: Ensembles can help to prevent overfitting by averaging out the predictions of individual models, which may have overfit to the training data.\n",
        "Increased Robustness: Ensemble models are less sensitive to noisy data or outliers, as the predictions of individual models are less likely to be affected by these issues.\n",
        "Better Generalization: Ensembles can improve the generalization ability of the model, meaning it is more likely to perform well on unseen data."
      ],
      "metadata": {
        "id": "-HqfjcQU-iHo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12.What is a Random Forest Classifier\n",
        "\n",
        "\n",
        "ans:-Okay, let's discuss the Random Forest Classifier.\n",
        "\n",
        "A Random Forest Classifier is an ensemble learning method used for classification tasks. It belongs to the bagging (Bootstrap Aggregating) family of ensemble methods and builds upon the concept of decision trees.\n",
        "\n",
        "Here's a breakdown of how it works:\n",
        "\n",
        "Building Multiple Decision Trees: A Random Forest creates multiple decision trees during its training phase. Each tree is built using a different bootstrap sample (random sampling with replacement) of the training data. This introduces diversity among the trees.\n",
        "\n",
        "Feature Randomness: When building each decision tree, the Random Forest randomly selects a subset of features at each node to determine the best split. This feature randomness further decorrelates the trees and improves the model's ability to generalize to unseen data.\n",
        "\n",
        "Aggregation of Predictions: To make a prediction on a new data point, the Random Forest passes the data point through each individual decision tree. Each tree makes its own prediction (class label). The Random Forest then aggregates these predictions, typically by majority voting, to produce the final prediction.\n",
        "\n",
        "Key Advantages of Random Forest Classifier:\n",
        "\n",
        "High Accuracy: Random Forests often achieve high accuracy compared to single decision trees and other classification algorithms.\n",
        "Robustness to Overfitting: By combining multiple trees and introducing randomness, Random Forests are less prone to overfitting the training data.\n",
        "Handles High-Dimensional Data: Random Forests can effectively handle datasets with a large number of features.\n",
        "Feature Importance Estimation: Random Forests provide a way to measure the importance of features in the prediction process.\n",
        "Handles Missing Values: Random Forests can handle missing values in the data without requiring imputation.\n",
        "In Scikit-learn:\n",
        "\n",
        "You can use the RandomForestClassifier class from the sklearn.ensemble module to implement a Random Forest Classifier in Python.\n",
        "\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Create a Random Forest Classifier with 100 trees\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Fit the model to the training data\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = rf_classifier.predict(X_test)"
      ],
      "metadata": {
        "id": "k5JfuScf-xuC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13.What are the main types of ensemble techniques\n",
        "\n",
        "ans:-Okay, let's discuss the main types of ensemble techniques in machine learning.\n",
        "\n",
        "Ensemble techniques can be broadly categorized into two main types:\n",
        "\n",
        "1. Averaging Methods:\n",
        "\n",
        "These methods aim to create multiple independent base learners and then average their predictions to obtain the final prediction. The key idea is to reduce variance by combining the predictions of multiple models.\n",
        "\n",
        "Bagging (Bootstrap Aggregating): This technique involves creating multiple subsets of the training data using bootstrapping (sampling with replacement). A base learner is trained on each subset, and the predictions from these learners are averaged to produce the final prediction. Random Forest is a popular example of bagging.\n",
        "Pasting: Similar to bagging, but without replacement during the sampling process.\n",
        "2. Boosting Methods:\n",
        "\n",
        "These methods focus on sequentially building an ensemble where each subsequent learner tries to correct the errors made by the previous learners. The key idea is to reduce bias by iteratively improving the model's predictions.\n",
        "\n",
        "AdaBoost (Adaptive Boosting): This technique assigns weights to training instances, giving higher weights to misclassified instances. Each subsequent learner focuses on correctly classifying the instances with higher weights.\n",
        "Gradient Boosting: This technique builds an ensemble by adding new learners that try to predict the residuals (errors) of the previous learners. Gradient descent is used to minimize the overall error. XGBoost, LightGBM, and CatBoost are popular examples of gradient boosting.\n",
        "Other Ensemble Techniques:\n",
        "\n",
        "Stacking: This technique combines multiple base learners (of different types) by training a meta-learner on their predictions. The meta-learner learns how to best combine the predictions of the base learners to improve overall performance.\n",
        "Voting: This technique combines predictions from multiple base learners by either majority voting (for classification) or averaging (for regression).\n",
        "Choosing the Right Ensemble Technique:\n",
        "\n",
        "The choice of ensemble technique depends on the specific problem and the characteristics of the data.\n",
        "\n",
        "Bagging is generally effective in reducing variance and improving stability, especially for models with high variance like decision trees.\n",
        "Boosting is typically better at reducing bias and improving accuracy, but it can be more prone to overfitting if not carefully tuned.\n",
        "Stacking can potentially achieve higher accuracy than individual base learners or simple averaging methods, but it can be more computationally expensive."
      ],
      "metadata": {
        "id": "UEZrz9US-5kB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14.What is ensemble learning in machine learning\n",
        "\n",
        "\n",
        "ans:-Ensemble learning is a machine learning paradigm where multiple models (often called \"weak learners\") are trained to solve the same problem and then combined to get better results. The main idea is that combining the predictions of multiple models can often lead to better overall performance than using a single model alone. This is analogous to the \"wisdom of the crowd\" concept, where collective knowledge is often more accurate than individual opinions.\n",
        "\n",
        "Here's a breakdown of the key aspects of ensemble learning:\n",
        "\n",
        "Base Learners: These are the individual models that make up the ensemble. They can be of the same type (e.g., all decision trees) or different types (e.g., a mix of decision trees, support vector machines, and neural networks).\n",
        "\n",
        "Ensemble Method: This is the technique used to combine the predictions of the base learners. Common methods include averaging, voting, and stacking.\n",
        "\n",
        "Diversity: Ensemble methods work best when the base learners are diverse, meaning they make different types of errors or capture different aspects of the data. This diversity can be achieved by using different algorithms, training on different subsets of the data, or introducing randomness into the model building process.\n",
        "\n",
        "Benefits of Ensemble Learning:\n",
        "\n",
        "Improved Accuracy: By combining the predictions of multiple models, ensemble methods often achieve higher accuracy than individual models.\n",
        "Reduced Overfitting: Ensembles can help to prevent overfitting by averaging out the predictions of individual models, which may have overfit to the training data.\n",
        "Increased Robustness: Ensemble models are less sensitive to noisy data or outliers, as the predictions of individual models are less likely to be affected by these issues.\n",
        "Better Generalization: Ensembles can improve the generalization ability of the model, meaning it is more likely to perform well on unseen data.\n",
        "Common Ensemble Techniques:\n",
        "\n",
        "Bagging (Bootstrap Aggregating): Random Forest is a popular example.\n",
        "Boosting: AdaBoost and Gradient Boosting are popular examples.\n",
        "Stacking: Combines multiple base learners using a meta-learner.\n",
        "Voting: Combines predictions using majority voting or averaging."
      ],
      "metadata": {
        "id": "l3XFGR1S_CKa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15.When should we avoid using ensemble methods\n",
        "\n",
        "\n",
        "ans;-Okay, let's discuss when it might be best to avoid using ensemble methods in machine learning.\n",
        "\n",
        "While ensemble methods offer many advantages, there are certain situations where they might not be the most suitable choice. Here are some scenarios when you should consider avoiding ensemble methods:\n",
        "\n",
        "Simple Tasks with Limited Data:\n",
        "\n",
        "If you have a relatively simple task with a small dataset, a single well-tuned model might be sufficient and more efficient than an ensemble. Ensembles are more effective when dealing with complex tasks and larger datasets.\n",
        "High Interpretability Requirements:\n",
        "\n",
        "Ensemble models, especially complex ones like Random Forests or stacked models, can be difficult to interpret compared to single models. If interpretability is a critical requirement for your application, you might want to consider using a simpler model that is easier to understand.\n",
        "Limited Computational Resources:\n",
        "\n",
        "Ensemble methods involve training and maintaining multiple models, which can be computationally expensive. If you have limited computational resources or need to deploy models with fast prediction times, a single model might be a more practical choice.\n",
        "Real-time or Low-latency Requirements:\n",
        "\n",
        "Ensembles can add overhead to the prediction process, as they require predictions from multiple base learners. If your application has strict real-time or low-latency requirements, a single model that can make predictions quickly might be preferable.\n",
        "High Memory Constraints:\n",
        "\n",
        "Storing and managing multiple models in an ensemble can require significant memory. If you have memory constraints, a single model might be more feasible.\n",
        "When Data is Too Noisy:\n",
        "\n",
        "In some cases, if the data is extremely noisy or contains a large number of irrelevant features, ensemble methods might amplify the noise and lead to poor performance. Careful feature engineering and data preprocessing might be necessary before applying ensemble methods.\n",
        "When Base Learners are Too Similar:\n",
        "\n",
        "Ensembles work best when the base learners are diverse and make different types of errors. If the base learners are too similar, the ensemble might not provide much improvement over a single model.\n",
        "In these situations, it might be more appropriate to focus on using a single well-tuned model or exploring alternative techniques. However, it's important to note that these are general guidelines, and the best approach depends on the specific problem and data you are working with."
      ],
      "metadata": {
        "id": "erEnc4IA_GFz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16.How does Bagging help in reducing overfitting\n",
        "\n",
        "\n",
        "ans:-Bagging is an ensemble technique that aims to improve the stability and accuracy of machine learning models by reducing variance, which is a key factor contributing to overfitting. Here's how it works:\n",
        "\n",
        "Bootstrap Sampling:\n",
        "\n",
        "Bagging starts by creating multiple subsets of the training data using bootstrapping, which involves random sampling with replacement. This means that some data points may appear multiple times in a single subset, while others may be left out.\n",
        "Training Multiple Base Learners:\n",
        "\n",
        "A base learner (e.g., a decision tree) is trained independently on each of the bootstrap samples. Since each sample is slightly different, the resulting base learners will have some variations in their learned patterns and predictions.\n",
        "Aggregation of Predictions:\n",
        "\n",
        "When making predictions on new data, each base learner makes its own prediction. Bagging then aggregates these predictions to produce the final prediction. For classification tasks, this is usually done through majority voting, while for regression tasks, it's typically done by averaging the predictions.\n",
        "How Bagging Reduces Overfitting:\n",
        "\n",
        "Reducing Variance: By averaging the predictions of multiple base learners, bagging reduces the variance of the model. This means that the model is less sensitive to the specific training data used and is more likely to generalize well to unseen data. High variance is a characteristic of overfitting, where the model captures noise and irrelevant patterns in the training data.\n",
        "Smoothing Out Predictions: The aggregation process in bagging smooths out the predictions by combining the outputs of multiple base learners. This helps to reduce the impact of individual model errors and produces a more stable and robust prediction.\n",
        "Decorrelating Base Learners: Since each base learner is trained on a different bootstrap sample, they are less likely to be highly correlated. This decorrelation helps to prevent the ensemble from overfitting to specific features or patterns in the training data.\n",
        "In essence, bagging helps to create a more robust and generalizable model by combining the strengths of multiple base learners and reducing the impact of their individual weaknesses, thereby mitigating overfitti"
      ],
      "metadata": {
        "id": "R8TGyETn_PO5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. Why is Random Forest better than a single Decision Tree\n",
        "\n",
        "ans:-Random Forest models often outperform single Decision Trees due to their ability to reduce overfitting and improve generalization performance. Here's a breakdown of the key reasons:\n",
        "\n",
        "Reduced Overfitting:\n",
        "\n",
        "Decision Trees are prone to overfitting, especially when they are deep and complex. This means they can capture noise and irrelevant patterns in the training data, leading to poor performance on unseen data.\n",
        "Random Forest mitigates overfitting by combining multiple Decision Trees trained on different subsets of the data and features. This averaging process smooths out the predictions and reduces the impact of individual tree errors, resulting in better generalization.\n",
        "Improved Accuracy:\n",
        "\n",
        "By combining the predictions of multiple trees, Random Forest often achieves higher accuracy compared to a single Decision Tree. The diversity among the trees ensures that different aspects of the data are captured, leading to more robust predictions.\n",
        "Robustness to Outliers and Noise:\n",
        "\n",
        "Random Forest is less sensitive to outliers and noisy data points compared to a single Decision Tree. Individual trees might be affected by outliers, but their influence is minimized when the predictions are aggregated.\n",
        "Handling High-Dimensional Data:\n",
        "\n",
        "Random Forest can effectively handle datasets with a large number of features. The random feature selection process during tree construction allows the model to focus on the most relevant features, reducing the risk of overfitting to irrelevant ones.\n",
        "Feature Importance Estimation:\n",
        "\n",
        "Random Forest provides a built-in mechanism for estimating the importance of features in the prediction process. This information can be valuable for feature selection and understanding the underlying relationships in the data.\n",
        "Handling Missing Values:\n",
        "\n",
        "Random Forest can handle missing values in the data without requiring imputation. It uses a proximity measure to estimate missing values based on the values of other data points with similar features.\n",
        "In summary: Random Forest leverages the power of ensemble learning to overcome the limitations of individual Decision Trees, resulting in a more accurate, robust, and generalizable model. This is why it's often preferred in many machine learning applications"
      ],
      "metadata": {
        "id": "0Zq7L2pP_X4e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18.What is the role of bootstrap sampling in Bagging\n",
        "\n",
        "\n",
        "ans:-Bootstrap sampling plays a crucial role in Bagging (Bootstrap Aggregating) by creating diverse subsets of the training data, which are then used to train multiple base learners. This diversity among the base learners is key to reducing variance and improving the overall performance of the ensemble model.\n",
        "\n",
        "Here's a breakdown of the role of bootstrap sampling in Bagging:\n",
        "\n",
        "Creating Diverse Training Subsets:\n",
        "\n",
        "Bootstrap sampling involves randomly selecting data points from the original training dataset with replacement. This means that some data points may appear multiple times in a single bootstrap sample, while others may be left out.\n",
        "By creating multiple bootstrap samples, Bagging generates diverse subsets of the training data, each with a slightly different distribution of data points.\n",
        "Training Independent Base Learners:\n",
        "\n",
        "Each bootstrap sample is used to train a separate base learner (e.g., a decision tree). Since the samples are diverse, the resulting base learners will have variations in their learned patterns and predictions.\n",
        "This independence among the base learners is important for reducing correlation and improving the ensemble's ability to generalize to unseen data.\n",
        "Reducing Variance and Overfitting:\n",
        "\n",
        "When making predictions, the outputs of the individual base learners are aggregated (e.g., by averaging or majority voting). This aggregation process helps to reduce the variance of the ensemble model, making it less sensitive to the specific training data used.\n",
        "By reducing variance, Bagging helps to mitigate overfitting, which is a common problem in machine learning where models perform well on training data but poorly on unseen data.\n",
        "In summary, bootstrap sampling in Bagging serves to:\n",
        "\n",
        "Create diverse training subsets.\n",
        "Train independent base learners.\n",
        "Reduce variance and overfitting.\n",
        "By introducing this randomness and diversity, Bagging helps to create a more robust and generalizable ensemble model compared to using a single learner trained on the entire dataset."
      ],
      "metadata": {
        "id": "-4IECer4_gRG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19..What are some real-world applications of ensemble techniques\n",
        "\n",
        "ans;-Bagging is an ensemble technique that aims to improve the stability and accuracy of machine learning models by reducing variance, which is a key factor contributing to overfitting. Here's how it works:\n",
        "\n",
        "Bootstrap Sampling:\n",
        "\n",
        "Bagging starts by creating multiple subsets of the training data using bootstrapping, which involves random sampling with replacement. This means that some data points may appear multiple times in a single subset, while others may be left out.\n",
        "Training Multiple Base Learners:\n",
        "\n",
        "A base learner (e.g., a decision tree) is trained independently on each of the bootstrap samples. Since each sample is slightly different, the resulting base learners will have some variations in their learned patterns and predictions.\n",
        "Aggregation of Predictions:\n",
        "\n",
        "When making predictions on new data, each base learner makes its own prediction. Bagging then aggregates these predictions to produce the final prediction. For classification tasks, this is usually done through majority voting, while for regression tasks, it's typically done by averaging the predictions.\n",
        "How Bagging Reduces Overfitting:\n",
        "\n",
        "Reducing Variance: By averaging the predictions of multiple base learners, bagging reduces the variance of the model. This means that the model is less sensitive to the specific training data used and is more likely to generalize well to unseen data. High variance is a characteristic of overfitting, where the model captures noise and irrelevant patterns in the training data.\n",
        "Smoothing Out Predictions: The aggregation process in bagging smooths out the predictions by combining the outputs of multiple base learners. This helps to reduce the impact of individual model errors and produces a more stable and robust prediction.\n",
        "Decorrelating Base Learners: Since each base learner is trained on a different bootstrap sample, they are less likely to be highly correlated. This decorrelation helps to prevent the ensemble from overfitting to specific features or patterns in the training data.\n",
        "In essence, bagging helps to create a more robust and generalizable model by combining the strengths of multiple base learners and reducing the impact of their individual weaknesses, thereby mitigating overfitting."
      ],
      "metadata": {
        "id": "dhQemgBL_mO_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20.2 What is the difference between Bagging and Boosting?\n",
        "\n",
        "\n",
        "ans:-Bagging and Boosting are both ensemble techniques that combine multiple base learners to improve the overall performance of a machine learning model. However, they differ in their approach to building the ensemble:\n",
        "\n",
        "Bagging (Bootstrap Aggregating):\n",
        "\n",
        "Parallel Learning: Bagging trains base learners independently and in parallel. Each learner is trained on a different bootstrap sample (random subset with replacement) of the training data.\n",
        "Reduces Variance: The primary goal of Bagging is to reduce variance, which is the sensitivity of the model to fluctuations in the training data. This is achieved by averaging the predictions of the individual base learners, which helps to smooth out the overall prediction.\n",
        "Example: Random Forest is a popular example of Bagging, where multiple decision trees are trained on different subsets of the data and features.\n",
        "Boosting:\n",
        "\n",
        "Sequential Learning: Boosting trains base learners sequentially, where each subsequent learner focuses on correcting the errors made by the previous learners.\n",
        "Reduces Bias: The primary goal of Boosting is to reduce bias, which is the error introduced by simplifying assumptions about the data. This is achieved by iteratively adjusting the weights of training instances, giving more importance to misclassified instances in subsequent learners.\n",
        "Example: AdaBoost and Gradient Boosting are popular examples of Boosting.\n",
        "Here's a table summarizing the key differences:\n",
        "\n",
        "Feature\tBagging\tBoosting\n",
        "Learning Process\tParallel\tSequential\n",
        "Primary Goal\tReduce variance\tReduce bias\n",
        "Data Sampling\tBootstrap sampling with replacement\tWeighted sampling, focusing on misclassified instances\n",
        "Base Learner Independence\tIndependent learners\tDependent learners\n",
        "Aggregation\tAveraging or majority voting\tWeighted averaging\n",
        "Example\tRandom Forest\tAdaBoost, Gradient Boosting\n",
        "In general:\n",
        "\n",
        "Bagging is effective for models with high variance (e.g., decision trees) and helps to improve stability and generalization.\n",
        "Boosting is effective for models with high bias (e.g., weak learners) and helps to improve accuracy and predictive power.\n",
        "The choice between Bagging and Boosting depends on the specific problem and the characteristics of the data. If the base learner is prone to overfitting (high variance), Bagging is a good choice. If the base learner is too simple and has high bias, Boosting is a better option"
      ],
      "metadata": {
        "id": "ZMFPQCVA_s6C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PRACTICAL"
      ],
      "metadata": {
        "id": "TUZ6Xiq3B6Je"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21.2 Train a Bagging Classifier using Decision Trees on a sample dataset and print model accuracy2\n",
        "\n",
        "ANS:-"
      ],
      "metadata": {
        "id": "8le_pzrdB-NP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris  # Sample dataset"
      ],
      "metadata": {
        "id": "6_uMLws2CVDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "RXcdSTVcCZEl",
        "outputId": "8da0a4ee-e896-424e-91f6-ff2c32b2780e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'load_iris' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-6d4b4b798044>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0miris\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_iris\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miris\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miris\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'load_iris' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "gBh01xqHCgUe",
        "outputId": "647710f6-cc02-44b7-d253-e6273eaddd46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_test_split' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-7ec3ece10f48>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'train_test_split' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bagging_classifier = BaggingClassifier(base_estimator=DecisionTreeClassifier(),\n",
        "                                      n_estimators=10,  # Number of decision trees\n",
        "                                      random_state=42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "Fo3dIxUtCkXR",
        "outputId": "034a967e-a789-4ec0-8988-0e5d54e7e533"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'BaggingClassifier' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-9d9dfb424e63>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m bagging_classifier = BaggingClassifier(base_estimator=DecisionTreeClassifier(), \n\u001b[0m\u001b[1;32m      2\u001b[0m                                       \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Number of decision trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                       random_state=42)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'BaggingClassifier' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bagging_classifier = BaggingClassifier(base_estimator=DecisionTreeClassifier(),\n",
        "                                      n_estimators=10,  # Number of decision trees\n",
        "                                      random_state=42)"
      ],
      "metadata": {
        "id": "bWg5FAnTCnUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "Zr-uTjg4CqYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "22.Train a Bagging Regressor using Decision Trees and evaluate using Mean Squared Error (MSE\n",
        "\n",
        "\n",
        "ANS"
      ],
      "metadata": {
        "id": "jVZdD9D-CMCY"
      }
    },
    {
      "source": [
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.datasets import load_boston  # Sample dataset for regression"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "34qpPMzACxG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "boston = load_boston()\n",
        "X, y = boston.data, boston.target"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "DNo43CzRCx2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "s62rRFoHCy-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "bagging_regressor = BaggingRegressor(base_estimator=DecisionTreeRegressor(),\n",
        "                                      n_estimators=10,  # Number of decision trees\n",
        "                                      random_state=42)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "dYm4-VqlCzgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "bagging_regressor.fit(X_train, y_train)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "hyKskcx5C0NW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "y_pred = bagging_regressor.predict(X_test)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "9xen-6mhC01D",
        "outputId": "5387e11b-118f-4d4e-d425-bce0a057024c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'bagging_regressor' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-39c2c65569a7>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbagging_regressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'bagging_regressor' is not defined"
          ]
        }
      ]
    },
    {
      "source": [
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean Squared Error:\", mse)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "qEgw22GJC2HT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "23.Train a Random Forest Classifier on the Breast Cancer dataset and print feature importance scores"
      ],
      "metadata": {
        "id": "_lAg033HC4nI"
      }
    },
    {
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "import pandas as pd  # For displaying feature importances"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "ZCuxCOMFDD6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "breast_cancer = load_breast_cancer()\n",
        "X, y = breast_cancer.data, breast_cancer.target\n",
        "feature_names = breast_cancer.feature_names  # Get feature names"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "UJzWhkmaDEkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "GFCZ0V2WDFND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_classifier.fit(X_train, y_train)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "7ynz8mtlDFtR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "importances = rf_classifier.feature_importances_"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "JVVBtz9jDGU3",
        "outputId": "52f9f318-fa80-4251-d1a9-f34b06befc4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'rf_classifier' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-c7614f9a43ac>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimportances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrf_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'rf_classifier' is not defined"
          ]
        }
      ]
    },
    {
      "source": [
        "feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
        "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "f7v9QW_zDHDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "print(feature_importance_df)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "gpWC7MJuDHid",
        "outputId": "e698973b-19e0-4eee-a1f5-ea09f6c97b0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'feature_importance_df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-0b891c4c98ba>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_importance_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'feature_importance_df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2 Train a Random Forest Regressor and compare its performance with a single Decision Tree2\n",
        "\n",
        "\n",
        "ANS-"
      ],
      "metadata": {
        "id": "vxHeV6XADKkf"
      }
    },
    {
      "source": [
        "from sklearn.ensemble import RandomForestRegressor, BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.datasets import load_boston"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "S0Lg90V9DWZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Load the Boston Housing dataset\n",
        "boston = load_boston()\n",
        "X, y = boston.data, boston.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create and train a Random Forest Regressor\n",
        "rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Create and train a single Decision Tree Regressor\n",
        "dt_regressor = DecisionTreeRegressor(random_state=42)\n",
        "dt_regressor.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "# Make predictions on the test set using both models\n",
        "rf_predictions = rf_regressor.predict(X_test)\n",
        "dt_predictions = dt_regressor.predict(X_test)\n",
        "\n",
        "# Calculate and print the Mean Squared Error (MSE) for both models\n",
        "rf_mse = mean_squared_error(y_test, rf_predictions)\n",
        "dt_mse = mean_squared_error(y_test, dt_predictions)\n",
        "\n",
        "print(\"Random Forest MSE:\", rf_mse)\n",
        "print(\"Decision Tree MSE:\", dt_mse)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "I_D7tTmiDXGw",
        "outputId": "4271bf4e-2dc2-45ac-b0f3-2564ed45823f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'load_boston' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-0d1283449ce7>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load the Boston Housing dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mboston\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_boston\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mboston\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboston\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Split the data into training and testing sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'load_boston' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute the Out-of-Bag (OOB) Score for a Random Forest Classifie\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OSI5dx0iDaIf"
      }
    },
    {
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_breast_cancer  # Sample dataset\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X, y = breast_cancer.data, breast_cancer.target\n",
        "\n",
        "# Split the dataset into training and testing sets (optional, for comparison)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Random Forest Classifier with oob_score enabled\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=42)\n",
        "\n",
        "# Train the Random Forest Classifier\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Get the OOB Score\n",
        "oob_score = rf_classifier.oob_score_\n",
        "\n",
        "# Print the OOB Score\n",
        "print(\"OOB Score:\", oob_score)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LjfTAUW2Dmt9",
        "outputId": "5a5010fe-23fa-43b6-f7bf-3180c8d20627"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OOB Score: 0.9547738693467337\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "26.Train a Bagging Classifier using SVM as a base estimator and print accuracy\n",
        "ANS="
      ],
      "metadata": {
        "id": "EeMou499DX40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_breast_cancer  # Sample dataset"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "LqwXHqLmD0Xp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Load the Breast Cancer dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X, y = breast_cancer.data, breast_cancer.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Bagging Classifier with SVM as the base estimator\n",
        "bagging_classifier = BaggingClassifier(base_estimator=SVC(),\n",
        "                                      n_estimators=10,  # Number of SVM classifiers\n",
        "                                      random_state=42)\n",
        "\n",
        "# Train the Bagging Classifier\n",
        "bagging_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = bagging_classifier.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "K_8RYGgUD03N",
        "outputId": "8f5c6671-cd6c-420d-97bf-48342a638463"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "BaggingClassifier.__init__() got an unexpected keyword argument 'base_estimator'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-1ba5fe89335f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Create a Bagging Classifier with SVM as the base estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m bagging_classifier = BaggingClassifier(base_estimator=SVC(), \n\u001b[0m\u001b[1;32m     10\u001b[0m                                       \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Number of SVM classifiers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                                       random_state=42)\n",
            "\u001b[0;31mTypeError\u001b[0m: BaggingClassifier.__init__() got an unexpected keyword argument 'base_estimator'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "27.Train a Bagging Classifier using SVM as a base estimator and print accuracy\n",
        "\n",
        "ANS:-"
      ],
      "metadata": {
        "id": "-5Ntw3BYD3_s"
      }
    },
    {
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_breast_cancer  # Sample dataset\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X, y = breast_cancer.data, breast_cancer.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Bagging Classifier with SVM as the base estimator\n",
        "bagging_classifier = BaggingClassifier(base_estimator=SVC(),\n",
        "                                      n_estimators=10,  # Number of SVM classifiers\n",
        "                                      random_state=42)\n",
        "\n",
        "# Train the Bagging Classifier\n",
        "bagging_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = bagging_classifier.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "HceRX2wEEE3v",
        "outputId": "783ad8b8-5155-4d7a-9099-7228cf84dd64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "BaggingClassifier.__init__() got an unexpected keyword argument 'base_estimator'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-8415ec450793>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Create a Bagging Classifier with SVM as the base estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m bagging_classifier = BaggingClassifier(base_estimator=SVC(),\n\u001b[0m\u001b[1;32m     16\u001b[0m                                       \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Number of SVM classifiers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                                       random_state=42)\n",
            "\u001b[0;31mTypeError\u001b[0m: BaggingClassifier.__init__() got an unexpected keyword argument 'base_estimator'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train a Bagging Classifier using Logistic Regression as a base estimator and print AUC score\n",
        "\n",
        "\n",
        "ANS:=-"
      ],
      "metadata": {
        "id": "FHiGMca8EG9Q"
      }
    },
    {
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.datasets import load_breast_cancer  # Sample dataset\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X, y = breast_cancer.data, breast_cancer.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Bagging Classifier with Logistic Regression as the base estimator\n",
        "bagging_classifier = BaggingClassifier(base_estimator=LogisticRegression(),\n",
        "                                      n_estimators=10,  # Number of Logistic Regression models\n",
        "                                      random_state=42)\n",
        "\n",
        "# Train the Bagging Classifier\n",
        "bagging_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities on the test set\n",
        "y_probs = bagging_classifier.predict_proba(X_test)[:, 1]  # Probability of positive class\n",
        "\n",
        "# Calculate and print the AUC score\n",
        "auc_score = roc_auc_score(y_test, y_probs)\n",
        "print(\"AUC Score:\", auc_score)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "KVLWU_asEmug",
        "outputId": "da537034-a6a9-4db1-c405-118c3e5a8956"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "BaggingClassifier.__init__() got an unexpected keyword argument 'base_estimator'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-6316d7da6274>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Create a Bagging Classifier with Logistic Regression as the base estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m bagging_classifier = BaggingClassifier(base_estimator=LogisticRegression(),\n\u001b[0m\u001b[1;32m     16\u001b[0m                                       \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Number of Logistic Regression models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                                       random_state=42)\n",
            "\u001b[0;31mTypeError\u001b[0m: BaggingClassifier.__init__() got an unexpected keyword argument 'base_estimator'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "29.Train a Bagging Classifier using Logistic Regression as a base estimator and print AUC score\n",
        "\n",
        "\n",
        "ANS:-"
      ],
      "metadata": {
        "id": "LaVy1J3jEnx6"
      }
    },
    {
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.datasets import load_breast_cancer  # Sample dataset\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X, y = breast_cancer.data, breast_cancer.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Bagging Classifier with Logistic Regression as the base estimator\n",
        "bagging_classifier = BaggingClassifier(base_estimator=LogisticRegression(),\n",
        "                                      n_estimators=10,  # Number of Logistic Regression models\n",
        "                                      random_state=42)\n",
        "\n",
        "# Train the Bagging Classifier\n",
        "bagging_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities on the test set\n",
        "y_probs = bagging_classifier.predict_proba(X_test)[:, 1]  # Probability of positive class\n",
        "\n",
        "# Calculate and print the AUC score\n",
        "auc_score = roc_auc_score(y_test, y_probs)\n",
        "print(\"AUC Score:\", auc_score)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "bqba2ApxEy71",
        "outputId": "fa510f70-debe-42d9-b826-d0f135e18954"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "BaggingClassifier.__init__() got an unexpected keyword argument 'base_estimator'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-6316d7da6274>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Create a Bagging Classifier with Logistic Regression as the base estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m bagging_classifier = BaggingClassifier(base_estimator=LogisticRegression(),\n\u001b[0m\u001b[1;32m     16\u001b[0m                                       \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Number of Logistic Regression models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                                       random_state=42)\n",
            "\u001b[0;31mTypeError\u001b[0m: BaggingClassifier.__init__() got an unexpected keyword argument 'base_estimator'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "30.Train a Bagging Classifier using Logistic Regression as a base estimator and print AUC score\n",
        "\n",
        "\n",
        "ANS:-"
      ],
      "metadata": {
        "id": "6TE6XoKnE0Lz"
      }
    },
    {
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.datasets import load_breast_cancer  # Sample dataset\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X, y = breast_cancer.data, breast_cancer.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Bagging Classifier with Logistic Regression as the base estimator\n",
        "bagging_classifier = BaggingClassifier(base_estimator=LogisticRegression(),\n",
        "                                      n_estimators=10,  # Number of Logistic Regression models\n",
        "                                      random_state=42)\n",
        "\n",
        "# Train the Bagging Classifier\n",
        "bagging_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities on the test set\n",
        "y_probs = bagging_classifier.predict_proba(X_test)[:, 1]  # Probability of positive class\n",
        "\n",
        "# Calculate and print the AUC score\n",
        "auc_score = roc_auc_score(y_test, y_probs)\n",
        "print(\"AUC Score:\", auc_score)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "jjixMWX_FPo0",
        "outputId": "9f4737d1-08dc-4895-e6ea-423ad6423b60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "BaggingClassifier.__init__() got an unexpected keyword argument 'base_estimator'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-6316d7da6274>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Create a Bagging Classifier with Logistic Regression as the base estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m bagging_classifier = BaggingClassifier(base_estimator=LogisticRegression(),\n\u001b[0m\u001b[1;32m     16\u001b[0m                                       \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Number of Logistic Regression models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                                       random_state=42)\n",
            "\u001b[0;31mTypeError\u001b[0m: BaggingClassifier.__init__() got an unexpected keyword argument 'base_estimator'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " 31.\n",
        "Train a Random Forest Classifier and tune hyperparameters using GridSearchCV"
      ],
      "metadata": {
        "id": "TjFQnJYbFQzR"
      }
    },
    {
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "# Import other necessary libraries (e.g., pandas for data loading)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "d-TV3VTOGJg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Assuming you have your data in a pandas DataFrame called 'df'\n",
        "# with features in 'X' and target variable in 'y'\n",
        "X = df[['feature1', 'feature2', ...]]\n",
        "y = df['target_variable']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "uny0vngFGJ9k",
        "outputId": "e8c5d25c-7d35-4773-aa02-97e537e2333e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-5fcb7cb310ea>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Assuming you have your data in a pandas DataFrame called 'df'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# with features in 'X' and target variable in 'y'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'feature1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'feature2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target_variable'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "QkUFz8aLGLEX"
      }
    },
    {
      "source": [
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],  # Number of trees in the forest\n",
        "    'max_depth': [None, 5, 10],  # Maximum depth of the trees\n",
        "    'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split an internal node\n",
        "    'min_samples_leaf': [1, 2, 4]  # Minimum number of samples required to be at a leaf node\n",
        "}"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "CR6Yed1aGLua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "rf_classifier = RandomForestClassifier(random_state=42)\n",
        "grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "RU-YfgUQGMUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "print(\"Best Hyperparameters:\", best_params)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "OxLP9JplGM_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Test Accuracy:\", accuracy)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "j7FWqK5zGNSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "rain a Bagging Regressor with different numbers of base estimators and compare performance="
      ],
      "metadata": {
        "id": "a9GJSy18GOdf"
      }
    },
    {
      "source": [
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor  # Or any other base regressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error  # Or any other regression metric\n",
        "import numpy as np"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "EXpoEbgSGfFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Assuming you have your data in a pandas DataFrame called 'df'\n",
        "# with features in 'X' and target variable in 'y'\n",
        "X = df[['feature1', 'feature2', ...]]\n",
        "y = df['target_variable']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "k7V68OasGfle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "n_estimators_list = [10, 50, 100, 200]  # List of base estimator numbers to try\n",
        "results = []\n",
        "\n",
        "for n_estimators in n_estimators_list:\n",
        "    # Create a Bagging Regressor with the current number of base estimators\n",
        "    bagging_regressor = BaggingRegressor(base_estimator=DecisionTreeRegressor(), n_estimators=n_estimators, random_state=42)\n",
        "\n",
        "    # Fit the model to the training data\n",
        "    bagging_regressor.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions on the test data\n",
        "    y_pred = bagging_regressor.predict(X_test)\n",
        "\n",
        "    # Calculate the mean squared error (or any other metric)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "    # Store the results\n",
        "    results.append({'n_estimators': n_estimators, 'mse': mse})\n",
        "\n",
        "# Print or visualize the results (e.g., using a bar plot)\n",
        "for result in results:\n",
        "    print(f\"Number of Base Estimators: {result['n_estimators']}, MSE: {result['mse']}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "YzYBZFS_GgGD",
        "outputId": "d025e4a8-a61d-4c8c-b609-55003837a937"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'BaggingRegressor' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-20a045754508>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn_estimators\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mn_estimators_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Create a Bagging Regressor with the current number of base estimators\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mbagging_regressor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBaggingRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDecisionTreeRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# Fit the model to the training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'BaggingRegressor' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "33.= Train a Random Forest Classifier and analyze misclassified samples=\n"
      ],
      "metadata": {
        "id": "ftJ2cQvIGh3T"
      }
    },
    {
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "ZIryr_OXGpvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "34.= Train a Bagging Classifier and compare its performance with a single Decision Tree Classifier="
      ],
      "metadata": {
        "id": "NyN3bz5GGqTt"
      }
    },
    {
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "sxgeiTcbG176"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "35.Train a Random Forest Classifier and visualize the confusion matrix="
      ],
      "metadata": {
        "id": "FERtLnIZG640"
      }
    },
    {
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "uAWEwPTVHNvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Assuming you have your data in a pandas DataFrame called 'df'\n",
        "# with features in 'X' and target variable in 'y'\n",
        "X = df[['feature1', 'feature2', ...]]\n",
        "y = df['target_variable']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "FJevI2OaHPGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Create a Random Forest Classifier with desired parameters\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Fit the model to the training data\n",
        "rf_classifier.fit(X_train, y_train)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "G8VtC9BfHQVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Make predictions on the test data\n",
        "y_pred = rf_classifier.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "tSbITWzCHQ1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Create the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualize the confusion matrix using Seaborn's heatmap\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
        "plt.xlabel(\"Predicted Labels\")\n",
        "plt.ylabel(\"True Labels\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "v0SWNJ2rHRsQ",
        "outputId": "423e8fd1-34a9-4153-8b80-0613be7a7633"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'confusion_matrix' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-bf4e18a36eb2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Create the confusion matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Visualize the confusion matrix using Seaborn's heatmap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'confusion_matrix' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "36.= Train a Stacking Classifier using Decision Trees, SVM, and Logistic Regression, and compare accuracy\n",
        "\n",
        "\n",
        "ans:-"
      ],
      "metadata": {
        "id": "CGtCcgFIHT3z"
      }
    },
    {
      "source": [
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "g1oPFB7NHdR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Assuming you have your data in a pandas DataFrame called 'df'\n",
        "# with features in 'X' and target variable in 'y'\n",
        "X = df[['feature1', 'feature2', ...]]\n",
        "y = df['target_variable']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "82bKs1Q_HeGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "estimators = [\n",
        "    ('dt', DecisionTreeClassifier(random_state=42)),\n",
        "    ('svm', SVC(random_state=42)),\n",
        "    ('lr', LogisticRegression(random_state=42))\n",
        "]\n",
        "final_estimator = LogisticRegression(random_state=42)  # You can choose a different final estimator if needed"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "xrmsPGSnHekQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "stacking_classifier = StackingClassifier(estimators=estimators, final_estimator=final_estimator)\n",
        "stacking_classifier.fit(X_train, y_train)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "E-yJPHkfHe-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Stacking Classifier\n",
        "y_pred_stacking = stacking_classifier.predict(X_test)\n",
        "accuracy_stacking = accuracy_score(y_test, y_pred_stacking)\n",
        "print(\"Stacking Classifier Accuracy:\", accuracy_stacking)\n",
        "\n",
        "# Individual models for comparison\n",
        "for name, estimator in estimators:\n",
        "    estimator.fit(X_train, y_train)\n",
        "    y_pred_individual = estimator.predict(X_test)\n",
        "    accuracy_individual = accuracy_score(y_test, y_pred_individual)\n",
        "    print(f\"{name} Accuracy:\", accuracy_individual)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "1sB5BfFdHfgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Train a Random Forest Classifier and print the top 5 most important features\n",
        "\n",
        "\n",
        " ans:-"
      ],
      "metadata": {
        "id": "JtclxT5hHhVr"
      }
    },
    {
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "rFNJcCJ_Hv9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Assuming you have your data in a pandas DataFrame called 'df'\n",
        "# with features in 'X' and target variable in 'y'\n",
        "X = df[['feature1', 'feature2', ...]]\n",
        "y = df['target_variable']\n",
        "\n",
        "# Split the data into training and testing sets (optional but recommended)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Ho1mOVPgHwnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Create a Random Forest Classifier with desired parameters\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Fit the model to the training data\n",
        "rf_classifier.fit(X_train, y_train)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "sA368PYdHxpa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Get feature importances from the trained model\n",
        "importances = rf_classifier.feature_importances_\n",
        "\n",
        "# Create a DataFrame to store feature names and importances\n",
        "feature_importances_df = pd.DataFrame({'feature': X_train.columns, 'importance': importances})\n",
        "\n",
        "# Sort the DataFrame by importance in descending order\n",
        "feature_importances_df = feature_importances_df.sort_values(by='importance', ascending=False)\n",
        "\n",
        "# Print the top 5 most important features\n",
        "print(feature_importances_df.head(5))"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "qljNmRgnHyBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "38.= Train a Bagging Classifier and evaluate performance using Precision, Recall, and F1-score\n",
        "\n",
        "ans:-"
      ],
      "metadata": {
        "id": "mNjWrZNwHr7e"
      }
    },
    {
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier # Or any other base classifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "fIy0ZHnrH8Ln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Assuming you have your data in a pandas DataFrame called 'df'\n",
        "# with features in 'X' and target variable in 'y'\n",
        "X = df[['feature1', 'feature2', ...]]\n",
        "y = df['target_variable']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "YOyzBJ1bH8qs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Create a Bagging Classifier with desired parameters\n",
        "bagging_classifier = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=100, random_state=42)\n",
        "\n",
        "# Fit the model to the training data\n",
        "bagging_classifier.fit(X_train, y_train)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "qXFQtPMdH9QX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Make predictions on the test data\n",
        "y_pred = bagging_classifier.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Calculate precision, recall, and F1-score\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-score:\", f1)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "foYCBzqpH9mS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train a Random Forest Classifier and analyze the effect of max_depth on accuracy\n",
        "\n",
        "\n",
        "ans:-"
      ],
      "metadata": {
        "id": "p5PnALkYH_U_"
      }
    },
    {
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "UzxJ5J0GIPqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Assuming you have your data in a pandas DataFrame called 'df'\n",
        "# with features in 'X' and target variable in 'y'\n",
        "X = df[['feature1', 'feature2', ...]]\n",
        "y = df['target_variable']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "wY-U6eL2IQQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "\n",
        "max_depth_values = [None, 2, 5, 10, 20] # Values for max_depth to try\n",
        "accuracy_scores = []\n",
        "\n",
        "for max_depth in max_depth_values:\n",
        "    # Create a Random Forest Classifier with the current max_depth\n",
        "    rf_classifier = RandomForestClassifier(n_estimators=100, max_depth=max_depth, random_state=42)\n",
        "\n",
        "    # Fit the model to the training data\n",
        "    rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions on the test data\n",
        "    y_pred = rf_classifier.predict(X_test)\n",
        "\n",
        "    # Calculate accuracy and store it\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracy_scores.append(accuracy)\n",
        "\n",
        "# Print the accuracy scores for each max_depth value\n",
        "for i, max_depth in enumerate(max_depth_values):\n",
        "    print(f\"Max Depth: {max_depth}, Accuracy: {accuracy_scores[i]}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtwSj_p3IRj4",
        "outputId": "ad6048f1-a453-452e-bb44-66eda16e5322"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max Depth: None, Accuracy: 0.9707602339181286\n",
            "Max Depth: 2, Accuracy: 0.9532163742690059\n",
            "Max Depth: 5, Accuracy: 0.9649122807017544\n",
            "Max Depth: 10, Accuracy: 0.9707602339181286\n",
            "Max Depth: 20, Accuracy: 0.9707602339181286\n"
          ]
        }
      ]
    },
    {
      "source": [
        "plt.plot(max_depth_values, accuracy_scores, marker='o')\n",
        "plt.xlabel(\"Max Depth\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Effect of Max Depth on Accuracy\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "HRYW-PdIIUml",
        "outputId": "bb23a99e-8d62-4732-ebdb-85d94c74f98d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'plt' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-4109261f8e85>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_depth_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'o'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Max Depth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Effect of Max Depth on Accuracy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "40. Train a Bagging Regressor using different base estimators (DecisionTree and KNeighbors) and compare\n",
        "performance\n",
        "\n",
        "\n",
        "ans:-"
      ],
      "metadata": {
        "id": "s6k_QZUCIWi2"
      }
    },
    {
      "source": [
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error  # Or any other regression metric"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "3txD2kfUIgI-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Assuming you have your data in a pandas DataFrame called 'df'\n",
        "# with features in 'X' and target variable in 'y'\n",
        "X = df[['feature1', 'feature2', ...]]\n",
        "y = df['target_variable']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "4gLV_WpvIgfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "base_estimators = [\n",
        "    ('DecisionTree', DecisionTreeRegressor()),\n",
        "    ('KNeighbors', KNeighborsRegressor())\n",
        "]\n",
        "\n",
        "results = []\n",
        "\n",
        "for name, estimator in base_estimators:\n",
        "    # Create a Bagging Regressor with the current base estimator\n",
        "    bagging_regressor = BaggingRegressor(base_estimator=estimator, n_estimators=100, random_state=42)\n",
        "\n",
        "    # Fit the model to the training data\n",
        "    bagging_regressor.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions on the test data\n",
        "    y_pred = bagging_regressor.predict(X_test)\n",
        "\n",
        "    # Calculate the mean squared error (or any other metric)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "    # Store the results\n",
        "    results.append({'Base Estimator': name, 'MSE': mse})\n",
        "\n",
        "# Print the results\n",
        "for result in results:\n",
        "    print(f\"Base Estimator: {result['Base Estimator']}, MSE: {result['MSE']}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "R3Xb4nU0Ig9Q",
        "outputId": "c16b4cda-c212-4885-8bde-658d51ab7d0b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'DecisionTreeRegressor' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-73f78fa4209b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m base_estimators = [\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0;34m'DecisionTree'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDecisionTreeRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0;34m'KNeighbors'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKNeighborsRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m ]\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'DecisionTreeRegressor' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "41.= Train a Random Forest Classifier and evaluate its performance using ROC-AUC Score\n",
        "\n",
        "\n",
        "ans:-"
      ],
      "metadata": {
        "id": "xnGHJQrEIioD"
      }
    },
    {
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "cNU-IQD_ItdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Assuming you have your data in a pandas DataFrame called 'df'\n",
        "# with features in 'X' and target variable in 'y'\n",
        "X = df[['feature1', 'feature2', ...]]\n",
        "y = df['target_variable']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "meoHpRHSItu-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Create a Random Forest Classifier with desired parameters\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Fit the model to the training data\n",
        "rf_classifier.fit(X_train, y_train)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "2BiAR5l1Iua6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Predict probabilities for the test data\n",
        "y_probs = rf_classifier.predict_proba(X_test)[:, 1]  # Probability of positive class\n",
        "\n",
        "# Calculate the ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_probs)\n",
        "print(\"ROC-AUC Score:\", roc_auc)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "m3ENOOyWIu_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "43.Train a Bagging Classifier and evaluate its performance using cross-validatio.\n",
        "\n",
        "\n",
        "ans:-"
      ],
      "metadata": {
        "id": "lqpfXIxkIvtE"
      }
    },
    {
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier  # Or any other base classifier\n",
        "from sklearn.model_selection import cross_val_score, train_test_split\n",
        "from sklearn.metrics import accuracy_score  # Or any other suitable metric"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "KRW7nTXEI50k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Assuming you have your data in a pandas DataFrame called 'df'\n",
        "# with features in 'X' and target variable in 'y'\n",
        "X = df[['feature1', 'feature2', ...]]\n",
        "y = df['target_variable']"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "IlQ55HJsI6HM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Create a Bagging Classifier with desired parameters\n",
        "bagging_classifier = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=100, random_state=42)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "4mnWCI2VI6hy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Perform cross-validation with 5 folds (you can adjust the number of folds)\n",
        "scores = cross_val_score(bagging_classifier, X, y, cv=5, scoring='accuracy')  # Use appropriate scoring metric\n",
        "\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", scores)\n",
        "\n",
        "# Print the average accuracy\n",
        "print(\"Average accuracy:\", scores.mean())"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "tzX6mIUnI6_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "43\n",
        ".Train a Random Forest Classifier and plot the Precision-Recall curv\u0010\n",
        "\n",
        "\n",
        "ans:-"
      ],
      "metadata": {
        "id": "R6H9i8tAI8d3"
      }
    },
    {
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_curve, auc\n",
        "import matplotlib.pyplot as plt"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "SeJirFmVJJSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Assuming you have your data in a pandas DataFrame called 'df'\n",
        "# with features in 'X' and target variable in 'y'\n",
        "X = df[['feature1', 'feature2', ...]]\n",
        "y = df['target_variable']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "vYDGJ5XqJJtj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Create a Random Forest Classifier with desired parameters\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Fit the model to the training data\n",
        "rf_classifier.fit(X_train, y_train)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "ZWUY8C0MJKPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Predict probabilities for the test data\n",
        "y_probs = rf_classifier.predict_proba(X_test)[:, 1]  # Probability of positive class\n",
        "\n",
        "# Calculate precision and recall values\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_probs)\n",
        "\n",
        "# Calculate the area under the precision-recall curve (AUC)\n",
        "auc_score = auc(recall, precision)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "GA4dudnfJKtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "plt.plot(recall, precision, label=f'Random Forest (AUC = {auc_score:.2f})')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.legend(loc='lower left')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "mKhpSZTyJLMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train a Stacking Classifier with Random Forest and Logistic Regression and compare accuracy\n",
        "\n",
        "\n",
        "ans:-"
      ],
      "metadata": {
        "id": "vEuB4TJVJMKL"
      }
    },
    {
      "source": [
        "from sklearn.ensemble import StackingClassifier, RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "O14qckP6JTme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Assuming you have your data in a pandas DataFrame called 'df'\n",
        "# with features in 'X' and target variable in 'y'\n",
        "X = df[['feature1', 'feature2', ...]]\n",
        "y = df['target_variable']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "wQ7Nlt33JUMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "estimators = [\n",
        "    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
        "    ('lr', LogisticRegression(random_state=42))\n",
        "]\n",
        "final_estimator = LogisticRegression(random_state=42)  # You can choose a different final estimator if needed"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "DFdbwMVHJVvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "stacking_classifier = StackingClassifier(estimators=estimators, final_estimator=final_estimator)\n",
        "stacking_classifier.fit(X_train, y_train)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "cxYhvWIRJWMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Stacking Classifier\n",
        "y_pred_stacking = stacking_classifier.predict(X_test)\n",
        "accuracy_stacking = accuracy_score(y_test, y_pred_stacking)\n",
        "print(\"Stacking Classifier Accuracy:\", accuracy_stacking)\n",
        "\n",
        "# Individual models for comparison\n",
        "for name, estimator in estimators:\n",
        "    estimator.fit(X_train, y_train)\n",
        "    y_pred_individual = estimator.predict(X_test)\n",
        "    accuracy_individual"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "uYmp2y3aJWve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "45.= Train a Bagging Regressor with different levels of bootstrap samples and compare performance\n",
        "\n",
        "\n",
        "ans:-"
      ],
      "metadata": {
        "id": "qqIsg5lNJYf_"
      }
    },
    {
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor # Or any other base estimator\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error # Or any other regression metric"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "31f2wXX7JmT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Assuming you have your data in a pandas DataFrame called 'df'\n",
        "# with features in 'X' and target variable in 'y'\n",
        "X = df[['feature1', 'feature2', ...]]\n",
        "y = df['target_variable']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "ncl7z_1yJmux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "bootstrap_sizes = [0.5, 0.7, 0.9, 1.0] # Different proportions of training data for bootstrap samples\n",
        "results = []\n",
        "\n",
        "for bootstrap_size in bootstrap_sizes:\n",
        "    # Create a Bagging Regressor with the current bootstrap sample size\n",
        "    bagging_regressor = BaggingRegressor(base_estimator=DecisionTreeRegressor(),\n",
        "                                        n_estimators=100,\n",
        "                                        max_samples=bootstrap_size,\n",
        "                                        random_state=42)\n",
        "\n",
        "    # Fit the model to the training data\n",
        "    bagging_regressor.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions on the test data\n",
        "    y_pred = bagging_regressor.predict(X_test)\n",
        "\n",
        "    # Calculate the mean squared error (or any other metric)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "    # Store the results\n",
        "    results.append({'Bootstrap Size': bootstrap_size, 'MSE': mse})\n",
        "\n",
        "# Print the results\n",
        "for result in results:\n",
        "    print(f\"Bootstrap Size: {result['Bootstrap Size']}, MSE: {result['MSE']}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "j3Sn0YlYJnQi",
        "outputId": "9ad958ce-ef2b-4a42-c1d0-303a8ce8da6b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'BaggingRegressor' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-06f46efd81df>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbootstrap_size\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbootstrap_sizes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Create a Bagging Regressor with the current bootstrap sample size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     bagging_regressor = BaggingRegressor(base_estimator=DecisionTreeRegressor(), \n\u001b[0m\u001b[1;32m      7\u001b[0m                                         \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                                         \u001b[0mmax_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbootstrap_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'BaggingRegressor' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bVPpRY-VD2bs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}